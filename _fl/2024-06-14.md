---
title: 連合学習 (2024-06-14 ~ 2024-06-20)
date: 2024-06-14
---

連合学習に関する論文まとめ (2024-06-14 ~ 2024-06-20)


- - -

### [Feasibility of Federated Learning from Client Databases with Different Brain Diseases and MRI Modalities](http://arxiv.org/abs/2406.11636)

**クライアントデータベースから異なる脳疾患とMRIモダリティを用いた連合学習の実現可能性**

Felix Wagner, Wentian Xu, Pramit Saha, Ziyun Liang, Daniel Whitehouse, David Menon, Natalie Voets, J. Alison Noble, Konstantinos Kamnitsas

- 特定の脳疾患用のセグメンテーションモデルは固定されたMRIモダリティセットで訓練される
- 連合学習を用いて異なる脳疾患と多様なMRIモダリティで単一モデルの訓練を試みる
- 全てのクライアントデータベースのモダリティをカバーする入力チャンネルやランダムなモダリティドロップの訓練を導入
- 7つの脳MRIデータベースで評価し、新規データベースでも高いセグメンテーション性能を確認

異なる脳疾患やモダリティの組み合わせでも、連合学習が有効なんて面白そう！将来はもっと多くの病気やデータに対応できるモデルが出てきてほしいな。



**トピック:** [連合学習](../../fl), **カテゴリ:** eess.IV, cs.CV, cs.LG, I.4.9; I.4.6; I.2.11; I.4.0, **投稿日時:** 2024-06-17 15:16


- - -

### [Pre-Training and Personalized Fine-Tuning via Over-the-Air Federated Meta-Learning: Convergence-Generalization Trade-Offs](http://arxiv.org/abs/2406.11569)

**空中連合メタ学習による事前トレーニングと個別調整: 収束と一般化のトレードオフ**

Haifeng Wen, Hong Xing, Osvaldo Simeone

- 大規模言語モデルなどのAIアプリでは、事前トレーニング後のファインチューニングが主流
- 連合学習（FL）が進み、事前トレーニングが中央集権型から分散型に移行中
- メタ学習ベースの個別FLは、基本的なパーソナライズを超え、新しいエージェントやタスクへの一般化を目指す
- 無線設定での一般化と収束のトレードオフを分析、チャンネルの欠陥が一般化を助ける一方で収束を悪化させる

無線通信を利用してAIモデルをパーソナライズするなんて、未来を感じるね。新しい技術がどんな風に役立つか楽しみ！

**Comment:** 37 pages, 7 figures, submitted for possible journal publication

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.IT, eess.SP, math.IT, **投稿日時:** 2024-06-17 14:06


- - -

### [Federated Active Learning Framework for Efficient Annotation Strategy in Skin-lesion Classification](http://arxiv.org/abs/2406.11310)

**皮膚病変分類における効率的なアノテーション戦略のための連合能動学習フレームワーク**

Zhipeng Deng, Yuqiao Yang, Kenji Suzuki

- 連合学習（FL）は複数の機関がプライベートデータを共有せずにモデルを共同で訓練できる
- 医療シナリオではデータのアノテーションに専門知識と労働力が必要であり、FLにおいて重大な問題
- 提案する連合能動学習（FedAL）フレームワークは、FLの下で定期的かつ対話的にALを実行する
- 実データセットで検証し、50%のサンプルで最先端の性能を達成、他のAL手法より優れた結果を示す

連合能動学習がどんな未来を切り開くのか気になる！これで医療データの負担が減れば、多くの人が救われそうだよね。

**Comment:** 14 pages, 3 figures

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CV, cs.LG, **投稿日時:** 2024-06-17 08:16


- - -

### [Save It All: Enabling Full Parameter Tuning for Federated Large Language Models via Cycle Black Gradient Descent](http://arxiv.org/abs/2406.11187)

**すべてを保存：Cycle Black Gradient Descentによる連合大規模言語モデルの完全なパラメータチューニングの実現**

Lin Wang, Zhichao Wang, Xiaoying Tang

- 大規模言語モデル（LLMs）の登場により、深層学習のパラダイムが革命的に変わった
- 連合学習（FL）では、LLMsの事前学習やファインチューニングが計算資源やメモリ消費、通信ボトルネックに直面する
- 提案手法FedCyBGDは、Cycle Block Gradient Descentを用いて周期的にモデルを更新し、通信や計算、メモリコストを削減する
- FedCyBGDは選択されたブロックの更新とアップロードだけで完全なパラメータ学習を可能にし、FL LLMトレーニングで最先端の性能を実現

この方法を使えば、みんなのパソコンの負担が軽くなるってことかな？本当に実用化されたら影響大きそうでワクワクするね！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, **投稿日時:** 2024-06-17 03:49


- - -

### [Federated Face Forgery Detection Learning with Personalized Representation](http://arxiv.org/abs/2406.11145)

**個別化表現による連合型顔偽造検出学習**

Decheng Liu, Zhan Dang, Chunlei Peng, Nannan Wang, Ruimin Hu, Xinbo Gao

- 深層生成技術は高品質な偽動画を作成でき、深刻な社会的脅威である
- 連合学習戦略により、データのプライバシーを保護しつつモデルパラメータを集約
- クライアントごとの個別化表現学習で、検出性能を向上
- 公開顔偽造検出データセットの実験で、最先端手法と比べて優れた性能を示した

個別化表現で性能向上とか新しい感じでワクワクする！実用化が進んだら、偽動画の脅威も怖くなくなるかもね。楽しみ〜。

**Comment:** The code is publicly available

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CV, **投稿日時:** 2024-06-17 02:20


- - -

### [Leveraging Foundation Models for Multi-modal Federated Learning with Incomplete Modality](http://arxiv.org/abs/2406.11048)

**基礎モデルを活用した不完全モダリティによるマルチモーダル連合学習**

Liwei Che, Jiaqi Wang, Xinyue Liu, Fenglong Ma

- 連合学習は分散データ環境での共同訓練をプライバシー保証と共に実現
- クライアントが複数のデータモダリティを保有する現実的なシナリオに注目
- モダリティ欠損問題を解決するためFedMVPを提案し、事前訓練モデルを利用
- モデルは現実世界の画像とテキスト分類データセットで優れた性能を示す

異なるモダリティ間でも連合学習を効率的に行えるってすごい！これが実用化されたら、もっと複雑なデータもプライバシーを守りながら分析できるようになるかもね。

**Comment:** Accepted by ECML-PKDD 2024

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-06-16 19:18


- - -

### [Promoting Data and Model Privacy in Federated Learning through Quantized LoRA](http://arxiv.org/abs/2406.10976)

**量子化されたLoRAによる連合学習におけるデータおよびモデルプライバシーの促進**

JianHao Zhu, Changze Lv, Xiaohua Wang, Muling Wu, Wenhao Liu, Tianlong Li, Zixuan Ling, Cenyuan Zhang, Xiaoqing Zheng, Xuanjing Huang

- 通常の連合学習は異なるエッジデバイス間でデータのプライバシーを保護
- 大規模言語モデル（LLMs）の開発には多くのデータと計算リソースが必要で、それは知的財産である
- 量子化されたモデルパラメータを分配することで、データとモデルの両方のプライバシーを保護
- LoRAを使った量子化戦略により通信コストを大幅に削減し、リソース効率の良い学習を実現

この方法で通信コストを削減しながら、データとモデルのプライバシーも守れるなんてすごいよね！未来の連合学習がもっと使いやすくなりそう、楽しみだな～。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.CL, cs.CR, **投稿日時:** 2024-06-16 15:23


- - -

### [Linkage on Security, Privacy and Fairness in Federated Learning: New Balances and New Perspectives](http://arxiv.org/abs/2406.10884)

**連合学習におけるセキュリティ、プライバシー、公平性の結びつき：新たなバランスと新しい視点**

Linlin Wang, Tianqing Zhu, Wanlei Zhou, Philip S. Yu

- モバイルデバイスや銀行システム、ヘルスケア、IoTシステムで連合学習が急速に普及中
- この研究では、プライバシー漏洩、セキュリティ脅威、公平性の相互関係を詳述
- 公平性とプライバシー、セキュリティと勾配共有のトレードオフを指摘
- 公平性がプライバシーとセキュリティ間の橋渡しとして機能し得る

プライバシーと公平性のバランスなんて難しそうだけど、めっちゃおもしろいね！未来の連合学習モデルがどんどん進化して、安全かつ公平になるといいな。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.CR, cs.DC, **投稿日時:** 2024-06-16 10:31


- - -

### [Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions](http://arxiv.org/abs/2406.10861)

**連合学習における知識蒸留：長年の課題と新たな解決策に関する調査**

Laiqiao Qin, Tianqing Zhu, Wanlei Zhou, Philip S. Yu

- 連合学習はデータをローカライズしたまま複数のクライアントがモデルを訓練する分散型プライバシー保護機械学習である
- 課題にはプライバシーリスク、データ異質性、通信のボトルネック、およびシステムの異質性が含まれる
- 知識蒸留はモデル圧縮および強化アルゴリズムとして2020年以降広く適用されている
- 知識蒸留の連合学習における適用例を包括的に調査し解決策を提示

知識蒸留が連合学習の課題をどう克服できるかを明らかにするなんて面白そう！未来の研究方向も示して、これからの進展が楽しみだね。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-06-16 09:12


- - -

### [Federated Learning Optimization: A Comparative Study of Data and Model Exchange Strategies in Dynamic Networks](http://arxiv.org/abs/2406.10798)

**連合学習最適化：動的ネットワークにおけるデータとモデル交換戦略の比較研究**

Alka Luqman, Yeow Wei Liang Brandon, Anupam Chattopadhyay

- 大規模な動的連合学習において、データとモデルのどちらを共有するべきかが重要な課題
- デバイス間での生データ、合成データ、（部分）モデル更新の交換を比較
- 基礎モデルの文脈でこれらの交換戦略の影響を詳細に調査
- 時間制限のある知識移転効率が最大で9.08%異なることが判明

効率的なデータとモデル交換ってどれがいいんだろうね？この研究で明らかになった9.08%の違い、実用的にどのくらい役立つのか気になるな～



**トピック:** [連合学習](../../fl), [合成データ](../../sd), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-06-16 03:46


- - -

### [Emerging Safety Attack and Defense in Federated Instruction Tuning of Large Language Models](http://arxiv.org/abs/2406.10630)

**連合指示調整における大規模言語モデルの新たな安全攻撃と防御**

Rui Ye, Jingyi Chai, Xiangrui Liu, Yaodong Yang, Yanfeng Wang, Siheng Chen

- 連合学習（FL）は、複数の当事者がデータを共有せずに協力して大規模言語モデル（LLM）を微調整できる
- この研究では、FedITの安全調整の脆弱性を暴露し、シンプルだが効果的な攻撃手法を提案
- 悪意のあるクライアントは自動生成された攻撃データを使い、FedITシステムの安全性を損なう
- 提案された防御法により、多くの既存のFL防御法が効果的でない中で、攻撃されたLLMの安全性を大幅に向上

安全性の調整がこんなに簡単に損なわれるのって怖いね。でも、新しい防御法がちゃんと対策できるなら、もっと安心して使えそう。楽しみだな。

**Comment:** 18 pages

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CL, cs.AI, cs.CR, cs.MA, **投稿日時:** 2024-06-15 13:24


- - -

### [Federated Neural Radiance Field for Distributed Intelligence](http://arxiv.org/abs/2406.10474)

**分散インテリジェンスのための連合ニューラルラジアンスフィールド**

Yintian Zhang, Ziyu Shao

- ARやVR応用における新規視点合成（NVS）の重要性
- Neural Radiance Field（NeRF）のNVSタスクでの性能優位性
- データプライバシーを保持しつつ異なるデータ所有者の画像を活用するFedNeRF
- 機能的多様でリソース豊富な連合学習テストベッドの構築とFedNeRFアルゴリズムの実験

FedNeRFってめっちゃおもしろそう！これで、異なる場所にあるデータでも効率的に活用できちゃうんだって。新しいAR/VR体験がさらに進化するかもね！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.DC, **投稿日時:** 2024-06-15 02:32


- - -

### [Byzantine-Robust Decentralized Federated Learning](http://arxiv.org/abs/2406.10416)

**ビザンチン耐性分散型連合学習**

Minghong Fang, Zifan Zhang, Hairi, Prashant Khanduri, Jia Liu, Songtao Lu, Yuchen Liu, Neil Gong

- 連合学習（FL）は、複数のクライアントがプライベートな訓練データを公開せずに協力して機械学習モデルを訓練する技術である
- 従来の連合学習は中央サーバーを介して調整されるが、スケーラビリティと信頼依存性の問題がある
- 分散型連合学習（DFL）はサーバーレスかつピアツーピア方式でモデルを共同訓練するが、完全に分散型であるため攻撃に弱い
- 新アルゴリズム「BALANCE」は、クライアントがローカルモデルを基準に受信モデルが悪意かどうかを判断し、防御力と収束保証を提供する

これは革新的だね！完全ピアツーピアでの連合学習、未来のAIインフラに道を開く革新かも！

**Comment:** To appear in ACM Conference on Computer and Communications Security   2024 (CCS '24)

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CR, cs.DC, cs.LG, **投稿日時:** 2024-06-14 21:28


- - -

### [Federated Learning with Flexible Architectures](http://arxiv.org/abs/2406.09877)

**柔軟なアーキテクチャを用いた連合学習**

Jong-Ik Park, Carlee Joe-Wong

- 従来の連合学習はクライアントの計算および通信能力のばらつきに対応できず非効率
- FedFAはクライアントごとに異なる幅と深さのモデルをトレーニング可能にする方法を提案
- レイヤーグラフティング技術を導入し、全クライアントの貢献を統一的にグローバルモデルに統合
- スケーラブルな集約法により重みの差異を管理し、従来の手法より優れた性能およびバックドア攻撃の耐性向上

多様なデバイスに対応するための工夫が面白いね。バックドア攻撃に強いってのも、これからの普及に大事だよね!



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, cs.DC, **投稿日時:** 2024-06-14 09:44


- - -

### [Federated Learning driven Large Language Models for Swarm Intelligence: A Survey](http://arxiv.org/abs/2406.09831)

**連合学習による大規模言語モデルの群知能への応用：調査**

Youyang Qu

- 連合学習（FL）は大規模言語モデル（LLMs）の訓練において、データプライバシーと分散化の課題に対応
- 機械学習の文脈での「忘却の権利」に対応するため、モデルから個々のデータ貢献を安全に削除する「機械アンラーニング」に注目
- 摂動技術、モデル分解、インクリメンタル学習などの効果的なアンラーニング戦略の調査
- 最近の文献からケーススタディや実験結果を基に、実際のシナリオでの方法の有効性と効率性を評価

連合学習でデータプライバシーを守りながらモデルを訓練できるのはすごいよね！これからも倫理と技術の両立にもっと発展しそう。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, cs.CL, cs.NE, **投稿日時:** 2024-06-14 08:40


- - -

### [Privacy-preserving Quantification of Non-IID Degree in Federated Learning](http://arxiv.org/abs/2406.09682)

**連合学習における非IID度のプライバシー保護定量化**

Yuping Yan, Yizhi Wang, Yingchao Yu, Yaochu Jin

- 連合学習は、生データの共有を避けつつ複数の協力者で機械学習を行う方法だが、非IIDデータセットが課題。
- 非IIDデータが精度低下や効率減少を引き起こし、実装の妨げとなっている。
- 初めて累積分布関数（CDF）を用いた非IID度の定量的定義を提案。
- 完全準同型暗号を用いて非IID度を推定し、CIFAR-100データセットで効果を検証。

新しいアプローチで連合学習がもっと効率化されそう！これでクライアント間のデータの違いも解消しやすくなるね。

**Comment:** 8 pages, 8 figures, FL@FM-IJCAI'24

**トピック:** [連合学習](../../fl), [準同型暗号](../../he), **カテゴリ:** cs.CR, **投稿日時:** 2024-06-14 03:08


- - -

### [Heterogeneous Federated Learning with Convolutional and Spiking Neural Networks](http://arxiv.org/abs/2406.09680)

**畳み込みおよびスパイキングニューラルネットワークを用いた異構連合学習**

Yingchao Yu, Yuping Yan, Jisong Cai, Yaochu Jin

- 連合学習（FL）は分散データ上でモデルを訓練しながらデータのプライバシーを守る
- 現在のFLシステムは同種のモデルを前提とするが、実際には異なるAIモデルを使用することが増えてきた
- 異なるモデルを使うことで特定のタスクや要件に適応しやすく、エッジコンピューティングプラットフォームの柔軟性を高める
- CNNとSNNを組み合わせた融合フレームワークが最良の性能を示す

複数のモデルが絡むと競争抑圧が見られるんだって、面白い！これからのエッジコンピューティング技術がどう進化するのか楽しみになるね。

**Comment:** 8 pages, 5 figures, FL@FM-IJCAI'24

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-06-14 03:05
