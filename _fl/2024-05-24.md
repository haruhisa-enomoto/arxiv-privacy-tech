---
title: 連合学習 (2024-05-24 ~ 2024-05-30)
date: 2024-05-24
---

連合学習に関する論文まとめ (2024-05-24 ~ 2024-05-30)


- - -

### [CAFe: Cost and Age aware Federated Learning](http://arxiv.org/abs/2405.15744)

**CAFe: コストとエイジを考慮した連合学習**

Sahan Liyanaarachchi, Kanchana Thilakarathna, Sennur Ulukus

- FLモデルの訓練進行を確保するために、少なくとも$M$クライアントのローカル勾配の報告を待つが、期限内に報告がない場合はラウンドが失敗と見なされ、再スタートが必要
- 小さい$T$（締め切り）と大きい$M$（クライアント数）は、多くの失敗ラウンドと過剰な通信および計算リソースの無駄を招く
- 大きい$T$はラウンド時間の延長、小さい$M$はノイズの多い勾配を生じるため、バランスが必要
- クライアントの平均エイジが理論的収束境界に影響し、収束のメトリックとして使用可能であり、$M$と$T$の選定を行う分析的手法を提供

この研究、クライアントのエイジを使って効率的に連合学習の収束を目指しているのが面白い！さらに、通信コスト削減もできる方法だから実用的かもね。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, cs.IT, math.IT, **投稿日時:** 2024-05-24 17:41


- - -

### [Harnessing Increased Client Participation with Cohort-Parallel Federated Learning](http://arxiv.org/abs/2405.15644)

**クライアント参加の向上を活用したコホート並列連合学習**

Akash Dhasade, Anne-Marie Kermarrec, Tuan-Anh Nguyen, Rafael Pires, Martijn de Vos

- 連合学習では、多くのノードが参加するほど、個々のモデル更新の効果が減少
- ネットワークをコホートに分割して、各コホートで独立に学習する新手法CPFLを提案
- コホートごとのモデルを知識蒸留とクロスドメインのラベルなしデータセットで統合
- 実験では、従来のFLと比較し、トレーニング時間とリソース使用量を大幅に削減しつつ精度低下を最小限に抑えた

"連合学習のさらなる最適化なんて面白そう！これでより効率的なモデルが期待できるね！"



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-05-24 15:34


- - -

### [Federated Behavioural Planes: Explaining the Evolution of Client Behaviour in Federated Learning](http://arxiv.org/abs/2405.15632)

**連合行動プレーン: 連合学習におけるクライアント行動の進化を解明する**

Dario Fenoglio, Gabriele Dominici, Pietro Barbiero, Alberto Tonda, Martin Gjoreski, Marc Langheinrich

- 連合学習はプライバシーリスクを減らすが、クライアント行動の理解が依然として課題
- 連合行動プレーン（FBP）を提案し、クライアントの行動を予測性能と意思決定の2つの観点から解析
- FBPによりクライアントの進化状態を示し、似た行動を持つクライアントを識別可能
- FBPのパターンを活用し、悪意あるクライアントモデルを検出する技術（連合行動シールド）を提案

クライアントの行動を可視化するなんておもしろそう！連合行動シールドの効果がどれくらいあるのか気になるね。

**Comment:** [v1] Preprint (24 pages)

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-05-24 15:17


- - -

### [DAGER: Exact Gradient Inversion for Large Language Models](http://arxiv.org/abs/2405.15586)

**DAGER: 大規模言語モデルのための厳密な勾配逆転手法**

Ivo Petrov, Dimitar I. Dimitrov, Maximilian Baader, Mark Niklas Müller, Martin Vechev

- 連合学習は複数のクライアントから計算された勾配を集約し、プライベートデータを共有せずに協力的な学習を実現する
- 過去の研究では、サーバーが勾配逆転攻撃を用いてデータを回復できることが示されていたが、テキストでは精度が低かった
- DAGERは、クライアントデータのトークンシーケンスを効率的に検出し、全バッチのテキストを正確に回復する初のアルゴリズム
- DAGERは、大規模言語モデルに対して高速（同バッチサイズで20倍）、スケーラブル（10倍大きいバッチ）、高品質な再構築（ROUGE-1/2 > 0.99）を実現

大規模言語モデルのテキストデータがそんなに正確に回復できるなんてびっくり！これからのプライバシー保護、どうなるんだろうね？



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, I.2.7; I.2.11, **投稿日時:** 2024-05-24 14:14


- - -

### [Thinking Forward: Memory-Efficient Federated Finetuning of Language Models](http://arxiv.org/abs/2405.15551)

**先を見据えた考え: メモリ効率の良い連合学習による言語モデルの微調整**

Kunjal Panchal, Nisarg Parikh, Sunav Choudhary, Lijun Zhang, Yuriy Brun, Hui Guan

- 従来のバックプロパゲーションを用いた微調整は中間活性化層が大きなメモリを消費し、リソース制約のあるデバイスには困難
- スプライ（Spry）は、各クライアントがフォワードモード自動微分を用いて訓練し、メモリ使用量を低減しつつ高精度と高速収束を実現
- 同質データ分布ではグローバル勾配の無偏推定を理論的に確認、不均質性が増すと推定のバイアスも増加
- 1.4-7.1倍のメモリ削減効果と、1.2-20.3倍の収束時間短縮、最先端の0次手法と比べて5.2-13.5%の精度向上を実証

メモリ効率がめっちゃ良くなるってすごいね！こんな技術でどんどんFLの用途が広がりそうだよ、スマホでもどんどん使えちゃうね！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, **投稿日時:** 2024-05-24 13:37


- - -

### [Unlearning during Learning: An Efficient Federated Machine Unlearning Method](http://arxiv.org/abs/2405.15474)

**学習中の忘却：効率的な連合機械学習忘却方法**

Hanlin Gu, Gongxi Zhu, Jie Zhang, Xinyuan Zhao, Yuxing Han, Lixin Fan, Qiang Yang

- 連合学習は分散機械学習の新たなパラダイムとして注目
- 従来の連合機械忘却は時間がかかり、実用性に欠ける
- FedAUは軽量な補助モジュールと直線的操作で効率化
- FedAUは個別データ、クラス、クライアントレベルの忘却をサポート

実際のFL環境で使える効率的な忘却方法が出てきたなんて、ワクワクするね！これからもっとデータプライバシーが進化していくって感じがするよ。

**Comment:** Accepted by IJCAI 2024

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-05-24 11:53


- - -

### [FedCal: Achieving Local and Global Calibration in Federated Learning via Aggregated Parameterized Scaler](http://arxiv.org/abs/2405.15458)

**FedCal: 連合学習における集約パラメータスケーラーを用いたローカルおよびグローバルキャリブレーションの実現**

Hongyi Peng, Han Yu, Xiaoli Tang, Xiaoxiao Li

- 連合学習におけるデータの多様性がモデルのキャリブレーションにチャレンジをもたらす
- 現存の連合学習手法はキャリブレーションにおいて最適ではないと理論分析から示される
- FedCalアプローチはローカルおよびグローバルキャリブレーションを重視し、クライアント固有スケーラーを利用
- 実験ではFedCalがベースラインより47.66%もグローバルキャリブレーションエラーを削減

Federated Learningって、データがバラバラでも協力して学習できるのが魅力的だよね。このFedCalの方法なら、もっと精度よくなるなんてワクワクするね！

**Comment:** This paper has been accepted by ICML'24

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-05-24 11:33


- - -

### [Towards Client Driven Federated Learning](http://arxiv.org/abs/2405.15407)

**クライアント主導型連合学習に向けて**

Songze Li, Chenqing Zhu

- 従来の連合学習はサーバー中心で、クライアントの非同期なニーズに対応できない課題がある
- CDFLはクライアントが独立してモデルを更新し、サーバーがクラスター分布を管理
- CDFLにより、時間変動するクライアントのデータ分布に迅速に適応可能となる
- クライアントへの送信モデルを単一にすることで計算効率と推定精度向上を実現

これはすごい！クライアント主体でプライバシー保護できるなんて未来の技術にワクワクしちゃう。CDFSが普及すれば色んな問題が解決できそうね！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-05-24 10:17


- - -

### [Transformer-based Federated Learning for Multi-Label Remote Sensing Image Classification](http://arxiv.org/abs/2405.15405)

**トランスフォーマーベースの連合学習によるマルチラベルリモートセンシング画像分類**

Barış Büyüktaş, Kenneth Weitzel, Sebastian Völkers, Felix Zailskas, Begüm Demir

- 連合学習はクライアントの訓練データにアクセスせずにモデルを学習するが、非独立同分布データの影響で収束が困難
- 最先端のトランスフォーマー（MLP-Mixer, ConvMixer, PoolFormer）が非独立同分布データの問題にどう対処するかを調査
- ResNet-50と比較し、異なる非独立同分布レベルでの頑健性、ローカルトレーニングの複雑さ、集約の複雑さを評価
- BigEarthNet-S2ベンチマークでの実験結果は、一般化能力が向上するがローカルトレーニングと集約の複雑さも増加

連合学習ってなんかおもしろいよね！トランスフォーマーのモデル選定についても詳しく研究されていて、実用的なガイドラインが得られるのがいいなぁ。

**Comment:** Accepted at IEEE International Geoscience and Remote Sensing   Symposium (IGARSS) 2024. Our code is available at   https://git.tu-berlin.de/rsim/FL-Transformer

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CV, **投稿日時:** 2024-05-24 10:13


- - -

### [Decaf: Data Distribution Decompose Attack against Federated Learning](http://arxiv.org/abs/2405.15316)

**Decaf: 連合学習に対するデータ分布分解攻撃**

Zhiyang Dai, Chunyi Zhou, Anmin Fu

- 既存のプライバシー攻撃技術とは異なり、連合学習に対する新しいプライバシー脅威を提案
- Decafの攻撃により、FLサーバーは被害者のデータ分布を高い精度でプロファイル可能
- 5つのベンチマークデータセットで実験し、多様なモデルアーキテクチャでDecafの有効性を検証
- Nullクラスがない場合、Decafのデータ分解精度は5%未満で、Nullクラス判定精度は100%

これは超面白そう！連合学習の仕組みを本当に分かってるFLサーバーだと、ついにこっそりデータ分析ができちゃうんだね。未来のAIプライバシー対策もますます重要になりそう。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.CR, **投稿日時:** 2024-05-24 07:56


- - -

### [Leakage-Resilient and Carbon-Neutral Aggregation Featuring the Federated AI-enabled Critical Infrastructure](http://arxiv.org/abs/2405.15258)

**漏洩耐性およびカーボンニュートラルな連合AI支援型重要インフラ集約手法**

Zehang Deng, Ruoxi Sun, Minhui Xue, Sheng Wen, Seyit Camtepe, Surya Nepal, Yang Xiang

- 連合学習技術を活用する重要インフラでも、勾配最適化を通じてデータ再構築攻撃のリスクが存在
- 新たな圧縮差分プライベート集約（CDPA）で漏洩耐性、通信効率、カーボンニュートラルを実現
- ランダムビット反転メカニズムによりノイズの分散を拡大し、差分プライバシー保護とエネルギー節約を両立
- CDPAは通信コストを半減させつつモデルの有用性を保持し、最新のデータ再構築攻撃を効果的に防御

省エネとプライバシー保護が同時に進められるなんて、未来のインフラにとって嬉しいポイントだよね。二酸化炭素排出量も大幅に減らせるから、環境面でも優しそう！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CR, **投稿日時:** 2024-05-24 06:35


- - -

### [RFLPA: A Robust Federated Learning Framework against Poisoning Attacks with Secure Aggregation](http://arxiv.org/abs/2405.15182)

**RFLPA：セキュアアグリゲーションを用いた毒素攻撃に対する強靭な連合学習フレームワーク**

Peihua Mai, Ran Yan, Yan Pang

- 連合学習はデータ共有なしで複数デバイスがモデルを共同訓練するが、プライバシー漏洩と毒素攻撃の脆弱性がある
- 毒素攻撃防御策は平文でのローカル更新の解析に依存しているため、セキュアアグリゲーションと互換性がない
- 提案するRFLPAフレームワークはセキュアアグリゲーションプロトコルに基づき、ローカル更新とサーバー更新のコサイン類似度を計算して強靭なアグリゲーションを実施
- 実験結果では、RFLPAは最先端の防御方法（BREA）と比較して通信および計算オーバーヘッドを75%以上削減し、競争力のある精度を維持

毒素攻撃もプライバシーも一挙に解決できるなんて、すごく斬新！これが普及したらデータの扱い方が一段と安心になりそう。

**Comment:** 22 pages

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CR, cs.AI, E.4, **投稿日時:** 2024-05-24 03:31
