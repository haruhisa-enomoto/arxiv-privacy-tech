---
title: 連合学習 (2024-10-11 ~ 2024-10-17)
date: 2024-10-11
---

連合学習に関する論文まとめ (2024-10-11 ~ 2024-10-17)


- - -

### [Evaluating Federated Kolmogorov-Arnold Networks on Non-IID Data](http://arxiv.org/abs/2410.08961)

**非独立同分布データにおける連合コルモゴロフ・アーノルドネットワークの評価**

Arthur Mendonça Sasse, Claudio Miceli de Farias

- 連合コルモゴロフ・アーノルドネットワーク（F-KANs）の評価はまだ初期段階にある
- Bスプラインとラジアル基底関数を用いたKANsとMLPsを比較
- MNIST分類タスクで100名のクライアントによる非独立同分布パーティションを用いた
- スプライン-KANsはMLPsと同等の精度を半分のラウンドで達成し、計算時間はやや増加

F-KANsは精度が高いのに効率も良さそう！非独立データでの連合学習って、便利そうな未来を感じちゃうね。

**Comment:** 10 pages, 5 figures, for associated code see   https://github.com/artsasse/fedkan

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, **投稿日時:** 2024-10-11 16:30


- - -

### [The Effect of Personalization in FedProx: A Fine-grained Analysis on Statistical Accuracy and Communication Efficiency](http://arxiv.org/abs/2410.08934)

**FedProxにおけるパーソナライゼーションの効果：統計精度と通信効率の詳細分析**

Xin Yu, Zelin He, Ying Sun, Lingzhou Xue, Runze Li

- FedProxは正則化を通じてモデルのパーソナライゼーションを可能にする連合学習法
- 正則化の強度を決めることは難しく、不適切な設定は精度を低下させうるリスクがある
- 正則化が統計精度に与える影響を分析し、適切な強度設定の理論的ガイドラインを提供
- 個別化は通信複雑度を低下させ、計算コストの追加なしに効率を向上させることが示された

FedProxってただの連合学習じゃないんだねー！正規化をうまく設定することで、通信も効率化できるなんてすごいね！どんなデータセットでもうまくいくなんて、未来の技術の一歩を感じちゃう！



**トピック:** [連合学習](../../fl), **カテゴリ:** stat.ML, cs.DC, cs.LG, math.ST, stat.CO, stat.TH, **投稿日時:** 2024-10-11 16:00


- - -

### [Federated Learning in Practice: Reflections and Projections](http://arxiv.org/abs/2410.08892)

**実践における連合学習: 振り返りと展望**

Katharine Daly, Hubert Eichner, Peter Kairouz, H. Brendan McMahan, Daniel Ramage, Zheng Xu

- 連合学習は複数のエンティティがデータを交換せずに共同でモデルを学習する技術
- GoogleやApple、Metaのシステムは、連合学習の実際の適用例を示している
- 課題はサーバー側の差分プライバシー保証の検証や異種デバイス間の調整
- 新たな連合学習フレームワークはプライバシー原則を優先し、未来の課題に対応

連合学習ってすごいね！みんなでデータをシェアせずに、匿名で一緒に学習できるなんて、まるで未来の技術みたい。これからさらに進化していくと、もっと便利なことができそうでワクワクするね。



**トピック:** [連合学習](../../fl), [差分プライバシー](../../dp), [TEE](../../tee), **カテゴリ:** cs.LG, cs.AI, cs.CR, **投稿日時:** 2024-10-11 15:10


- - -

### [Unlocking FedNL: Self-Contained Compute-Optimized Implementation](http://arxiv.org/abs/2410.08760)

**FedNLの解明: 自律的かつ計算最適化された実装**

Konstantin Burlachenko, Peter Richtárik

- 連合学習は分散型で学習モデルを共同訓練し、データ共有を不要にするパラダイム
- FedNLアルゴリズムは第二次手法をFLに適用する進展を示したがプロトタイプには課題がある
- FedNL-LS, FedNL-PPをシングル・マルチノード設定で実装し、壁時計時間を1000倍短縮
- 実用的な圧縮器を提案し、FedNLの理論を実現する新しい圧縮手法を導入

連合学習の現実的なチューニングってすごくクール！プロトタイプの_TIME短縮_により実用性が飛躍的にアップしそうだね。ますます連合学習の普及が進みそうで楽しみだなぁ。

**Comment:** 55 pages, 12 figures, 12 tables

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, cs.MS, cs.PF, math.OC, G.4; C.3; I.2.11, **投稿日時:** 2024-10-11 12:19


- - -

### [Gradients Stand-in for Defending Deep Leakage in Federated Learning](http://arxiv.org/abs/2410.08734)

**連合学習における深層リーク防御のための勾配の代替案**

H. Yi, H. Ren, C. Hu, Y. Li, J. Deng, X. Xie

- 連合学習は、感度の高いデータをローカルにしてモデル勾配のみをサーバーに送信しプライバシーを保護する手法。
- FLの勾配交換に脆弱性が指摘されており、新手法「AdaDefense」でこれを防ぐ。
- ローカル勾配の代わりにスタンドインを使用し、勾配リークを防ぎながらモデルの性能を維持する。
- 理論的枠組みによってプライベート情報の漏えいを防ぐ手法の有効性を立証し、ベンチマーク実験でモデルの一貫性と安全性を確認。

勾配リークってそんなに危ないんだね！でも、「AdaDefense」でそれを防ぎつつも性能も落ちないなんて、すごく興味深い！これからのプライバシー技術に大きな貢献しそうだよね。❤️



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.CV, **投稿日時:** 2024-10-11 11:44


- - -

### [DistDD: Distributed Data Distillation Aggregation through Gradient Matching](http://arxiv.org/abs/2410.08665)

**DistDD: 勾配マッチングによる分散データ蒸留集約**

Peiran Wang, Haohan Wang

- DistDDは連合学習での通信を減らし、クライアント上で直接データを蒸留する手法
- 従来のモデル更新を要さず、一度の蒸留でグローバルなデータセットを抽出しプライバシーを確保
- DistDDで得たデータは、連合学習のパラメータ調整やニューラルアーキテクチャ検索に活用できる
- 実験結果で非独立同分布や誤ラベルデータにも頑強で、コミュニケーションコスト削減を証明

DistDDってすごく未来が見えるね！データを効率的に使って、もっと賢いAIモデルができそうな予感！私たちの身近なところでも役立つ日が楽しみ〜。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, **投稿日時:** 2024-10-11 09:43


- - -

### [GAI-Enabled Explainable Personalized Federated Semi-Supervised Learning](http://arxiv.org/abs/2410.08634)

**GAIを活用した説明可能な個別化連合半教師あり学習**

Yubo Peng, Feibo Jiang, Li Dong, Kezhi Wang, Kun Yang

- 連合学習はモバイルユーザーのAIモデル訓練に使われるが、ラベル不足やデータの非IID性、説明不能性などの課題がある
- 提案するXPFLフレームワークは、生成AIを用いた個別化連合半教師あり学習GFedでローカルFLモデルを強化
- グローバル集約では、ローカルとグローバルFLモデルの知識を特定の割合で融合し、個別化を保ちながら知識を共有
- FLモデルの説明性を向上させるため、決定木とt-SNEを活用してモデルの入力と出力や集約前後の可視化を実現

生成AIを使ってラベル不足を補うアイデア、めっちゃ新しい！モデルの融合方法もよく考えられてて、個別化のまま力を合わせる感じがいいと思った～。可視化の部分も面白そう、データが目に見えるってすごいよね。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.IT, math.IT, **投稿日時:** 2024-10-11 08:58


- - -

### [Accelerated Distributed Stochastic Non-Convex Optimization over Time-Varying Directed Networks](http://arxiv.org/abs/2410.08508)

**時間変動する有向ネットワークにおける加速分散確率非凸最適化**

Yiyue Chen, Abolfazl Hashemi, Haris Vikalo

- 信号処理やコンピュータビジョン、自然言語処理など分散学習システムの応用で注目される。
- 時間変動する有向ネットワークをモデル化、通信遅延やストラグラー効果を考慮した分布設定。
- 勾配追跡とモメンタム付き確率的勾配降下法を用いて、非凸最適化問題を解決するアルゴリズムを提案。
- 提案手法は既存手法を上回る性能を示し、様々な学習タスクで有効であることを実証。

分散学習システムが活躍する未来がますます近づいてきた感じ！この手法でさらに色んなタスクが効率よくできちゃうのかな？MNISTやCIFAR-10みたいな有名な学習タスクで試されてるところもすごく興味深いね！

**Comment:** This work has been accepted at IEEE Transactions on Automatic Control

**トピック:** [連合学習](../../fl), **カテゴリ:** eess.SY, cs.SY, **投稿日時:** 2024-10-11 04:18
