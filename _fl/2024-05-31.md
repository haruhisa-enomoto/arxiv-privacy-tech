---
title: 連合学習 (2024-05-31 ~ 2024-06-06)
date: 2024-05-31
---

連合学習に関する論文まとめ (2024-05-31 ~ 2024-06-06)


- - -

### [R-CONV: An Analytical Approach for Efficient Data Reconstruction via Convolutional Gradients](http://arxiv.org/abs/2406.04227)

**R-CONV: 畳み込み勾配を通じた効率的なデータ再構築のための分析アプローチ**

Tamer Ahmed Eltaras, Qutaibah Malluhi, Alessandro Savino, Stefano Di Carlo, Adnan Qayyum, Junaid Qadir

- 連合学習は、勾配共有を利用してプライバシーを保護しつつ分散データから学習する方法
- 畳み込み層に対しては効果が低いが、それでも勾配から入力データを再構築可能
- ReLUなどの完全な逆関数を持たない場合でも、勾配から訓練サンプルを再構築できる
- 既存の分析手法は勾配攻撃のリスク評価が不正確で、実際には報告の5%未満の制約で攻撃可能

勾配からデータが漏れちゃうことを考えると、連合学習も完全じゃないんだね。でも、新しい手法でさらに深掘りできるのは面白そう！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.CR, cs.CV, **投稿日時:** 2024-06-06 16:28


- - -

### [Federated TrustChain: Blockchain-Enhanced LLM Training and Unlearning](http://arxiv.org/abs/2406.04076)

**連合信頼チェーン：ブロックチェーン強化されたLLM訓練と忘却**

Xuhan Zuo, Minghao Wang, Tianqing Zhu, Lefeng Zhang, Dayong Ye, Shui Yu, Wanlei Zhou

- LLMの訓練には多くの新しいデータが必要であり、公開データが枯渇する問題がある
- 連合学習が新しい解決策として登場し、プライベートデータをLLMグローバルモデルに貢献させる
- ブロックチェーン技術を利用し、貢献内容の改ざん防止記録と透明性の確保を提案する
- Hyperledger Fabricを統合し、安全性、透明性、および検証可能性を高める

連合学習とブロックチェーンってすごいね！これでLLMの透明性も確保できるし、もっと公平で便利になりそう。楽しみ～😘

**Comment:** 16 pages, 7 figures,

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CR, **投稿日時:** 2024-06-06 13:44


- - -

### [FedPylot: Navigating Federated Learning for Real-Time Object Detection in Internet of Vehicles](http://arxiv.org/abs/2406.03611)

**FedPylot: インターネット・オブ・ビークルズにおけるリアルタイム物体検出のための連合学習のナビゲーション**

Cyprien Quéméneur, Soumaya Cherkaoui

- 自動運転車は機械学習に依存しており、エッジで生成される膨大なセンシングデータが有益
- 連合学習により道路利用者のプライバシーを保護し、通信負荷を軽減可能
- YOLOv7モデルを用いて、データの不均衡やコンセプトドリフトなどの問題に対処
- FedPylotは、ハイブリッド暗号化でサーバーとクライアントの通信を保護するプロトタイプ

新しい研究ってワクワクするよね！特に、IoVでの連合学習がこんなに実用的に使えるなんて、本当に未来の技術って感じるな。これは絶対に注目だよ！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.CV, cs.DC, **投稿日時:** 2024-06-05 20:06


- - -

### [Fantastyc: Blockchain-based Federated Learning Made Secure and Practical](http://arxiv.org/abs/2406.03608)

**Fantastyc: ブロックチェーンに基づいた連合学習のセキュアかつ実用的な実現**

William Boitier, Antonella Del Pozzo, Álvaro García-Pérez, Stephane Gazut, Pierre Jobic, Alexis Lemaire, Erwan Mahe, Aurelien Mayoue, Maxence Perion, Deepika Singh, Tuanir Franca Rezende, Sara Tucci-Piergiovanni

- 連合学習は、複数のクライアントが中央サーバの調整の下で協力して機械学習モデルを訓練する分散型フレームワークである
- ブロックチェーンベースの連合学習アプローチは完全な分散型解決策と追跡性を保証するが、依然として整合性、機密性、スケーラビリティにおける課題がある
- Fantastycはこれらの課題を同時に解決するために設計されたものである
- 本研究では、これらの未解決の課題に取り組む新しいソリューションを提案する

Fantastycがどんな風に実際の課題を解決するのか気になる！今後の実用化が楽しみだね！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CR, cs.DC, **投稿日時:** 2024-06-05 20:01


- - -

### [Noise-Aware Algorithm for Heterogeneous Differentially Private Federated Learning](http://arxiv.org/abs/2406.03519)

**異種差分プライバシー連合学習のためのノイズ対応アルゴリズム**

Saber Malekmohammadi, Yaoliang Yu, Yang Cao

- 連合学習システムは高効用と厳密なデータプライバシーを目指している
- クライアント毎に異なるプライバシー要件があると、効用が低下する問題がある
- 提案されたRobust-HDPは、クライアントのモデル更新における真のノイズレベルを効果的に推定し、集約モデルのノイズレベルを大幅に低減する
- Extensiveな実験結果と理論解析により、Robust-HDPの効果が確認された

クライアントに信頼をおかなくて済むのっておもしろいよね。さまざまなデータ量でも適応できるのすごく未来志向だよ！

**Comment:** Proceedings of the 41 st International Conference on Machine   Learning, Vienna, Austria. PMLR 235, 2024

**トピック:** [連合学習](../../fl), [差分プライバシー](../../dp), **カテゴリ:** cs.LG, cs.CR, cs.DC, **投稿日時:** 2024-06-05 17:41


- - -

### [Buffered Asynchronous Secure Aggregation for Cross-Device Federated Learning](http://arxiv.org/abs/2406.03516)

**クロスデバイス連合学習のためのバッファード非同期セキュアアグリゲーション**

Kun Wang, Yi-Rui Yang, Wu-Jun Li

- 非同期連合学習（AFL）はデバイスの異質性に対処する有効な方法である
- 従来のセキュアアグリゲーションプロトコルは同期アグリゲーションに基づいており、AFLと互換性がない
- 提案するBASAは、各ユーザーがサーバーと1回の非同期通信でセキュアアグリゲーションを実現できる
- BASAを用いたAFLは、追加のハードウェア要件なしでトレーニング効率とスケーラビリティが向上する

この研究、画期的だね！特にユーザーの負担を最小限に抑えつつ、効率を上げられるところがすごく魅力的💡



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CR, cs.AI, cs.LG, **投稿日時:** 2024-06-05 16:39


- - -

### [Towards Federated Domain Unlearning: Verification Methodologies and Challenges](http://arxiv.org/abs/2406.03078)

**連合ドメイン消去に向けて：検証手法と課題**

Kahou Tam, Kewei Xu, Li Li, Huazhu Fu

- 連合学習は共同モデルトレーニングのための強力なツールであるが、RTBFの導入は新たな課題を生む
- 従来の連合学習の消去方法は、多ドメインシナリオでは不十分で、モデルの精度を低下させる
- 既存の方法は、ドメイン固有データの影響を無視するため、特に深層層での学習内容が消去される
- 新たな評価方法を提案し、ドメイン固有データの消去を正確に検証し、モデルの整合性と性能を維持

RTBFってどんなんだろう？必要だよね～。でも、複数の分野で使うなら、一筋縄ではいかないんだな。この論文、そこに挑戦するのめっちゃ面白そう！

**Comment:** 16 pages, 12 figures

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, **投稿日時:** 2024-06-05 09:05


- - -

### [FedStaleWeight: Buffered Asynchronous Federated Learning with Fair Aggregation via Staleness Reweighting](http://arxiv.org/abs/2406.02877)

**FedStaleWeight: 停滞の重み付けによるフェアな集約を実現するバッファード非同期連合学習**

Jeffrey Ma, Alan Tu, Yiling Chen, Vijay Janapa Reddi

- FLはプライバシーを保ちつつ分散データを活用するが、性能やスケーラビリティの課題がある
- 非同期FLはアプローチとして有望だが、収束保証や計算異質性に関する公平性が課題
- FedStaleWeightは平均停滞時間を利用して、非同期クライアント更新の公平な重み付けを実現
- 理論的収束保証を提供し、FedBuffと比較して公平性と収束速度の面で優れていることを実証

公平性と収束速度を両立させる新しい方法、気になるよね。FedStaleWeightの実践検証も楽しみ！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-06-05 02:52


- - -

### [Reducing Bias in Federated Class-Incremental Learning with Hierarchical Generative Prototypes](http://arxiv.org/abs/2406.02447)

**階層的生成プロトタイプによる連合増分学習におけるバイアスの低減**

Riccardo Salami, Pietro Buzzega, Matteo Mosconi, Mattia Verasani, Simone Calderara

- 連合継続学習（FCL）は、時間と共に進化するデータ分布に対処
- 増分バイアスとローカル分布へのバイアスが生じる問題を指摘
- 学習可能なプロンプトを使用して事前学習済みのバックボーンを微調整しバイアスを抑制
- 生成プロトタイプを活用し、グローバルモデルの予測バランスを改善し、精度が平均+7.9%向上

生成プロトタイプなんて斬新なアイデア！未来のデータ分析がもっと公平になるかもね、ワクワクするよね。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, **投稿日時:** 2024-06-04 16:12


- - -

### [Improved Modelling of Federated Datasets using Mixtures-of-Dirichlet-Multinomials](http://arxiv.org/abs/2406.02416)

**ディリクレ多項分布混合モデルを用いた連合データセットの改善モデリング**

Jonathan Scott, Áine Cahill

- 連合学習のトレーニングは中央集権型に比べると非常に遅く、実験やチューニングが困難
- サーバ側のプロキシデータでシミュレーションを行い、ハイパーパラメータチューニングを高速化
- 中央集権データセットをクライアントに分割する際、不適切な分割は実際の連合学習を正確に反映しない
- 提案するアルゴリズムは、真のクライアントの分布を学習し、シミュレーションクライアントを効率的に作成

複雑な連合学習の課題に挑戦してて面白いよね。データの分割方法が改善されると、いろんなデータ活用の可能性が広がりそう！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-06-04 15:27


- - -

### [FedDr+: Stabilizing Dot-regression with Global Feature Distillation for Federated Learning](http://arxiv.org/abs/2406.02355)

**FedDr+: グローバル特徴抽出の蒸留を用いた連合学習におけるドット回帰の安定化**

Seongyoon Kim, Minchan Jeong, Sungnyun Kim, Sungwoo Cho, Sumyeong Ahn, Se-Young Yun

- 連合学習 (FL) は、クライアント間の異種非独立同分布データで効果的なグローバルモデルやパーソナライズモデルを開発
- クライアントドリフトがFLの課題で、データの異質性が知識の集約を妨げる
- 新しいアルゴリズムFedDr+は、未観測クラスの表現を保持しつつドット回帰損失を使用してローカルモデルの整合性を強化
- frozen classifierと特徴蒸留メカニズムを併用することで寄与されたグローバルモデルを改善、既存手法を超える性能を実証

FedDr+って難しそうだけど、未観測クラスについての情報保持しながらローカルな整合性を取るって画期的！連合学習の未来がまた開けそうだね！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CV, cs.AI, cs.DC, cs.LG, **投稿日時:** 2024-06-04 14:34


- - -

### [One-Shot Federated Learning with Bayesian Pseudocoresets](http://arxiv.org/abs/2406.02177)

**ベイズ・ピソードコアセットを用いたワンショット連合学習**

Tim d'Hondt, Mykola Pechenizkiy, Robert Peharz

- 連合学習は通信コストが高く、サーバとクライアント間で高次元モデルパラメータを繰り返し通信する必要がある
- 提案手法はベイズアプローチを用い、ローカルクライアントの事後分布の積としてグローバル推論問題を解決し、ワンショット通信を実現
- ニューラルネット等の多峰性尤度を持つモデルに対して、クライアントが異なる事後モードを捉えるため、素朴なアプローチでは後部が崩壊する
- 分散型関数空間推論とベイズ・ピソードコアセット学習の関連性を示し、通信コストを最大二桁減少させ、予測性能も最新技術と競合することを実証

ワンショットで連合学習を実現するなんて、革新的だね！通信コストが二桁も減るのは未来のネットワーク負荷軽減に繋がるね。

**Comment:** 10 pages

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, cs.DC, stat.ML, **投稿日時:** 2024-06-04 10:14


- - -

### [Mixed-Precision Over-The-Air Federated Learning via Approximated Computing](http://arxiv.org/abs/2406.03402)

**近似計算による混合精度の無線連合学習**

Jinsheng Yuan, Zhuangkun Wei, Weisi Guo

- 既存のOTA-FL研究は均一なクライアント計算精度を前提としている課題がある
- エネルギー効率と計算効率のためにビット精度を調整する近似計算（AxC）を利用する提案
- サーバとクライアントの量子化性能を最適化し、異なるエッジコンピューティング能力に対応する
- 物理層のOTA集約と互換性のある異種の勾配解像度のOTA-FL変調スキームを開発

この研究すごーい！エネルギー効率を考えた新しいアプローチで、クライアントの性能を最大限に活かす未来が見えるね。早く試してみたいな。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, **投稿日時:** 2024-06-04 09:07


- - -

### [Parameterizing Federated Continual Learning for Reproducible Research](http://arxiv.org/abs/2406.02015)

**再現可能な研究のための連合継続学習のパラメータ化**

Bart Cox, Jeroen Galjaard, Aditya Shankar, Jérémie Decouchant, Lydia Y. Chen

- 連合学習システムは異質で常に変化する環境で進化し、性能に挑戦がある
- クライアントの学習タスクも時間と共に進化し、継続学習の統合が必要
- 複雑な学習シナリオを正確にキャプチャしエミュレートする実験ベストプラクティスを提案
- Freddieは完全に構成可能な連合継続学習フレームワークで、大規模なマシン上での展開が可能

連合学習と継続学習を組み合わせて、現場ですぐ活用できるってスゴイね！将来的には、もっと多くの分野での応用が期待できそう。

**Comment:** Preprint: Accepted at the 1st WAFL (Workshop on Advancements in   Federated Learning) workshop, ECML-PKDD 2023

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, I.2.11, **投稿日時:** 2024-06-04 06:54


- - -

### [A Comparative Study of Sampling Methods with Cross-Validation in the FedHome Framework](http://arxiv.org/abs/2406.01950)

**FedHomeフレームワークにおけるクロスバリデーションを活用したサンプリング手法の比較研究**

Arash Ahmadi, Sarah S. Sharif, Yaser M. Banad

- FedHomeは連合学習と生成畳み込みオートエンコーダを使用し、個人の自宅での健康モニタリングをプライバシー保護しながら向上させる
- 健康データのクラス不均衡が課題であり、転倒などの重大なイベントが過小評価されモデル性能に悪影響
- 6つのオーバーサンプリング技術（SMOTE、Borderline-SMOTE、Random OverSampler、SMOTE-Tomek、SVM-SMOTE、SMOTE-ENN）を評価
- SMOTE-ENNが最も安定した試験精度を示し、他のサンプラーに比べて信頼性と精度の向上が期待できる

SMOTE-ENNがハズレなさそうなのが安心！他の手法と比べてどれだけ実際に差がでるのか、自分でも試してみたくなるね。

**Comment:** 11 Figures

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, cs.CY, **投稿日時:** 2024-06-04 04:03


- - -

### [Efficient Data Distribution Estimation for Accelerated Federated Learning](http://arxiv.org/abs/2406.01774)

**加速連合学習のための効率的なデータ分布推定**

Yuanli Wang, Lei Huang

- 連合学習は、多数の分散エッジデバイス上でプライバシーを守る機械学習パラダイムである
- デバイスはリソースと訓練データが異質で、それによりデバイス選択が難しい
- 新たに効率的なデータ分布要約計算アルゴリズムを提案、その結果オーバーヘッドを削減
- 提案された解決策はデータ要約時間を最大30倍、クラスタリング時間を最大360倍削減した

フェデレーテッドラーニングっていっぱい聞くけど、こうやって具体的な改善方法を提案していくのが面白いよね。大規模な環境でどれくらい効率化できるか、未来が楽しみ！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.DC, cs.LG, **投稿日時:** 2024-06-03 20:33


- - -

### [Federated Learning-based Collaborative Wideband Spectrum Sensing and Scheduling for UAVs in UTM Systems](http://arxiv.org/abs/2406.01727)

**連合学習に基づくUAVのための協調的ワイドバンドスペクトラムセンシングおよびスケジューリング**

Sravan Reddy Chintareddy, Keenan Roach, Kenny Cheung, Morteza Hashemi

- UAVを二次利用者として、検出された「スペクトラムホール」を利用するデータ駆動型フレームワークを提案
- 学習データセット生成と連合学習アーキテクチャを統合し、空中信号からI/Qサンプルを取得
- 協調スペクトラム推論のための戦略を提案し、UTMエコシステムと互換性を持たせた
- 検出されたスペクトラムホールを動的に割り当てるために強化学習手法を活用

このフレームワークでUAVが効率的にスペクトラムを利用する未来にワクワクするね。現実に近いデータセット生成がすごくキーポイントになりそう！

**Comment:** This is a preprint version submitted to IEEE Transactions on Machine   learning in Communications and Networking. arXiv admin note: text overlap   with arXiv:2308.05036

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.MA, eess.SP, **投稿日時:** 2024-06-03 18:39


- - -

### [Asynchronous Multi-Server Federated Learning for Geo-Distributed Clients](http://arxiv.org/abs/2406.01439)

**地理分散クライアント向けの非同期マルチサーバ連合学習**

Yuncong Zuo, Bart Cox, Jérémie Decouchant, Lydia Y. Chen

- 既存の連合学習システムは単一サーバ使用で同期通信により効率が低下することがある
- 新提案の非同期マルチサーバFLアーキテクチャは、サーバとクライアントを常にアクティブに保つ
- 提案手法では、クライアントは最も近いサーバとだけ対話し、サーバ間も非同期で更新
- より高い精度を達成しつつ従来手法より61%少ない時間で収束する結果を示す

地理的に分散した環境での効率的な連合学習って、未来のIoTとかにぴったりじゃない？ぜんぶのデバイスがスムーズにつながるなんて、楽しみだね！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, I.2.11, **投稿日時:** 2024-06-03 15:29


- - -

### [Asynchronous Byzantine Federated Learning](http://arxiv.org/abs/2406.01438)

**非同期ビザンチン連合学習**

Bart Cox, Abele Mălan, Jérémie Decouchant, Lydia Y. Chen

- 地理的に分散したクライアントが、サーバを通じてモデルを共同訓練する
- 非同期学習により、遅いクライアントや異種ネットワークでも速度維持可能
- 補助サーバデータセットを使わず、過去の問題点を解決する新アルゴリズムを提案
- 画像とテキストのデータセットで、高速かつ高精度の学習が可能であることを確認

非同期ビザンチン連合学習なんて面白そう！ぶっちゃけ遅いクライアントに妨げられないのがすごく魅力的だよね。これからもっと多様なネットワークでも連合学習が活用できそう！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, I.2.11, **投稿日時:** 2024-06-03 15:29


- - -

### [Accelerating Heterogeneous Federated Learning with Closed-form Classifiers](http://arxiv.org/abs/2406.01116)

**閉形式分類器を用いた異種連合学習の加速**

Eros Fanì, Raffaello Camoriano, Barbara Caputo, Marco Ciccone

- 非IIDデータ分布に起因するクライアントドリフトとローカル解の偏りが、収束速度と精度に悪影響を及ぼす
- Federated Recursive Ridge Regression (Fed3R)を導入し、事前学習済みの特徴を利用してリッジ回帰分類器を閉形式で計算する
- Fed3Rは統計的異質性やクライアントのサンプリング順序に影響されず、通信と計算コストも競合技術に比べて大幅に低い
- Fed3Rのパラメータをソフトマックス分類器の初期化に利用し、さらにFed3R+FTとして任意のFLアルゴリズムで微調整する方法を提案

これ、異質なデータでも学習が効率的にできるのがポイントね。特に、通信や計算リソースを大幅に節約できるのはすごい！未来のデバイス間連携がもっとスムーズになるかもってワクワクしちゃう。

**Comment:** Accepted at ICML 2024 - https://fed-3r.github.io/

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, cs.CV, **投稿日時:** 2024-06-03 08:52


- - -

### [Cohort Squeeze: Beyond a Single Communication Round per Cohort in Cross-Device Federated Learning](http://arxiv.org/abs/2406.01115)

**コホートスクイーズ：クロスデバイス連合学習における単一通信ラウンドを越えて**

Kai Yi, Timur Kharisov, Igor Sokolov, Peter Richtárik

- 連合学習の従来手法は、サーバとクライアントの単一通信ラウンドに依存
- 新手法が各コホートからより多くの情報を引き出す可能性を検討
- 提案手法は通信コストを最大74%削減する効果を確認
- SPPM-ASを用い、新たなクライアントサンプリング手法で従来手法よりも向上

これ、すごい興味深いね！通信コストが大幅に削減されたら、実用化も早まりそうだよね。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, **投稿日時:** 2024-06-03 08:48


- - -

### [ACCO: Accumulate while you Communicate, Hiding Communications in Distributed LLM Training](http://arxiv.org/abs/2406.02613)

**ACCO：通信中の集積、分散LLMトレーニングにおける通信の隠蔽**

Adel Nabli, Louis Fournier, Pierre Erbacher, Louis Serrano, Eugene Belilovsky, Edouard Oyallon

- 大規模言語モデル（LLM）のトレーニングは分散実装に依存し、多数のGPUが並行して確率的勾配を計算
- データ並列設定での勾配同期は通信オーバーヘッドを引き起こし、効果的な並列化を妨げる
- 提案するACCOは、ワーカー間の通信を隠しながらオプティマイザの状態を分散させ、メモリ効率を向上
- ACCOは勾配計算と通信を重ね合わせ、通信遅延を軽減し従来の分散最適化に比べて高速収束を実現

並列処理の通信オーバーヘッドをどう減らすかってすごく重要だよね！ACCOみたいな新技術で効率が上がれば、もっと大規模なモデルもお手軽に扱えるようになりそう。未来のAIには期待が高まるね！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, **投稿日時:** 2024-06-03 08:23


- - -

### [FedAdOb: Privacy-Preserving Federated Deep Learning with Adaptive Obfuscation](http://arxiv.org/abs/2406.01085)

**FedAdOb: 適応的難読化によるプライバシー保護型連合深層学習**

Hanlin Gu, Jiahuan Luo, Yan Kang, Yuan Yao, Gongxi Zhu, Bowen Li, Lixin Fan, Qiang Yang

- 連合学習は、プライベートデータを共有せずに機械学習モデルを共同学習
- 従来のプライバシー保護方法はモデル性能を低下させる問題がある
- FedAdObはパスポートベースの適応的難読化でデータプライバシーを保護
- 実験でFedAdObは既存方法を超えたプライバシーと性能のトレードオフを示した

最強の連合学習って感じかな💡 プライバシーとモデル性能のバランスがすごくいいみたい！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CR, cs.AI, **投稿日時:** 2024-06-03 08:12


- - -

### [No Vandalism: Privacy-Preserving and Byzantine-Robust Federated Learning](http://arxiv.org/abs/2406.01080)

**バンダリズムなし: プライバシー保護とビザンチン耐性の連合学習**

Zhibo Xing, Zijian Zhang, Zi'ang Zhang, Jiamou Liu, Liehuang Zhu, Giovanni Russello

- 連合学習は複数のクライアントがプライベートデータを共有せずにモデルを共同学習可能
- ただし、従来の連合学習はデータやモデルへの破壊攻撃に弱く、パフォーマンス低下やバックドアのリスクがある
- 新提案のモデルフィルターが毒性のあるローカルモデルを検出し、ゼロ知識証明でプライバシーを強化
- 秘密分散を採用し、安全な集約を実現しつつ、悪意あるクライアントを排除

毒性攻撃とか、ゼロ知識証明の使い方が興味深いなぁ。ついでに、従来の方法よりパフォーマンスが良くなるのもすごいかもね！



**トピック:** [連合学習](../../fl), [ゼロ知識証明](../../zkp), **カテゴリ:** cs.CR, cs.DC, cs.LG, **投稿日時:** 2024-06-03 07:59


- - -

### [Guaranteeing Data Privacy in Federated Unlearning with Dynamic User Participation](http://arxiv.org/abs/2406.00966)

**動的ユーザ参加を伴う連合学習におけるデータプライバシーの保証**

Ziyao Liu, Yu Jiang, Weifeng Jiang, Jiale Guo, Jun Zhao, Kwok-Yan Lam

- 伝統的な方法では削除されたユーザの影響を除去し、全ユーザで再訓練するが大きな負担が発生する
- クラスタリングを使用してFLユーザをクラスターに分け、それぞれのモデルで投票を集計することで効率を向上
- クラスター内でのSecAggスキーム統合がユーザの勾配からの情報漏洩を防ぎ、プライバシーを保護
- 理論的評価と実験結果により、提案手法が動的なユーザ参加に対応しつつプライバシーと効果を両立することが示される

動的なユーザ参加にも強いクラスタリングとプライバシー保護の組み合わせ、気になるー！これで本当に効果が上がるなら、今後の連合学習がもっと安全・効率的になりそうだね。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CR, **投稿日時:** 2024-06-03 03:39


- - -

### [Local Methods with Adaptivity via Scaling](http://arxiv.org/abs/2406.00846)

**適応性を持つローカルメソッドによるスケーリング**

Savelii Chezhegov, Sergey Skorik, Nikolas Khachaturov, Danil Shalagin, Aram Avetisyan, Aleksandr Beznosikov, Martin Takáč, Yaroslav Kholodov, Alexander Gasnikov

- 機械学習や深層学習の進展に伴い、最適化の課題が複雑化
- 分散環境での訓練が必須であり、連合学習にも重要
- 通信ボトルネックを軽減するためにローカルトレーニングを利用
- Adamのような適応手法とローカルトレーニングの統合を提案

適応手法とローカルトレーニングの組み合わせがどう実際の訓練効率を上げるか、気になるね。どんなコツがあるのか、追って実験結果も見てみたい！

**Comment:** 42 pages, 2 algorithms, 6 figures, 1 table

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, math.OC, **投稿日時:** 2024-06-02 19:50


- - -

### [Blockchain-aided wireless federated learning: Resource allocation and client scheduling](http://arxiv.org/abs/2406.00752)

**ブロックチェーン支援型無線連合学習：リソース割り当てとクライアントスケジューリング**

Jun Li, Weiwei Zhang, Kang Wei, Guangji Chen, Feng Shu, Wen Chen, Shi Jin

- 集中型設計の連合学習（FL）は信頼問題と単一障害点の課題がある
- ブロックチェーン支援型連合学習（BDFL）はこれらを解決し、分散型ネットワークで効果的に中央集権型の欠点を克服
- 無線ネットワークでのBDFL導入は帯域幅、計算能力、エネルギー消費の制約に直面する
- DRC-BDFLアルゴリズムは、Lyapunov最適化を使用して動的にリソース割り当てとクライアントスケジューリングを最適化し、トレーニング遅延を約12％削減

無線環境での学習遅延が解決できたとしたら、これすっごく便利じゃない？新しい技術が色々な制約を克服できるの楽しみだよね！

**Comment:** 14 pages, 4 figures

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.DC, **投稿日時:** 2024-06-02 14:10


- - -

### [A Novel Defense Against Poisoning Attacks on Federated Learning: LayerCAM Augmented with Autoencoder](http://arxiv.org/abs/2406.02605)

**連合学習に対する毒性攻撃防御策の新手法：オートエンコーダを用いたLayerCAM**

Jingjing Zheng, Xin Yuan, Kai Li, Wei Ni, Eduardo Tovar, Jon Crowcroft

- 連合学習（FL）における毒性攻撃を防ぐため、LayerCAMとオートエンコーダ（AE）を統合した新防御戦略LayerCAM-AEを提案。
- LayerCAM-AEは各ローカルモデル更新のヒートマップを生成し、よりコンパクトな視覚形式に変換することで検出能力を向上。
- 誤検出リスクを軽減するため、複数回の通信ラウンドでヒートマップが一貫して疑わしい場合にマリシャスモデルとみなす投票アルゴリズムを開発。
- SVHNおよびCIFAR-100データセットでの実験結果、LayerCAM-AEはResNet-50およびREGNETY-800MFよりも高い検出率とテスト精度を示し、FL性能を向上させる。

この研究は連合学習における毒性攻撃の検出精度を大幅に向上できるなんてすごく未来が明るいね！これからのセキュリティ対策がどう進化していくのか、ワクワクしちゃうな！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CR, cs.AI, cs.CV, cs.LG, **投稿日時:** 2024-06-02 12:37


- - -

### [Redefining Contributions: Shapley-Driven Federated Learning](http://arxiv.org/abs/2406.00569)

**貢献の再定義: Shapley駆動の連合学習**

Nurbek Tastan, Samar Fares, Toluwani Aremu, Samuel Horvath, Karthik Nandakumar

- 連合学習（FL）は、生データを共有せずに複数の参加者が共同でグローバルモデルを訓練できるが、参加者の貢献度が均等でない場合や誠実でない場合、モデル収束が困難となることがある。
- 従来の貢献度評価は全体的な精度評価に依存しており、クラス固有の影響を捉えることに失敗することが多い。
- 本論文では、協力ゲーム理論のShapley値を利用して、連合学習における参加者の細かい貢献度評価を行う新たな方法ShapFedを提案する。
- ShapFedに基づく重み付け集約方法ShapFed-WAは、特にクラス不均衡シナリオにおいて従来の連合平均法よりも優れており、効用、効率、公平性の改善が実証された。

この研究、本当に面白そう！特にクラス不均衡にどう対応するかの部分、すごく参考になりそうだよね。

**Comment:** Accepted by IJCAI 2024

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, **投稿日時:** 2024-06-01 22:40


- - -

### [Federated Model Heterogeneous Matryoshka Representation Learning](http://arxiv.org/abs/2406.00488)

**連合モデル異質マトリョーシカ表現学習**

Liping Yi, Han Yu, Chao Ren, Gang Wang, Xiaoguang Liu, Xiaoxiao Li

- 異質な構造を持つ連合学習クライアントが協調訓練できるが、既存手法は知識交換が制限されている
- 補助的な小規模の均質モデルを追加し、クライアント間の知識の融合を実現するFedMRLを提案
- 個別の軽量な表現プロジェクタで表現を融合し、ローカルデータ分布に適応させる
- FedMRLは非凸収束率$O(1/T)$を達成し、最新手法と比較して最大8.48%および24.94%の精度向上

連合学習って、ほんとに未来感じちゃう。知識の融合とかマトリョーシカみたいで面白そう！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-06-01 16:37


- - -

### [SpaFL: Communication-Efficient Federated Learning with Sparse Models and Low computational Overhead](http://arxiv.org/abs/2406.00431)

**SpaFL: スパースモデルと低計算オーバーヘッドを用いた通信効率の高い連合学習**

Minsu Kim, Walid Saad, Merouane Debbah, Choong Seon Hong

- 連合学習(FL)の大きな通信および計算負荷がリソース制約のあるクライアントとシステムでの実用化の課題
- SpaFLはスパースモデル構造を最適化し低計算オーバーヘッドで通信効率を向上させるFLフレームワーク
- フィルタ/ニューロンごとに訓練可能な閾値を定義し、接続パラメータをプルーニングすることで構造的スパース性を実現
- 閾値のみをサーバとクライアント間で通信し、プルーニング方法を学習、グローバル閾値でモデルパラメータを更新

スパースモデルを使うことで、通信と計算コストをぐっと下げながら精度を保てるとか、すごくない？これ、本当に実用化されたら、もっと多くのシステムでFLが使えるようになるかもね。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, cs.DC, **投稿日時:** 2024-06-01 13:10


- - -

### [FedAST: Federated Asynchronous Simultaneous Training](http://arxiv.org/abs/2406.00302)

**FedAST: 連合非同期同時トレーニング**

Baris Askin, Pranay Sharma, Carlee Joe-Wong, Gauri Joshi

- 連合学習（FL）は、プライベートデータを共有せずにエッジデバイスが共同で機械学習モデルをトレーニングする技術である
- 多くの既存のFL研究は単一タスクの効率的なモデル学習に焦点を当てている
- FedASTは、複数タスクのモデルをエッジデバイスで同時にトレーニングするためのバッファ非同期アルゴリズムを提案
- 実世界のデータセットを用いた実験で、FedASTは既存の同時FL手法に比べ、最大46.0%のトレーニング時間短縮を達成

この研究、めっちゃ面白そう！複数のタスクを同時に処理できるなんて、効率アップ間違いなしだね。

**Comment:** Accepted to UAI 2024

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-06-01 05:14


- - -

### [Wait or Not to Wait: Evaluating Trade-Offs between Speed and Precision in Blockchain-based Federated Aggregation](http://arxiv.org/abs/2406.00181)

**待つべきか待たざるべきか：ブロックチェーンを用いた連合集約における速度と精度のトレードオフ評価**

Huong Nguyen, Tri Nguyen, Lauri Lovén, Susanna Pirttikangas

- ブロックチェーンを用いた連合学習アーキテクチャを提案し、訓練と集約を分散化することで単一障害点を排除
- 共有モデルの選択とカスタマイズ可能な集約により、システム性能と正確な推論結果を最適化
- プライベートなイーサリアムプラットフォームでの実験結果は、中央集約と分散集約で推論精度が比較可能であることを示す
- 非同期集約は単純な学習モデルには適しているが、複雑なモデルには集約関与が必要で、精度と非同期集約のバランスが重要

ブロックチェーンで連合学習がもっと便利になるなんてびっくり！複雑なモデルだと工夫が必要だけど、それがまた面白そうだよね。

**Comment:** Accepted at Workshop on Engineering techniques for Distributed   Computing Continuum Systems 2024

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.DC, **投稿日時:** 2024-05-31 20:16


- - -

### [Non-Federated Multi-Task Split Learning for Heterogeneous Sources](http://arxiv.org/abs/2406.00150)

**非連合型マルチタスク分割学習による異質データ源の処理**

Yilin Zheng, Atilla Eryilmaz

- エッジネットワークとモバイルコンピューティングの発展で異質なデータ源を処理する新たな分散機械学習メカニズムが必要
- 連合学習（FL）は収束性とデータプライバシー保証があるが、データの不均質性や計算の不均質性に対応できない
- 代替として、分割学習（SL）と分散ネットワークアーキテクチャの柔軟性を組み合わせたマルチタスク分割学習（MTSL）を提案
- 理論的分析でMTSLが迅速な収束を達成できることを示し、画像分類データセット上でFLに比べてトレーニング速度、通信コスト、不均質データへの耐性で優位性を示す

新たな学習パラダイムを提案するってかなり期待できそう！私たちのプロジェクトにも応用できるかもね！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-05-31 19:27


- - -

### [ACE: A Model Poisoning Attack on Contribution Evaluation Methods in Federated Learning](http://arxiv.org/abs/2405.20975)

**ACE: 連合学習における貢献評価方法へのモデルポイズニング攻撃**

Zhangchen Xu, Fengqing Jiang, Luyao Niu, Jinyuan Jia, Bo Li, Radha Poovendran

- 連合学習（FL）は、クライアントがローカルデータを共有せずにグローバルモデルを共同で訓練する手法
- ACE攻撃は、悪意のあるクライアントが自身の貢献を高く評価させるための手法
- 理論的および実証的評価により、ACEが貢献評価方法を効果的に欺くことを示す
- 提案された対策はACEへの効果が不十分であり、新たな防御策の必要性を強調

悪意のあるクライアントがシステムを欺く手法が存在するなんて驚きだよね！これからもっと強力な防御策が開発されるといいな。

**Comment:** To appear in the 33rd USENIX Security Symposium, 2024

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CR, cs.AI, cs.LG, **投稿日時:** 2024-05-31 16:21


- - -

### [Sheaf HyperNetworks for Personalized Federated Learning](http://arxiv.org/abs/2405.20882)

**連合学習のための層ハイパーネットワーク**

Bao Nguyen, Lorenzo Sani, Xinchi Qiu, Pietro Liò, Nicholas D. Lane

- GHNsはGNNsとHNsを組み合わせたが、過剰平滑化や異種性などの問題がある
- PFLにはGHNsが直接適用できず、あらかじめクライアント関係グラフがない場合が多い
- PFLの文脈で限界を克服するため、セルラー層理論とHNsを組み合わせたSHNsを提案
- SHNsは複雑な非IIDシナリオで精度が最大2.7%、平均二乗誤差が最大5.3%改善

新しいアーキテクチャがどれだけ性能を引き上げるのかワクワクするね！応用先も広くて、交通予測や天気予報にも使えるなら未来が楽しみ！

**Comment:** 25 pages, 12 figures, 7 tables, pre-print under review

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, **投稿日時:** 2024-05-31 14:55


- - -

### [BackdoorIndicator: Leveraging OOD Data for Proactive Backdoor Detection in Federated Learning](http://arxiv.org/abs/2405.20862)

**BackdoorIndicator: 連合学習におけるバックドア検出のためのOODデータ活用**

Songze Li, Yanbo Dai

- 連合学習システムでは、悪意あるクライアントが毒されたモデルをアップロードしバックドアを植え付ける可能性がある
- 既存のバックドア防御はシステムや攻撃者設定によって性能が不安定である
- BackdoorIndicatorは、サーバがOODデータを用いてバックドアの存在を正確に検出する新たな検出メカニズムを提案する
- 一連の実証実験を通じて、BackdoorIndicatorは既存の防御策に比べ一貫した優れた性能と実用性を示した

バックドア検出のソリューションなんてめっちゃかっこいいじゃん！セキュリティと機械学習両方好きな人にはすごく興味深い内容だね。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CR, **投稿日時:** 2024-05-31 14:44


- - -

### [Pursuing Overall Welfare in Federated Learning through Sequential Decision Making](http://arxiv.org/abs/2405.20821)

**連合学習における逐次意思決定による全体的福祉の追求**

Seok-Ju Hahn, Gi-Soo Kim, Junghye Lee

- 単一のグローバルモデルでは全クライアントに対して均等に性能を提供できない
- クライアントごとの公平性を達成するため、適応的な集約方式の導入が必要
- 既存の公平性を考慮した集約戦略はオンライン凸最適化フレームワークで統一できる
- 細分化された新手法AAggFFは既存方法よりもクライアントレベルの公平性が向上

連合学習でクライアントごとの公平性をめちゃ考えてるね。新しいAAggFFがどれだけ実用的か、コード見て試したくなっちゃう♪

**Comment:** Accepted at ICML 2024

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, stat.ML, **投稿日時:** 2024-05-31 14:15


- - -

### [Share Your Secrets for Privacy! Confidential Forecasting with Vertical Federated Learning](http://arxiv.org/abs/2405.20761)

**秘密を共有してプライバシーを守ろう！垂直連合学習での機密予測**

Aditya Shankar, Lydia Y. Chen, Jérémie Decouchant, Dimitra Gkorou, Rihan Hai

- 垂直連合学習は、予知保全と機械制御などの産業用時系列予測に有望
- データプライバシーと小規模・ノイズデータでの過学習が課題
- 秘密共有と秘密計算を用いたサーバーレス予測を提案
- 提案フレームワークは中央集権的手法と比較して予測精度が向上し、23.81%の精度向上を達成

データプライバシーを保ちながら予測精度も上げるなんて、すごく興味深いよね！セキュリティに煩わされずに産業がもっと効率的になるかも。

**Comment:** Submitted to the 27TH EUROPEAN CONFERENCE ON ARTIFICIAL INTELLIGENCE   (ECAI 2024)

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.CR, cs.DC, **投稿日時:** 2024-05-31 12:27


- - -

### [GANcrop: A Contrastive Defense Against Backdoor Attacks in Federated Learning](http://arxiv.org/abs/2405.20727)

**GANcrop: 連合学習におけるバックドア攻撃に対するコントラスト防御**

Xiaoyun Gan, Shanyu Gan, Taizhi Su, Peng Liu

- 連合学習はデータプライバシー保護のために注目されているが、バックドア攻撃の機会も提供する
- GANcropはコントラスト学習を用いて悪意あるモデルと善良なモデルの違いを探る新たな防御機構を提案
- GANを使用してバックドアトリガーを回収し、ターゲットを絞った緩和戦略を実施する
- 実験結果では、特に非独立同分布（non-IID）シナリオで、バックドア攻撃から効果的に保護しつつ、高いモデル精度を維持することが示された

連合学習のバックドア攻撃を防ぐ新手法って、めちゃ楽しそう！GANを使ってうまく対処してるってみたいだね！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CR, cs.AI, cs.DC, **投稿日時:** 2024-05-31 09:33


- - -

### [GI-NAS: Boosting Gradient Inversion Attacks through Adaptive Neural Architecture Search](http://arxiv.org/abs/2405.20725)

**GI-NAS: 適応的ニューラルアーキテクチャ検索による勾配逆転攻撃の強化**

Wenbo Yu, Hao Fang, Bin Chen, Xiaohang Sui, Chuan Chen, Hao Wu, Shu-Tao Xia, Ke Xu

- 連合学習システムで勾配を逆転し、ローカルクライアントの機微データを再構築する手法
- 多くの手法は明示的な事前知識（例: 事前訓練済み生成モデル）に依存しているが、これは現実シナリオでは入手困難
- 提案手法GI-NASはニューラルアーキテクチャ検索を通じて暗黙的な事前知識を活用し、ネットワークを自動検索
- 高解像度画像や大きなバッチサイズ、先進的な防御戦略など、より実用的な設定でも優れた攻撃性能を示した

適応的にニューラルアーキテクチャを探索することで、従来の手法では難しかったシナリオでも成功するって、めっちゃ興味深い！今後のプライバシー技術にとって重要なステップになりそうだね。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.AI, cs.CV, **投稿日時:** 2024-05-31 09:29


- - -

### [Prune at the Clients, Not the Server: Accelerated Sparse Training in Federated Learning](http://arxiv.org/abs/2405.20623)

**クライアントで剪定を行う: 連合学習における加速スパーストレーニング**

Georg Meinhardt, Kai Yi, Laurent Condat, Peter Richtárik

- 連合学習（FL）は複数のクライアントがローカルデータを保持しながら共有モデルを訓練する方式
- クライアントのリソース制限と通信コストが大規模モデルの訓練において主要な問題
- サーバー側でのスパーストレーニングと加速は失敗し、クライアント側で適切に実行する新手法を提案
- Sparse-ProxSkipは、非凸設定でのスパーストレーニングと加速を組み合わせ、実験でも良好な性能を示す

クライアント側で剪定を行うとか、新しい発想だね！これで連合学習の効率がもっと上がるかも。試してみたくなるね。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, math.OC, **投稿日時:** 2024-05-31 05:21


- - -

### [Selective Knowledge Sharing for Personalized Federated Learning Under Capacity Heterogeneity](http://arxiv.org/abs/2405.20589)

**キャパシティ不均一性におけるパーソナライズされた連合学習のための選択的知識共有**

Zheng Wang, Zheng Wang, Zhaopeng Peng, Zihui Wang, Cheng Wang

- 連合学習は低キャパシティデバイスからのプライベートデータと計算力を利用可能
- クライアント特有のデータに基づくモデルのパーソナライズが不十分で、効率に課題
- Pa3dFLはレイヤーを一般パラメータと個別パラメータに分けて、効率的な知識の共有を実現
- 実験結果では、Pa3dFLがベースライン法を性能面で一貫して上回り、通信・計算効率も良好

Pa3dFL、すごく面白いね！低キャパシティデバイスの活用が進むと、もっと多様なデバイスでの学習ができるかもね。今後の連合学習がどう発展するか楽しみだな。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, cs.DC, **投稿日時:** 2024-05-31 02:59
