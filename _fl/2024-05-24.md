---
title: 連合学習 (2024-05-24 ~ 2024-05-30)
date: 2024-05-24
---

連合学習に関する論文まとめ (2024-05-24 ~ 2024-05-30)


- - -

### [FedSAC: Dynamic Submodel Allocation for Collaborative Fairness in Federated Learning](http://arxiv.org/abs/2405.18291)

**FedSAC: 連合学習における協調的公平性のための動的サブモデル配分**

Zihui Wang, Zheng Wang, Lingjuan Lyu, Zhaopeng Peng, Zhicheng Yang, Chenglu Wen, Rongshan Yu, Cheng Wang, Xiaoliang Fan

- 連合学習でクライアント毎の貢献に基づく報酬配分を行い、公平性を確保する「BCF」を提案
- BCFを実現するため、高性能サブモデルで貢献の大きいクライアントを優遇するモジュールを設計
- 動的集約モジュールを開発し、多様なニューロンを公平に扱うことでモデル精度を向上
- 実験でFedSACが公平性と精度の両面で既存の方法を凌駕することを証明

連合学習の大きな進歩かも！クライアントのやる気も増えそうだね。自分の貢献がちゃんと評価されるのって嬉しいもん！

**Comment:** Accepted by KDD'24

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, cs.DC, **投稿日時:** 2024-05-28 15:43


- - -

### [Fast-FedUL: A Training-Free Federated Unlearning with Provable Skew Resilience](http://arxiv.org/abs/2405.18040)

**Fast-FedUL: 訓練不要の連合学習におけるスキュー耐性のある証明付き学習削除**

Thanh Trung Huynh, Trong Bang Nguyen, Phi Le Nguyen, Thanh Tam Nguyen, Matthias Weidlich, Quoc Viet Hung Nguyen, Karl Aberer

- 連合学習(FL)の普及とプライバシー保護の重要性が増し、「忘れられる権利」やデータ改ざん攻撃への対策が必要
- 中央集権型学習には多くの学習削除方法があるが、FLの操作方法とは根本的に異なり適用が困難
- Fast-FedULは再訓練を不要にし、ターゲットクライアントの影響を順次削除するアルゴリズムを提案
- バックドア攻撃シナリオで、ターゲットクライアントの痕跡をほぼ完全に除去し、メインタスクの98%の高い精度を保持

連合学習で再訓練不要なのは大きな進歩かも！バックドア攻撃への対策もすごく重要だろうね。

**Comment:** Accepted in ECML PKDD 2024

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, cs.DC, cs.ET, **投稿日時:** 2024-05-28 10:51


- - -

### [Towards Communication-efficient Federated Learning via Sparse and Aligned Adaptive Optimization](http://arxiv.org/abs/2405.17932)

**通信効率の高い連合学習を目指したスパースで整合した適応最適化**

Xiumei Deng, Jun Li, Kang Wei, Long Shi, Zeihui Xiong, Ming Ding, Wen Chen, Shi Jin, H. Vincent Poor

- 連合学習で広く使われるAdamは収束が早いが通信オーバーヘッドが大きい
- 新たに提案されたFedAdam-SSMはモデル更新とモーメント推定をスパース化
- 共有スパースマスクを使用し、通信オーバーヘッドをさらに削減
- FedAdam-SSMは収束速度とテスト精度で他の手法よりも優れている

新しいアルゴリズムが通信を減らしつつ性能を保てるか試してみたいよね。共有スパースマスクってどんな感じで実現されるんだろう？



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-05-28 07:56


- - -

### [Decentralized Directed Collaboration for Personalized Federated Learning](http://arxiv.org/abs/2405.17876)

**個別化連合学習のための分散指向協力**

Yingqi Liu, Yifan Shi, Qinglun Li, Baoyuan Wu, Xueqian Wang, Li Shen

- 連合学習(FL)の中央サーバ依存から脱却し、分散型個別連合学習(DPFL)に焦点
- P2P方式のDPFLは非対称トポロジーの影響でモデルの個別化性能が劣化
- DFedPGPという枠組みを提案、部分モデル個別化と方向付き勾配プッシュを活用
- 提案手法はデータと計算資源の異質性シナリオでもSOTA精度を達成

非対称トポロジーを活用しながらモデルの個別化を目指しているところが面白そう。新しい方法で連合学習の可能性が広がるかもね！

**Comment:** CVPR 2024. arXiv admin note: text overlap with arXiv:2305.15157

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, math.OC, **投稿日時:** 2024-05-28 06:52


- - -

### [PeerFL: A Simulator for Peer-to-Peer Federated Learning at Scale](http://arxiv.org/abs/2405.17839)

**PeerFL: 大規模なピアツーピア連合学習のためのシミュレーター**

Alka Luqman, Shivanshu Shekhar, Anupam Chattopadhyay

- ピアツーピア連合学習ツールをNS3と統合し、新たなシミュレーターを作成
- 異なるデバイスを使った実験が可能で、既存のシミュレーターの不足を補う
- NS3を利用し、移動する参加者のためにWiFiダイナミクスをシミュレーション
- 最大450の異なるデバイスをモデリングし、計算資源の利用効率を実証

ピアツーピア連合学習のシミュレーションなんて面白そう！これなら大規模実験も簡単にできちゃうかも。オープンソースだから、みんなで使い倒せるね！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.DC, cs.AI, **投稿日時:** 2024-05-28 05:30


- - -

### [An Innovative Networks in Federated Learning](http://arxiv.org/abs/2405.17836)

**連合学習における革新的なネットワーク**

Zavareh Bozorgasl, Hao Chen

- Wavelet Kolmogorov-Arnold Networks（Wav-KAN）を連合学習に導入し、クライアント側に実装
- 連続ウェーブレット変換（CWT）と離散ウェーブレット変換（DWT）を使い、異種データ分布を多解像度で対応
- Wav-KANは解釈性、計算速度、訓練とテストの精度に優れた結果を実験で示した
- ウェーブレットに基づく活性化関数を使用して、ローカルおよびグローバルモデルの性能を向上

ウェーブレットと連合学習って新しい感じでドキドキするね。いろんなデータに対応できるところが未来っぽい！

**Comment:** Work in progress

**トピック:** [連合学習](../../fl), **カテゴリ:** eess.SP, cs.LG, stat.ML, **投稿日時:** 2024-05-28 05:20


- - -

### [Post-Fair Federated Learning: Achieving Group and Community Fairness in Federated Learning via Post-processing](http://arxiv.org/abs/2405.17782)

**フェアネス後処理型連合学習：後処理による連合学習におけるグループおよびコミュニティフェアネスの実現**

Yuying Duan, Yijun Tian, Nitesh Chawla, Michael Lemmon

- 連合学習（FL）は、複数のコミュニティがデータをローカルに保持したままで共有モデルを学習する枠組み
- グループフェアネスとコミュニティフェアネスの2つのフェアネス概念が重要な問題として浮上
- 本論文はポスト処理型連合学習（post-FFL）というフレームワークを提案し、両フェアネスを同時に実現
- 菌質系モデルの使用実験で、従来の方法より高いフェアネスと効率的な通信・計算コストを達成

新しいフェアネスの実現方法が提案されていて、特に医療ネットワークで使えそうなのが面白いね！こういう具体的な応用例があると、現実に近い感じがしてワクワクするよね！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.CY, **投稿日時:** 2024-05-28 03:26


- - -

### [Wireless Federated Learning over Resource-Constrained Networks: Digital versus Analog Transmissions](http://arxiv.org/abs/2405.17759)

**リソース制約されたネットワークにおける無線連合学習: デジタル伝送とアナログ伝送の比較**

Jiacheng Yao, Wei Xu, Zhaohui Yang, Xiaohu You, Mehdi Bennis, H. Vincent Poor

- デジタルとアナログの伝送方式を、デバイスの不均衡なサンプリングや厳しい遅延目標、送信電力制約の下で比較
- デジタル方式は通信設計をFLの計算タスクから分離し、大量のデバイスからの上り伝送は難しく主に通信に制約される
- アナログ通信はエアコンピューティング(AirComp)を可能にし、スペクトラム利用効率を改善
- 計算指向のアナログ伝送は電力効率が低く、CSIの不完全さから計算誤差に敏感である

デジタルとアナログの伝送方式、どっちが良いんだろうって悩むよね！だけど、それぞれの特性をフルに活かせればもっといい結果が出せそうだよね。

**Comment:** Accepted by IEEE TWC. arXiv admin note: text overlap with   arXiv:2402.09657

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.IT, math.IT, **投稿日時:** 2024-05-28 02:23


- - -

### [Federating Dynamic Models using Early-Exit Architectures for Automatic Speech Recognition on Heterogeneous Clients](http://arxiv.org/abs/2405.17376)

**異種クライアントにおける自動音声認識のための早期退出アーキテクチャを用いた動的モデルの連合**

Mohamed Nabih Ali, Alessio Brutti, Daniele Falavigna

- 自動音声認識モデルの訓練には大量の音声記録が必要であり、データ収集にプライバシーの懸念がある。
- 連合学習を用いて複数クライアントでデータをローカルのまま共有モデルを学習することが可能である。
- クライアントデバイスの計算・通信資源が限られており、単一モデルの生成は最適でない。
- 効率的なアーキテクチャとして早期退出ソリューションを用いた動的モデルを提案し、様々なデバイスで利用可能であることを実証。

早期退出するモデルとか、なんかキャッチーな感じがして良いね！色々なデバイスで一つのモデルが使えちゃうなんて、未来の技術って感じがするなぁ。

**Comment:** The paper is under review in Future Generation Computer Systems   Journal

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CL, **投稿日時:** 2024-05-27 17:32


- - -

### [FedHPL: Efficient Heterogeneous Federated Learning with Prompt Tuning and Logit Distillation](http://arxiv.org/abs/2405.17267)

**FedHPL: プロンプトチューニングとロジット蒸留による効率的な異種連合学習**

Yuting Ma, Lechao Cheng, Yaxiong Wang, Zhun Zhong, Xiaohua Xu, Meng Wang

- 連合学習はデータをローカルに保持しつつモデルを協調訓練するプライバシー保護パラダイム
- 異なるモデル構造やデータ分布、限られたリソースでの性能低下や収束速度の遅れが発生
- FedHPLはプロンプトチューニングとロジット蒸留を基にした統一的連合学習フレームワークを提案
- 実験で既存手法を上回る性能と少ない計算負荷、訓練ラウンドで効率的であることを確認

プロンプトチューニングとロジット蒸留が鍵になってるみたい！効率的にモデルを強化する方法が気になるね。😊

**Comment:** 35 pages

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.CV, **投稿日時:** 2024-05-27 15:25


- - -

### [Efficient Model Compression for Hierarchical Federated Learning](http://arxiv.org/abs/2405.17522)

**階層型連合学習のための効率的なモデル圧縮**

Xi Zhu, Songcan Yu, Junbo Wang, Qinglin Yang

- 連合学習は、データ共有を避けつつモデルパラメータを共有することでプライバシーを維持
- モバイル・エッジコンピューティング環境での通信資源の消費が、モデルサイズ増大に伴い問題化
- 新規の階層型連合学習フレームワークと適応クラスタリングアルゴリズムを提案
- 提案アルゴリズムは予測精度を保ちながら、既存の連合学習手法よりエネルギー消費を大幅に削減

モデル圧縮とクラスタリングを融合させた連合学習の効率化、すごく革新的だよね。これで通信量の削減が叶えば、もっとエコでスマートな学習が広がりそう！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-05-27 12:17


- - -

### [LabObf: A Label Protection Scheme for Vertical Federated Learning Through Label Obfuscation](http://arxiv.org/abs/2405.17042)

**LabObf: ラベルあいまい化による垂直連合学習のためのラベル保護スキーム**

Ying He, Mingyang Niu, Jingyu Hua, Yunlong Mao, Xu Huang, Chen Li, Sheng Zhong

- 垂直連合学習におけるスプリット学習は、プライバシー保護特性のため広く利用されている
- 悪意ある参加者がアップロードされた埋め込みベクトルからラベル情報を推測し、プライバシー漏洩が懸念される
- Embedding Extension Attackを提案し、現行防御戦略を無力化する手法を扱う
- ラベルのあいまい化防御戦略「LabObf」を提案し、攻撃者の成功率をランダム推測レベルまで低減する

この研究、攻撃と防御のせめぎ合いが面白いよね！LabObfの防御戦略が今後どこまで実用化されるか興味津々だな～。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.CR, **投稿日時:** 2024-05-27 10:54


- - -

### [Visualizing the Shadows: Unveiling Data Poisoning Behaviors in Federated Learning](http://arxiv.org/abs/2405.16707)

**影を可視化する：連合学習におけるデータポイズニング行動の解明**

Xueqing Zhang, Junkai Zhang, Ka-Ho Chow, Juntao Chen, Ying Mao, Mohamed Rahouti, Xiang Li, Yuchen Liu, Wenqi Wei

- 連合学習システムは、ターゲットを絞ったデータポイズニング攻撃に対して脆弱である
- ラベル反転を用いたデータポイズニング攻撃をシミュレーションし、モデル性能への影響を分析
- 五つのコンポーネントで構成されるシステムを使用し、攻撃の可視化と対策を実施
- ラベル操作、攻撃タイミング、悪意ある攻撃の可用性からの観察で、システムの脆弱性を理解

連合学習に対する攻撃の可視化って、実際にどうやって見える化するのか興味津々！未来のセキュリティ対策に役立ちそう。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CR, **投稿日時:** 2024-05-26 21:58


- - -

### [A Systematic Review of Federated Generative Models](http://arxiv.org/abs/2405.16682)

**連合生成モデルの体系的レビュー**

Ashkan Vedadi Gargary, Emiliano De Cristofaro

- 連合学習（FL）と生成モデルを組み合わせることで分散システムでデータを共有せずにモデル訓練が可能
- FLと生成モデルの組み合わせは攻撃に脆弱で、最適なアーキテクチャの設計が困難
- 2019年から2024年にかけての約100本の論文を体系的に比較し、FLと生成モデルの手法、およびプライバシー考慮の点に焦点
- 最先端の進展を強調し、未解決の課題を特定することで、新参者にも理解しやすく今後の研究に洞察を提供

連合学習と生成モデルの融合って、新たなプライバシー技術の未来が見えそう！どんな技術が出てくるのかワクワクするね。

**Comment:** 24 Pages, 3 Figures, 5 Tables

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.CL, cs.CR, **投稿日時:** 2024-05-26 20:20


- - -

### [Fair Federated Learning under Domain Skew with Local Consistency and Domain Diversity](http://arxiv.org/abs/2405.16585)

**異なるドメインにおける局所的一貫性とドメイン多様性を考慮した公平な連合学習**

Yuhang Chen, Wenke Huang, Mang Ye

- ドメインスキュー下での連合学習（FL）は偏りがあり、公平性の問題が二つある
- パラメータアップデートの矛盾があり、不一致なアップデート方向が発生
- モデルの集約バイアスがあり、不公平な重み配分とドメイン多様性の無視が問題
- 提案手法は不要なパラメータアップデートを選択的に破棄し、公平な集約目標を設定

この研究、実践的なリアルな問題を解決しようとしてて、けっこうおもしろい！公平性を保つための工夫、現場にどう役立つか気になる〜。

**Comment:** Accepted by CVPR2024

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, **投稿日時:** 2024-05-26 14:29


- - -

### [Multi-Level Additive Modeling for Structured Non-IID Federated Learning](http://arxiv.org/abs/2405.16472)

**非独立同分布な連合学習のための多層加法モデリング**

Shutong Chen, Tianyi Zhou, Guodong Long, Jie Ma, Jing Jiang, Chengqi Zhang

- 連合学習の主要な課題は、クライアント間の非独立同分布（Non-IID）をモデル化すること
- 非IIDに対応するため、マルチレベル加法モデル（MAM）を用いて知識共有を改善
- 各クライアントは任意のレベルの1つのモデルに割り当てられ、個別の予測はモデルの出力を統合して算出
- FeMAMは既存のクラスタFLおよび個別FL手法を上回り、非IID環境で優れた性能を発揮することを実証

異なるクライアント間での知識共有を効率化するなんて、面白そう！普遍的なモデルと個別モデルを組み合わせるアプローチにワクワクするなぁ！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, **投稿日時:** 2024-05-26 07:54


- - -

### [Machine learning in business process management: A systematic literature review](http://arxiv.org/abs/2405.16396)

**ビジネスプロセスマネジメントにおける機械学習: 系統的文献レビュー**

Sven Weinzierl, Sandra Zilker, Sebastian Dunzer, Martin Matzner

- 機械学習はプログラムを明示的にコーディングせずにデータに基づいて作成するアルゴリズムを提供
- BPMにおける機械学習の用途は、意思決定支援、正確なプロセスモデルの発見、リソース配分の改善に多様
- 研究はBPMタスクをプロセスのライフサイクルのフェーズごとに整理し、機械学習がそのタスクにどう寄与するかを解説
- 結論として、今後の研究の方向性や新しい機械学習概念の適用に関する研究アジェンダを提供

機械学習とビジネスプロセスを一緒にするなんて、超面白そうじゃない！？今後の研究の方向性も提示されてて、革新的なアイデアが次々に生まれそうだね！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, **投稿日時:** 2024-05-26 01:12


- - -

### [Secure Hierarchical Federated Learning in Vehicular Networks Using Dynamic Client Selection and Anomaly Detection](http://arxiv.org/abs/2405.17497)

**車両ネットワークにおける動的クライアント選択と異常検出を用いた安全な階層型連合学習**

M. Saeid HaghighiFard, Sinem Coleri

- 階層型連合学習は、悪意のある車両による誤った更新でモデルの整合性が損なわれる問題に直面している
- 新しいフレームワークを提案し、動的車両選択と異常検出メカニズムを統合することでリスクを軽減
- 過去の性能、貢献頻度、異常記録を考慮した包括的な車両の信頼性評価を行う
- シミュレーション評価により、提案手法が攻撃下でも高いレジリエンスを示し、最悪のシナリオでも63%の収束時間を達成する

これ、車両ネットワークでの連合学習をさらに安全にするアイディアがいっぱいって感じね。異常検出の仕組みとかめちゃ面白そうじゃない？結果がアツい感じする！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, cs.SY, eess.SY, **投稿日時:** 2024-05-25 18:31


- - -

### [Federated Unsupervised Domain Generalization using Global and Local Alignment of Gradients](http://arxiv.org/abs/2405.16304)

**連合学習を用いた教師なしドメイン一般化のための全球および局所の勾配整合**

Farhad Pourpanah, Mahdiyar Molahasani, Milad Soltany, Michael Greenspan, Ali Etemad

- 連合学習におけるドメインシフトと勾配整合の関連性を理論的に確立
- 新手法「FedGaLA」を提案し、クライアントとサーバーでの勾配整合を実施
- PACS、OfficeHome、DomainNet、TerraIncのデータセットで実験し、手法の有効性を検証
- 各コンポーネントとパラメータの影響を調査するためのアブレーション研究と感度分析を実施

勾配整合で新しいドメインに対応できるモデルを作るのすごいね！これって色んなデータセットで試してみてもうまくいくってことなら、めっちゃ注目されそう♪

**Comment:** 23 pages, 4 figure

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, **投稿日時:** 2024-05-25 17:12


- - -

### [Vertical Federated Learning for Effectiveness, Security, Applicability: A Survey](http://arxiv.org/abs/2405.17495)

**効果、安全性、適用性のための垂直連合学習: 調査**

Mang Ye, Wei Shen, Eduard Snezhko, Vassili Kovalev, Pong C. Yuen, Bo Du

- 垂直連合学習（VFL）は、プライバシーを保護しつつ異なるパーティが協力してモデルを学習する手法である
- 最近の研究は、VFLにおける課題を克服し、実用的なクロスドメイン協力の可能性を示している
- 本調査では、VFLの歴史、背景、一般的なトレーニングプロトコルの概要を提供しつつ、最近のレビューの分類法とその限界を分析
- 効果、安全性、適用性の三つの視点から最近の研究を総合し、将来の重要な研究方向を議論

VFLっておもしろそう！将来的には、もっと多分野での協力が進んでいきそうだね。最新の研究をまとめてくれるのって助かるよね！

**Comment:** 31 pages, 9 figures, 10 tables

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.CR, **投稿日時:** 2024-05-25 16:05


- - -

### [Analytic Federated Learning](http://arxiv.org/abs/2405.16240)

**解析的連合学習**

Huiping Zhuang, Run He, Kai Tong, Di Fang, Han Sun, Haoran Li, Tianyi Chen, Ziqian Zeng

- AFL（解析的連合学習）は、連合学習に解析的（閉形式）ソリューションを導入する新たなトレーニングパラダイム
- ローカルクライアントのトレーニングステージでは、1エポックでのトレーニングを実現し、多エポック更新不要
- 集約ステージで絶対集約法（AA法）を導出し、単一ラウンドの集約を可能にし、複数ラウンド集約が不要
- データ異質性やクライアント数に影響されず、AFLは競争力を持ちながら絶対収束し、初のハイパーパラメータフリーメソッド

解析的なアプローチで連合学習がどこまで進化するか楽しみね。クライアント数が多くても影響を受けないなんて、本当だったらすごく面白いかも。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, **投稿日時:** 2024-05-25 13:58


- - -

### [Client2Vec: Improving Federated Learning by Distribution Shifts Aware Client Indexing](http://arxiv.org/abs/2405.16233)

**Client2Vec: 分布シフトに対応したクライアントインデックスによる連合学習の改善**

Yongxin Guo, Lin Wang, Xiaoying Tang, Tao Lin

- 連合学習はプライバシー保護の分散型機械学習だが、クライアント間の大きな分布シフトが課題である
- Client2Vecという機構を導入し、FLトレーニング開始前にクライアントごとの固有のインデックスを生成
- 生成されたクライアントインデックスを利用して、クライアントサンプリング、モデル集約、ローカルトレーニングを改善
- 多様なデータセットとモデルアーキテクチャにおいて、Client2Vecが効率的に機能することを実験で検証

この研究では事前準備としてクライアントインデックスを使うことで、より効率的な学習ができるみたい。クライアントごとの個性を活かすって斬新だよね！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, **投稿日時:** 2024-05-25 13:49


- - -

### [An Experimental Study of Different Aggregation Schemes in Semi-Asynchronous Federated Learning](http://arxiv.org/abs/2405.16086)

**半非同期連合学習における異なる集約方式の実験的研究**

Yunbo Li, Jiaping Gui, Yue Wu

- 連合学習は分散環境での高性能計算とデータプライバシーの保護が特徴
- 半非同期連合学習（SAFL）ではリソースの異質性に対応
- FedSGDとFedAvgの2つのモードを比較、前者は精度高く収束が速いが精度の揺れが大きい
- 後者はストラグラー問題に強いが収束遅く、精度が低い

SAFLの特性を理解して使い分けることがキーだね。精度重視か安定性重視か、どっちが大事か考えるのが面白そう！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.DC, cs.PF, **投稿日時:** 2024-05-25 06:33


- - -

### [FedSheafHN: Personalized Federated Learning on Graph-structured Data](http://arxiv.org/abs/2405.16056)

**FedSheafHN: グラフ構造データにおけるパーソナライズ連合学習**

Wenfei Liang, Yanan Zhao, Rui She, Yiming Li, Wee Peng Tay

- グラフニューラルネットワーク（GNN）のカスタマイズによりクライアントごとのニーズに対応する
- FedSheafHNは協力グラフ埋め込みと効率的なパーソナルモデル生成を提案
- シーフ拡散を用いてクライアントの特性を学び、複雑なクライアント特徴の統合と解釈を改善
- エンピリカル評価で他の既存手法より優れ、モデル収束が速く新しいクライアントへの対応も効果的

この研究、めっちゃ面白そう！特に複雑なクライアント特性をシーフ拡散で改善するところ、すごく未来的だよね。今後この技術が普及したら、いろんなデータに対応できちゃうかも！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, **投稿日時:** 2024-05-25 04:51


- - -

### [Federated Learning for Non-factorizable Models using Deep Generative Prior Approximations](http://arxiv.org/abs/2405.16055)

**非因数分解モデルのための連合学習：深層生成事前分布近似の利用**

Conor Hassan, Joshua J Bon, Elizaveta Semenova, Antonietta Mira, Kerrie Mengersen

- 連合学習（FL）はデータの共有を避けることでプライバシーを保ちながら分散型クライアント間で協調モデルを訓練可能である
- 現在のFL手法はクライアントモデル間の条件付き独立を仮定し、依存を捉える事前分布の使用を制限する
- SIGMA事前分布は深層生成モデルの近似を用いてクライアント間での非因数分解モデルのFLを可能にする
- SIGMA事前分布の有効性は合成データで示され、オーストラリアの空間データFLの実例で実用性が立証された

SIGMA事前分布を使うことで、空間統計や疫学など依存関係をモデル化するのが重要な分野でFLが可能になるんだね。新しい応用分野が広がりそうでワクワクする！

**Comment:** 25 pages, 7 figures, 2 tables

**トピック:** [連合学習](../../fl), [合成データ](../../sd), **カテゴリ:** stat.ML, cs.LG, stat.CO, stat.ME, **投稿日時:** 2024-05-25 04:44


- - -

### [Achieving Dimension-Free Communication in Federated Learning via Zeroth-Order Optimization](http://arxiv.org/abs/2405.15861)

**連合学習におけるゼロ次最適化を通じた次元依存のない通信の実現**

Zhe Li, Bicheng Ying, Zidong Liu, Haibo Yang

- 連合学習は分散データソース間で協働的かつプライバシー保護型機械学習を提供する
- 大規模モデルでの通信コストが次元に比例し、効率性の大きな障害となっている
- 新しいアルゴリズム「FedDisco」により、各通信ラウンドでスカラー値のみを送信
- 実証結果として、従来の連合学習アプローチと比較して通信オーバーヘッドが大幅に削減される

次元に依存しない通信で連合学習がもっとスムーズになるなんて、すごく未来がラクになりそうだね。FedDiscoの名前もディスコみたいでちょっと楽しい気がする。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-05-24 18:07


- - -

### [CAFe: Cost and Age aware Federated Learning](http://arxiv.org/abs/2405.15744)

**CAFe: コストとエイジを考慮した連合学習**

Sahan Liyanaarachchi, Kanchana Thilakarathna, Sennur Ulukus

- FLモデルの訓練進行を確保するために、少なくとも$M$クライアントのローカル勾配の報告を待つが、期限内に報告がない場合はラウンドが失敗と見なされ、再スタートが必要
- 小さい$T$（締め切り）と大きい$M$（クライアント数）は、多くの失敗ラウンドと過剰な通信および計算リソースの無駄を招く
- 大きい$T$はラウンド時間の延長、小さい$M$はノイズの多い勾配を生じるため、バランスが必要
- クライアントの平均エイジが理論的収束境界に影響し、収束のメトリックとして使用可能であり、$M$と$T$の選定を行う分析的手法を提供

この研究、クライアントのエイジを使って効率的に連合学習の収束を目指しているのが面白い！さらに、通信コスト削減もできる方法だから実用的かもね。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, cs.IT, math.IT, **投稿日時:** 2024-05-24 17:41


- - -

### [Harnessing Increased Client Participation with Cohort-Parallel Federated Learning](http://arxiv.org/abs/2405.15644)

**クライアント参加の向上を活用したコホート並列連合学習**

Akash Dhasade, Anne-Marie Kermarrec, Tuan-Anh Nguyen, Rafael Pires, Martijn de Vos

- 連合学習では、多くのノードが参加するほど、個々のモデル更新の効果が減少
- ネットワークをコホートに分割して、各コホートで独立に学習する新手法CPFLを提案
- コホートごとのモデルを知識蒸留とクロスドメインのラベルなしデータセットで統合
- 実験では、従来のFLと比較し、トレーニング時間とリソース使用量を大幅に削減しつつ精度低下を最小限に抑えた

"連合学習のさらなる最適化なんて面白そう！これでより効率的なモデルが期待できるね！"



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-05-24 15:34


- - -

### [Federated Behavioural Planes: Explaining the Evolution of Client Behaviour in Federated Learning](http://arxiv.org/abs/2405.15632)

**連合行動プレーン: 連合学習におけるクライアント行動の進化を解明する**

Dario Fenoglio, Gabriele Dominici, Pietro Barbiero, Alberto Tonda, Martin Gjoreski, Marc Langheinrich

- 連合学習はプライバシーリスクを減らすが、クライアント行動の理解が依然として課題
- 連合行動プレーン（FBP）を提案し、クライアントの行動を予測性能と意思決定の2つの観点から解析
- FBPによりクライアントの進化状態を示し、似た行動を持つクライアントを識別可能
- FBPのパターンを活用し、悪意あるクライアントモデルを検出する技術（連合行動シールド）を提案

クライアントの行動を可視化するなんておもしろそう！連合行動シールドの効果がどれくらいあるのか気になるね。

**Comment:** [v1] Preprint (24 pages)

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-05-24 15:17


- - -

### [DAGER: Exact Gradient Inversion for Large Language Models](http://arxiv.org/abs/2405.15586)

**DAGER: 大規模言語モデルのための厳密な勾配逆転手法**

Ivo Petrov, Dimitar I. Dimitrov, Maximilian Baader, Mark Niklas Müller, Martin Vechev

- 連合学習は複数のクライアントから計算された勾配を集約し、プライベートデータを共有せずに協力的な学習を実現する
- 過去の研究では、サーバーが勾配逆転攻撃を用いてデータを回復できることが示されていたが、テキストでは精度が低かった
- DAGERは、クライアントデータのトークンシーケンスを効率的に検出し、全バッチのテキストを正確に回復する初のアルゴリズム
- DAGERは、大規模言語モデルに対して高速（同バッチサイズで20倍）、スケーラブル（10倍大きいバッチ）、高品質な再構築（ROUGE-1/2 > 0.99）を実現

大規模言語モデルのテキストデータがそんなに正確に回復できるなんてびっくり！これからのプライバシー保護、どうなるんだろうね？



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, I.2.7; I.2.11, **投稿日時:** 2024-05-24 14:14


- - -

### [Thinking Forward: Memory-Efficient Federated Finetuning of Language Models](http://arxiv.org/abs/2405.15551)

**先を見据えた考え: メモリ効率の良い連合学習による言語モデルの微調整**

Kunjal Panchal, Nisarg Parikh, Sunav Choudhary, Lijun Zhang, Yuriy Brun, Hui Guan

- 従来のバックプロパゲーションを用いた微調整は中間活性化層が大きなメモリを消費し、リソース制約のあるデバイスには困難
- スプライ（Spry）は、各クライアントがフォワードモード自動微分を用いて訓練し、メモリ使用量を低減しつつ高精度と高速収束を実現
- 同質データ分布ではグローバル勾配の無偏推定を理論的に確認、不均質性が増すと推定のバイアスも増加
- 1.4-7.1倍のメモリ削減効果と、1.2-20.3倍の収束時間短縮、最先端の0次手法と比べて5.2-13.5%の精度向上を実証

メモリ効率がめっちゃ良くなるってすごいね！こんな技術でどんどんFLの用途が広がりそうだよ、スマホでもどんどん使えちゃうね！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, **投稿日時:** 2024-05-24 13:37


- - -

### [Unlearning during Learning: An Efficient Federated Machine Unlearning Method](http://arxiv.org/abs/2405.15474)

**学習中の忘却：効率的な連合機械学習忘却方法**

Hanlin Gu, Gongxi Zhu, Jie Zhang, Xinyuan Zhao, Yuxing Han, Lixin Fan, Qiang Yang

- 連合学習は分散機械学習の新たなパラダイムとして注目
- 従来の連合機械忘却は時間がかかり、実用性に欠ける
- FedAUは軽量な補助モジュールと直線的操作で効率化
- FedAUは個別データ、クラス、クライアントレベルの忘却をサポート

実際のFL環境で使える効率的な忘却方法が出てきたなんて、ワクワクするね！これからもっとデータプライバシーが進化していくって感じがするよ。

**Comment:** Accepted by IJCAI 2024

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-05-24 11:53


- - -

### [FedCal: Achieving Local and Global Calibration in Federated Learning via Aggregated Parameterized Scaler](http://arxiv.org/abs/2405.15458)

**FedCal: 連合学習における集約パラメータスケーラーを用いたローカルおよびグローバルキャリブレーションの実現**

Hongyi Peng, Han Yu, Xiaoli Tang, Xiaoxiao Li

- 連合学習におけるデータの多様性がモデルのキャリブレーションにチャレンジをもたらす
- 現存の連合学習手法はキャリブレーションにおいて最適ではないと理論分析から示される
- FedCalアプローチはローカルおよびグローバルキャリブレーションを重視し、クライアント固有スケーラーを利用
- 実験ではFedCalがベースラインより47.66%もグローバルキャリブレーションエラーを削減

Federated Learningって、データがバラバラでも協力して学習できるのが魅力的だよね。このFedCalの方法なら、もっと精度よくなるなんてワクワクするね！

**Comment:** This paper has been accepted by ICML'24

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-05-24 11:33


- - -

### [Towards Client Driven Federated Learning](http://arxiv.org/abs/2405.15407)

**クライアント主導型連合学習に向けて**

Songze Li, Chenqing Zhu

- 従来の連合学習はサーバー中心で、クライアントの非同期なニーズに対応できない課題がある
- CDFLはクライアントが独立してモデルを更新し、サーバーがクラスター分布を管理
- CDFLにより、時間変動するクライアントのデータ分布に迅速に適応可能となる
- クライアントへの送信モデルを単一にすることで計算効率と推定精度向上を実現

これはすごい！クライアント主体でプライバシー保護できるなんて未来の技術にワクワクしちゃう。CDFSが普及すれば色んな問題が解決できそうね！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-05-24 10:17


- - -

### [Transformer-based Federated Learning for Multi-Label Remote Sensing Image Classification](http://arxiv.org/abs/2405.15405)

**トランスフォーマーベースの連合学習によるマルチラベルリモートセンシング画像分類**

Barış Büyüktaş, Kenneth Weitzel, Sebastian Völkers, Felix Zailskas, Begüm Demir

- 連合学習はクライアントの訓練データにアクセスせずにモデルを学習するが、非独立同分布データの影響で収束が困難
- 最先端のトランスフォーマー（MLP-Mixer, ConvMixer, PoolFormer）が非独立同分布データの問題にどう対処するかを調査
- ResNet-50と比較し、異なる非独立同分布レベルでの頑健性、ローカルトレーニングの複雑さ、集約の複雑さを評価
- BigEarthNet-S2ベンチマークでの実験結果は、一般化能力が向上するがローカルトレーニングと集約の複雑さも増加

連合学習ってなんかおもしろいよね！トランスフォーマーのモデル選定についても詳しく研究されていて、実用的なガイドラインが得られるのがいいなぁ。

**Comment:** Accepted at IEEE International Geoscience and Remote Sensing   Symposium (IGARSS) 2024. Our code is available at   https://git.tu-berlin.de/rsim/FL-Transformer

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CV, **投稿日時:** 2024-05-24 10:13


- - -

### [Decaf: Data Distribution Decompose Attack against Federated Learning](http://arxiv.org/abs/2405.15316)

**Decaf: 連合学習に対するデータ分布分解攻撃**

Zhiyang Dai, Chunyi Zhou, Anmin Fu

- 既存のプライバシー攻撃技術とは異なり、連合学習に対する新しいプライバシー脅威を提案
- Decafの攻撃により、FLサーバーは被害者のデータ分布を高い精度でプロファイル可能
- 5つのベンチマークデータセットで実験し、多様なモデルアーキテクチャでDecafの有効性を検証
- Nullクラスがない場合、Decafのデータ分解精度は5%未満で、Nullクラス判定精度は100%

これは超面白そう！連合学習の仕組みを本当に分かってるFLサーバーだと、ついにこっそりデータ分析ができちゃうんだね。未来のAIプライバシー対策もますます重要になりそう。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.CR, **投稿日時:** 2024-05-24 07:56


- - -

### [Leakage-Resilient and Carbon-Neutral Aggregation Featuring the Federated AI-enabled Critical Infrastructure](http://arxiv.org/abs/2405.15258)

**漏洩耐性およびカーボンニュートラルな連合AI支援型重要インフラ集約手法**

Zehang Deng, Ruoxi Sun, Minhui Xue, Sheng Wen, Seyit Camtepe, Surya Nepal, Yang Xiang

- 連合学習技術を活用する重要インフラでも、勾配最適化を通じてデータ再構築攻撃のリスクが存在
- 新たな圧縮差分プライベート集約（CDPA）で漏洩耐性、通信効率、カーボンニュートラルを実現
- ランダムビット反転メカニズムによりノイズの分散を拡大し、差分プライバシー保護とエネルギー節約を両立
- CDPAは通信コストを半減させつつモデルの有用性を保持し、最新のデータ再構築攻撃を効果的に防御

省エネとプライバシー保護が同時に進められるなんて、未来のインフラにとって嬉しいポイントだよね。二酸化炭素排出量も大幅に減らせるから、環境面でも優しそう！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CR, **投稿日時:** 2024-05-24 06:35


- - -

### [Federated Offline Policy Optimization with Dual Regularization](http://arxiv.org/abs/2405.17474)

**二重正則化を用いた連合型オフライン政策最適化**

Sheng Yue, Zerui Qin, Xingyuan Hua, Yongheng Deng, Ju Ren

- 連合強化学習（FRL）は、人工物のインターネット時代における知的意思決定の有望な解決策である
- 既存のFRL手法は、ローカル更新の際に環境と繰り返し相互作用する必要があり、現実世界では高コストまたは不可能なことが多い
- これを克服するため、環境と相互作用せずにプライベートかつ静的なデータから意思決定を学ぶ新しいアルゴリズム$\texttt{DRPO}$を提案
- $\texttt{DRPO}$は二重正則化を活用し、ローカルの行動方針とグローバルの集約方針の2つを統合して分布のシフトに対処、理論分析によりパフォーマンス向上を確認

新しいFRLアルゴリズムで環境との相互作用を省くとかすごいね！今後、いろんな分野で応用されそうでワクワクするね。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, **投稿日時:** 2024-05-24 04:24


- - -

### [RFLPA: A Robust Federated Learning Framework against Poisoning Attacks with Secure Aggregation](http://arxiv.org/abs/2405.15182)

**RFLPA：セキュアアグリゲーションを用いた毒素攻撃に対する強靭な連合学習フレームワーク**

Peihua Mai, Ran Yan, Yan Pang

- 連合学習はデータ共有なしで複数デバイスがモデルを共同訓練するが、プライバシー漏洩と毒素攻撃の脆弱性がある
- 毒素攻撃防御策は平文でのローカル更新の解析に依存しているため、セキュアアグリゲーションと互換性がない
- 提案するRFLPAフレームワークはセキュアアグリゲーションプロトコルに基づき、ローカル更新とサーバー更新のコサイン類似度を計算して強靭なアグリゲーションを実施
- 実験結果では、RFLPAは最先端の防御方法（BREA）と比較して通信および計算オーバーヘッドを75%以上削減し、競争力のある精度を維持

毒素攻撃もプライバシーも一挙に解決できるなんて、すごく斬新！これが普及したらデータの扱い方が一段と安心になりそう。

**Comment:** 22 pages

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CR, cs.AI, E.4, **投稿日時:** 2024-05-24 03:31


- - -

### [Momentum-Based Federated Reinforcement Learning with Interaction and Communication Efficiency](http://arxiv.org/abs/2405.17471)

**モーメンタムベースの連合強化学習による相互作用と通信効率の向上**

Sheng Yue, Xingyuan Hua, Lili Chen, Ju Ren

- 連合強化学習（FRL）は注目されているが、データ分布の時空間的非定常性で高い相互作用と通信コストが課題
- モーメンタム、重要度サンプリング、サーバー側調整を利用した新アルゴリズム\alg{}を提案
- モーメンタムパラメータと相互作用頻度の適切な選定により、相互作用と通信の複雑度が大幅に改善
- 実験により、複雑かつ高次元のベンチマークで既存手法を大幅に上回る性能向上を確認

FRLで通信コストを減らせるって期待持てるね！みんなで協力して学習するともっと賢くなれるのかな、ワクワク♪



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, **投稿日時:** 2024-05-24 03:23
