---
title: 連合学習 (2024-09-06 ~ 2024-09-12)
date: 2024-09-06
---

連合学習に関する論文まとめ (2024-09-06 ~ 2024-09-12)


- - -

### [FedProphet: Memory-Efficient Federated Adversarial Training via Theoretic-Robustness and Low-Inconsistency Cascade Learning](http://arxiv.org/abs/2409.08372)

**FedProphet: 理論的ロバストネスと低一貫性カスケード学習によるメモリ効率の高い連合敵対的訓練**

Minxue Tang, Yitu Wang, Jingyang Zhang, Louis DiValentin, Aolin Ding, Amin Hass, Yiran Chen, Hai "Helen" Li

- 連合学習でデータを共有せずにローカルトレーニングし、連合敵対的訓練により敵対的な事例に対するロバスト性を強化
- メモリ制約のあるエッジデバイスでの訓練が遅く、一貫性のないローカルおよびグローバルモデルが課題
- FedProphetは大きなモデルを小さなカスケードモジュールに分割し、メモリ制約のあるデバイスがモジュールごとの敵対的訓練を実施
- 強い凸正則化を導入し、理論的にロバストネスを保証し、適応的撹乱調整と差別モジュール割当てをサーバ側で実施して一貫性を改善

FedProphetはメモリの使用量を大幅に削減しながら、驚くべき速度で訓練を完了できるのはすごいね！80%もメモリを節約できるなんて、未来のAI技術の発展が楽しみだな〜。

**Comment:** Preprint

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, **投稿日時:** 2024-09-12 19:39


- - -

### [Multi-Model based Federated Learning Against Model Poisoning Attack: A Deep Learning Based Model Selection for MEC Systems](http://arxiv.org/abs/2409.08237)

**モデル毒を防ぐためのマルチモデル連合学習: MECシステムにおける深層学習モデル選択**

Somayeh Kianpisheh, Chafika Benzaid, Tarik Taleb

- 連合学習はデータプライバシーを保ちながら分散データからグローバルモデルを訓練する技術
- 単一モデルベースの連合学習では、モデル毒攻撃のリスクが存在
- 提案手法はマスターモデルと複数のスレーブモデルの構成を利用し攻撃緩和を図る
- 強化学習を用いたモデル選択により、DDoS攻撃下でも効率的な結果を示す

モデル毒攻撃をこんな感じで防げちゃう連合学習、すごーい！強化学習とかも使ってるから、これからもっと安全になりそうだね！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.NI, **投稿日時:** 2024-09-12 17:36


- - -

### [Privacy-preserving federated prediction of pain intensity change based on multi-center survey data](http://arxiv.org/abs/2409.07997)

**複数センター調査データに基づいた痛みの変化予測のプライバシー保護型連合学習**

Supratim Das, Mahdie Rafie, Paula Kammer, Søren T. Skou, Dorte T. Grønne, Ewa M. Roos, André Hajek, Hans-Helmut König, Md Shihab Ullaha, Niklas Probul, Jan Baumbacha, Linda Baumbach

- 患者報告データを用いて予後モデルを構築するが、データの分散により集中化が困難
- ローカル学習モデルは精度、堅牢性、一般化能力が劣る
- 連合学習技術により、データがセンターを離れずに予後モデルを構築
- 連合学習モデルはローカルモデルより性能が良く、集中型モデルと同等の成果を示す

連合学習を使うことでプライバシーを守りつつ、精度の高い予測モデルが構築できるなんてすごい！今後の医療分野での応用が楽しみだね。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, **投稿日時:** 2024-09-12 12:41


- - -

### [Over-the-Air Federated Learning via Weighted Aggregation](http://arxiv.org/abs/2409.07822)

**無線による連合学習の重み付け集約を介して**

Seyed Mohammad Azimi-Abarghouyi, Leandros Tassiulas

- 無線計算を活用した新しい連合学習手法を紹介
- 集約時に適応的な重みを使用し、無線チャネル条件の影響を軽減
- 計算の異種性と一般的な損失関数に対する収束境界を数学的に導出
- 提案手法は従来の手法に比べ、最大30%の精度向上を実現

無線チャネル条件が厳しくても、これだけ精度が上がるなんてめっちゃ興味湧くよね。数学的な部分も多そうだけど、それを乗り越えたらかなり成果が期待できそう！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.IT, cs.AI, cs.LG, math.IT, **投稿日時:** 2024-09-12 08:07


- - -

### [FedHide: Federated Learning by Hiding in the Neighbors](http://arxiv.org/abs/2409.07808)

**FedHide: 近隣に隠れて連合学習する方法**

Hyunsin Park, Sungrack Yun

- クライアントが単一クラスのデータを持つシナリオで、埋め込みネットワークを開発
- 真のクラスプロトタイプ共有がプライバシーを侵害する可能性を回避するため、代理クラスプロトタイプを提案
- 近隣との線形結合によって代理クラスプロトタイプを生成、真のクラスプロトタイプを隠す
- CIFAR-100、VoxCeleb1、VGGFace2の3つのベンチマークデータセットで効果を実証

クラスプロトタイプを隠すことでプライバシー保護しながら学習できるなんてすごい！その上、実験結果もあり信頼できるね。

**Comment:** ECCV 2024

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, **投稿日時:** 2024-09-12 07:37


- - -

### [DFDG: Data-Free Dual-Generator Adversarial Distillation for One-Shot Federated Learning](http://arxiv.org/abs/2409.07734)

**DFDG: データフリーのデュアルジェネレータ敵対的蒸留を用いたワンショット連合学習**

Kangyang Luo, Shuai Wang, Yexuan Fu, Renrong Shao, Xiang Li, Yunshi Lan, Ming Gao, Jinlong Shu

- 連合学習は、クライアントがプライベートデータセットを共有せずにモデル情報を共有してグローバルモデルを共同訓練する分散機械学習スキームである
- 通信とプライバシーの懸念から、単一通信ラウンドによるワンショット連合学習が有望視されている
- 既存のワンショット連合学習法は、公開データセットの必要性や同質なモデル設定に依存し、ローカルモデルからの知識蒸留が限られているため、実用的ではない
- 提案するDFDGは、デュアルジェネレータの訓練を通じてローカルモデルの訓練空間を広げ、性能の向上を達成する

データフリーでどんなデータでも対応できるのかが気になる〜。いっぱい検証したみたいだから、他のベースラインに勝ったのも納得だね。期待でワクワクしちゃう！

**Comment:** Accepted by ICDM2024 main conference (long paper)

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.DC, cs.LG, **投稿日時:** 2024-09-12 03:44


- - -

### [HERL: Tiered Federated Learning with Adaptive Homomorphic Encryption using Reinforcement Learning](http://arxiv.org/abs/2409.07631)

**HERL: 強化学習を用いた適応準同型暗号を用いた階層的連合学習**

Jiaxang Tang, Zeshan Fayyaz, Mohammad A. Salahuddin, Raouf Boutaba, Zhi-Li Zhang, Ali Anwar

- 準同型暗号の統合はデータ機密性を確保するが、計算と通信のオーバーヘッドが増大
- クライアントの計算能力とセキュリティニーズの違いに対応するため、強化学習を用いる
- Q学習によって暗号化パラメータを動的に最適化し、階層別にクライアントを分類
- HERLは計算オーバーヘッドを大幅に削減し、効率性とユーティリティを向上

強化学習で連合学習が効率的になるってすごくない？計算のオーバーヘッドも減るから、リアルタイムのアプリとかにも使えそうだね！



**トピック:** [連合学習](../../fl), [準同型暗号](../../he), **カテゴリ:** cs.CR, cs.DC, **投稿日時:** 2024-09-11 21:26


- - -

### [Federated Impression for Learning with Distributed Heterogeneous Data](http://arxiv.org/abs/2409.07351)

**分散異種データを用いた学習のための連合的印象**

Sana Ayromlou, Atrin Arya, Armin Saadat, Purang Abolmaesumi, Xiaoxiao Li

- 標準的な深層学習ベースの分類アプローチは、すべてのサンプルを集中収集する必要があり、現実の臨床応用には非現実的
- FL（連合学習）はデータを共有せずにクライアント間で分散データから学習でき、プライバシーとデータの所有権問題を緩和できる
- データヘテロジニティにより、局所トレーニング中に破滅的忘却が発生しやすくなる
- FedImpresを提案し、合成データを連合的印象として復元し、グローバルな情報を表現することで破滅的忘却を軽減

データヘテロジニティ問題も解決して、FLで20%も精度がアップするなんてすごくない？それに、医療分野での実用化が進むと患者さんにも優しいね！



**トピック:** [連合学習](../../fl), [合成データ](../../sd), **カテゴリ:** cs.LG, cs.AI, cs.CV, cs.DC, **投稿日時:** 2024-09-11 15:37


- - -

### [Federated -armed Bandit with Flexible Personalisation](http://arxiv.org/abs/2409.07251)

**柔軟なパーソナライゼーションを備えた連合$\mathcal{X}$-armedバンディット**

Ali Arabzadeh, James A. Grant, David S. Leslie

- 個別クライアントの嗜好と集約された全体的な知識を組み合わせた代替目的関数を使用
- パーソナライゼーションと集団学習の間で柔軟なトレードオフを可能にする方法を提案
- サブリニアリグレットと対数的通信オーバーヘッドを達成するフェーズベースの除去アルゴリズムを提案
- ヘルスケア、スマートホームデバイス、eコマースなどの多様な分野での応用が期待される

クライアントごとの個別化に対してしっかりとしたアプローチを取っていて面白そう！特にスマートホームデバイスで、どんどん私たちの生活にも役立ちそうだね！



**トピック:** [連合学習](../../fl), **カテゴリ:** stat.ML, cs.LG, **投稿日時:** 2024-09-11 13:19


- - -

### [Riemannian Federated Learning via Averaging Gradient Stream](http://arxiv.org/abs/2409.07223)

**リーマン連合学習による勾配ストリームの平均化**

Zhenwei Huang, Wen Huang, Pratik Jawanpuria, Bamdev Mishra

- リーマン多様体上の問題を対象にRFedAGSアルゴリズムを提案
- 固定ステップサイズで近似定常解に対して収束速度がサブリニア
- 減衰ステップサイズを用いる場合、グローバル収束が証明される
- 数値シミュレーションで合成データと実データを使った性能評価

リーマン多様体上の連合学習って面白そうだね！本当に収束するかどうか、自分のデータで試してみたいな。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, math.OC, **投稿日時:** 2024-09-11 12:28


- - -

### [Heterogeneity-Aware Coordination for Federated Learning via Stitching Pre-trained blocks](http://arxiv.org/abs/2409.07202)

**連合学習における異質性対応調整：事前学習済みブロックのスティッチングによる手法**

Shichen Zhan, Yebo Wu, Chunlin Tian, Yan Zhao, Li Li

- 連合学習は複数デバイスで協力してモデルを学習しデータプライバシーを保護
- 訓練中の大きなメモリ使用量と高いエネルギー消費が低スペックデバイスを排除
- FedStitchは事前学習済みブロックを活用し、新たなタスクに対しモデルを構築
- 従来の手法よりも精度が最大20.93%向上し、メモリ使用量が79.5%削減

FedStitch、めっちゃ面白そう！低スペックのデバイスも活用できるようになったら、もっと多様で強力なモデルができるかもね！想像するだけでワクワクする～。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, **投稿日時:** 2024-09-11 11:47


- - -

### [Privacy-Preserving Federated Learning with Consistency via Knowledge Distillation Using Conditional Generator](http://arxiv.org/abs/2409.06955)

**条件生成器を用いた知識蒸留による一貫性を持つプライバシー保護連合学習**

Kangyang Luo, Shuai Wang, Xiang Li, Yunshi Lan, Ming Gao, Jinlong Shu

- 連合学習（FL）は分散学習フレームワークだが、プライバシー保護の課題がある
- FedMD-CGという新しいFL方法を提案、モデル性能とプライバシー保護を両立
- クライアントのローカルモデルを特徴抽出器と分類器に分離、条件生成器を活用
- 豊富な実験でFedMD-CGの優越性を実証、多様な画像分類タスクで効果を確認

条件生成器とか知識蒸留とか、難しそうだけどすごく挑戦的な感じ！これが実用化されたら、もっと安全にデータを使えて、色んな分野で役に立ちそうだね。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-09-11 02:36


- - -

### [Applied Federated Model Personalisation in the Industrial Domain: A Comparative Study](http://arxiv.org/abs/2409.06904)

**産業分野における連合学習モデルの個別化応用: 比較研究**

Ilias Siniosoglou, Vasileios Argyriou, George Fragulis, Panagiotis Fouliras, Georgios Th. Papadopoulos, Anastasios Lytos, Panagiotis Sarigiannidis

- 複雑な機械学習および深層学習モデルのトレーニングと展開は時間がかかり、特に連合学習の分野で課題が多い
- Active Learning、Knowledge Distillation、Local Memorizationという3つの方法が、効率的な最適化とトレーニング時間の短縮を目指して提案されている
- これらの方法により、計算リソースを少なくしながらモデルを個別化し、現行モデルの有効性を向上させることができる
- 新しい連合学習システムを提案し、これらの個別化方法の効果をローカルおよび連合ドメインで比較し、高精度とユーザー体験向上を目指している

連合学習で個別に最適化する方法がたくさん試されてて、すごく実践的だと思う！特にNG-IoTアプリでの効果が気になるな～！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, cs.PF, **投稿日時:** 2024-09-10 23:00


- - -

### [Personalized Federated Learning Techniques: Empirical Analysis](http://arxiv.org/abs/2409.06805)

**パーソナライズド連合学習技術：実証分析**

Azal Ahmad Khan, Ahmad Faraz Khan, Haider Ali, Ali Anwar

- 個人化された連合学習（pFL）は、データプライバシーを保ちながら個別のユーザーニーズに応じたモデルを構築可能
- pFLの性能を最大化するには、メモリのオーバーヘッドとモデルの精度のバランスが重要
- 10の主要なpFL技術を様々なデータセットとデータ分割で評価した結果、大きな性能差が見られた
- データの不均一性や潜在的な攻撃に直面する微調整法よりも、個別の集計技術が通信と計算の効率性で最速の収束を実現

pFLの通信効率が資源使用に大きく影響するのは面白そうだよね！実践に役立つ具体的な技術がどんどん出てきたらいいな。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, cs.CR, **投稿日時:** 2024-09-10 18:16


- - -

### [Advancing Hybrid Defense for Byzantine Attacks in Federated Learning](http://arxiv.org/abs/2409.06474)

**連合学習におけるビザンチン攻撃へのハイブリッド防御の進展**

Kai Yue, Richeng Jin, Chau-Wai Wong, Huaiyu Dai

- 連合学習は複数のクライアントがデータを共有せずに共同でグローバルモデルを訓練
- 攻撃者戦略や悪意のあるクライアント数を事前に知らないFLシナリオでの攻撃耐性を調査
- 状況に応じた防御選択により、大量の毒性更新があってもサーバーの頑強性を立証
- 新しいTrapsetter攻撃は既存のFL防御をすり抜け、モデル精度をさらに8-10%低下

新しい防御メカニズムがくしゃみしないで強力ってすごいよね！まだまだ課題はいろいろあるけど、この研究を基にもっと安全な連合学習ができるようになるかもね！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.DC, **投稿日時:** 2024-09-10 13:04


- - -

### [Compute-Update Federated Learning: A Lattice Coding Approach](http://arxiv.org/abs/2409.06343)

**計算更新連合学習：格子符号化アプローチ**

Seyed Mohammad Azimi-Abarghouyi, Lav R. Varshney

- 無線通信を介した計算を可能にする新たな連合学習フレームワークを提案
- 格子符号を使い、モデルパラメータを量子化してデバイス間の干渉を活用
- サーバー上で量子化モデルパラメータの整数の組み合わせを格子点として集約する受信構造を提案
- 提案手法がさまざまなパラメータで一貫した学習精度を提供し、他の無線通信手法を上回ることを実証

格子符号化を使ってエラー訂正しながら連合学習するなんて、面白そう！通信状態が変わっても一貫して精度がいいってすごいね！

**Comment:** Extended version of the preprint available at arXiv:2403.01023

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.IT, cs.AI, cs.LG, math.IT, **投稿日時:** 2024-09-10 08:52


- - -

### [Rate-Constrained Quantization for Communication-Efficient Federated Learning](http://arxiv.org/abs/2409.06319)

**通信効率の高い連合学習のためのレート制約付き量子化**

Shayan Mohajer Hamidi, Ali Bereyhi

- 連合学習の通信コストを軽減するために量子化を使用する
- 量子化されたローカルパラメータはヒュフマン符号などのエントロピー符号化技術でさらに圧縮
- 新たな量子化FLフレームワーク「RC-FED」を提案し、信号忠実度とデータレートの両方を制約
- RC-FEDは、最適化を通じて量子化歪みを最小化しつつ通信コスト目標を下回ることを実現

量子化フレームワークであるRC-FEDの提案とか、通信コストを最適化しつつ精度も保つのめっちゃ面白そう！他の手法と比較してもパフォーマンスが良いなら期待できるよね！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, eess.SP, **投稿日時:** 2024-09-10 08:22


- - -

### [On Applying Bandit Algorithm to Fault Localization Techniques](http://arxiv.org/abs/2409.06268)

**バンディットアルゴリズムの故障位置特定技術への適用について**

Masato Nakao, Kensei Hamamoto, Masateru Tsunoda, Amjed Tahir, Koji Toda, Akito Monden, Keitaro Nakasai, Kenichi Matsumoto

- 開発者は高性能な故障位置特定（FL）技術を選ぶ必要がある
- 伝統的な方法はデバッグ前に一つのFL技術を選ぶこと
- 本研究はデバッグ中に動的により良いFL技術を選択する新しい方法を提案
- 新しいアプローチは動的選択により性能向上が期待される

バンディットアルゴリズム使うのってなんかゲームみたいで面白そうじゃない？この新しいアプローチでデバッグがもっと楽になるといいね！

**Comment:** 2 pages, 2 figures, 1 table

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.SE, **投稿日時:** 2024-09-10 07:15


- - -

### [Contrastive Federated Learning with Tabular Data Silos](http://arxiv.org/abs/2409.06123)

**タブラー形式データサイロを用いた対照的連合学習**

Achmad Ginanjar, Xue Li, Wen Hua

- データサイロからの学習は、ラベル無し、独立非同分布(NIID)、垂直分割等の課題がある
- プライバシー保護が課題を複雑化し、協力的な作業の意欲を阻害
- 対照学習を用いてラベルコスト問題に対処するが、タブラー形式には適用困難
- 半教師付き対照的連合学習(CFL)を提案し、精度面での改善を実証

対照的連合学習がこれまでの方法より効果的だなんて、未来のデータ解析がもっと楽しみになるね！クライアント環境の複雑さも克服できるところがポイント高いね。

**Comment:** 18 Pages. Was submitted on Artificial Intelligence Journal, Jan 29,   2024, ARTINT-D-24-00098

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, 68A00, I.1.1, **投稿日時:** 2024-09-10 00:24


- - -

### [MLLM-FL: Multimodal Large Language Model Assisted Federated Learning on Heterogeneous and Long-tailed Data](http://arxiv.org/abs/2409.06067)

**MLLM-FL: 異質で長尾分布データにおけるマルチモーダル大規模言語モデルを活用した連合学習**

Jianyi Zhang, Hao Frank Yang, Ang Li, Xin Guo, Pu Wang, Haiming Wang, Yiran Chen, Hai Li

- 異なるクライアント間のデータの異質性により発生するパフォーマンス低下に焦点
- MLLMをサーバー側に導入し、異質性や長尾分布の課題に対応
- オープンソースデータと強力なサーバー計算資源を活用し、プライバシー漏洩や端末負担増加を回避
- 実験評価により、データの異質性や長尾分布下でのパフォーマンス向上を確認

MLLM使うとかめっちゃ面白そう！データ多様でもちゃんと学習できるってすごいなぁ。実際のアプリケーションでどう活かせるんだろうね。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.AI, **投稿日時:** 2024-09-09 21:04


- - -

### [FLoRA: Federated Fine-Tuning Large Language Models with Heterogeneous Low-Rank Adaptations](http://arxiv.org/abs/2409.05976)

**FLoRA: 異質な低ランク適応による連合型大規模言語モデルの微調整**

Ziyao Wang, Zheyu Shen, Yexiao He, Guoheng Sun, Hongyi Wang, Lingjuan Lyu, Ang Li

- LLMの急速な発展がAIの進歩を促進し、連合学習によりプライバシーを保護しながら微調整を可能に
- クライアントのリソースに制約があるため、既存の低ランク適応手法は効果が低下
- FLORAという新しいアプローチを提案、スタッキングベースの集約方法で異質なLoRAアダプタ対応を実現
- 実験により、同質および異質の設定の両方で既存手法を上回る性能を示す

FLORAって、何だか未来的でワクワクするよね！プライバシーと効率性を両立できるなんて素敵✨



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-09-09 18:21


- - -

### [pFedGPA: Diffusion-based Generative Parameter Aggregation for Personalized Federated Learning](http://arxiv.org/abs/2409.05701)

**個別連合学習のための拡散ベース生成パラメータ集約：pFedGPA**

Jiahao Lai, Jiaqi Li, Jian Xu, Yanru Wu, Boshi Tang, Siqi Chen, Yongfeng Huang, Wenbo Ding, Yang Li

- 連合学習 (FL) は、データをローカルに保持しモデルパラメータのみを共有する分散アプローチ。
- 従来の線形集約方法（FedAvgなど）は、異質なデータ分布でのパラメータ空間の複雑さを見落とす可能性がある。
- 拡散モデルを使用し、サーバー上で多様なパラメータ分布を統合し、個別のパラメータ生成を提案。
- 提案手法（pFedGPA）は高次元拡散モデルを使用し、各クライアントのデータ分布依存を効果的に解消、実験で優れた性能を実証。

これはめっちゃ面白そう！これからの個別対応の連合学習の常識を変えるかもね。実績あるって心強いなー。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, **投稿日時:** 2024-09-09 15:13


- - -

### [HMAFlow: Learning More Accurate Optical Flow via Hierarchical Motion Field Alignment](http://arxiv.org/abs/2409.05531)

**HMAFlow: 階層的動きフィールド整合によるより正確なオプティカルフローの学習**

Dianbo Ma, Kousuke Imamura, Ziyan Gao, Xiangjie Wang, Satoshi Yamane

- オプティカルフロー推定を改良する新しい手法HMAFlowを提案
- 階層的動きフィールド整合モジュールと相関セルフアテンションモジュールを導入
- Multi-Scale Correlation Search層で4Dコストボリュームを再構築し精度向上
- RAFTと比較してSintelおよびKITTIベンチマークで誤差を大幅に削減

新しい手法でオプティカルフロー推定がずっと精度高くなるなんてすごいよね！コード公開も嬉しいし、これからの研究がさらに進展しそうで楽しみ～。

**Comment:** 11 pages, 6 figures

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CV, cs.AI, **投稿日時:** 2024-09-09 11:43


- - -

### [FedBrain-Distill: Communication-Efficient Federated Brain Tumor Classification Using Ensemble Knowledge Distillation on Non-IID Data](http://arxiv.org/abs/2409.05359)

**FedBrain-Distill: 非識別データに対するアンサンブル知識蒸留を用いた通信効率の良い連合型脳腫瘍分類**

Rasoul Jafari Gohari, Laya Aliahmadipour, Ezat Valipour

- 頭部の複雑さにより、脳腫瘍の分類は依然として大きな課題である
- 連合学習（FL）がプライバシー保護を図るが、通信コストやモデルの依存性が問題
- 提案のFedBrain-Distillはアンサンブル知識蒸留でモデルの独立性とプライバシーを維持
- 実験結果は、現実世界のデータセットで高精度と低通信コストの両方を実証

脳腫瘍の分類ってめっちゃ難しいんだって。FedBrain-Distillなら通信コストもかからないし、色んなモデルを使えるってところが面白いね！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CV, **投稿日時:** 2024-09-09 06:42


- - -

### [TriplePlay: Enhancing Federated Learning with CLIP for Non-IID Data and Resource Efficiency](http://arxiv.org/abs/2409.05347)

**TriplePlay: 非独立同一分布データとリソース効率のためのCLIPを用いた連合学習の強化**

Ahmed Imteaj, Md Zarif Hossain, Saika Zaman, Abdur R. Shahid

- 連合学習でCLIPのような大規模基盤モデルを統合して、プライバシー、効率性、適応性を向上
- 非IIDデータ分布、計算・通信の負荷、データセット内のクラスの偏りに焦点を当てている
- TriplePlayフレームワークを提案し、CLIPをアダプタとして統合し公平性とリソース要求を軽減
- シミュレーション結果はGPU使用コストの大幅削減と学習過程の高速化、通信オーバーヘッドの削減を示す

連合学習にCLIPを統合するなんて、すごく未来的でワクワクするね！効率性の課題も解決しそうだし、これからの研究が楽しみだな～。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, **投稿日時:** 2024-09-09 06:04


- - -

### [Towards Practical Overlay Networks for Decentralized Federated Learning](http://arxiv.org/abs/2409.05331)

**実用的な分散型連合学習のためのオーバーレイネットワークへの挑戦**

Yifan Hua, Jinlong Pang, Xiaoxue Zhang, Yi Liu, Xiaofeng Shi, Bao Wang, Yang Liu, Chen Qian

- 分散型連合学習（DFL）はピアツーピア通信を使用し、単一点故障問題を回避
- DFLのオーバーレイネットワークにおける高速トレーニングと低通信、分散型構築と保守の課題解決
- FedLayというネットワークを提案し、分散型で近似ランダムなレギュラートポロジーの構築とノード参加・故障時の維持を実現
- 実験ではFedLayが他のDFLソリューションと比較して最速のモデル収束、高精度、低通信コスト、ノードへの耐性を示した

分散型連合学習でオーバーレイネットワークの欠点を解決するFedLay、頼もしいよね！みんなのデバイスでの機械学習がもっとスムーズになる未来が楽しみだね。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.DC, cs.NI, **投稿日時:** 2024-09-09 05:20


- - -

### [FedFT: Improving Communication Performance for Federated Learning with Frequency Space Transformation](http://arxiv.org/abs/2409.05242)

**FedFT：周波数空間変換による連合学習の通信性能の向上**

Chamath Palihawadana, Nirmalie Wiratunga, Anjana Wijekoon, Harsha Kalutarage

- 連合学習における通信効率問題に対応するためのFedFTを提案
- モデルパラメータを離散コサイン変換（DCT）で周波数空間に変換し効率的に圧縮
- FedFTは既存の連合学習手法やニューラルアーキテクチャと互換性があり、線形特性を持つ
- 通信オーバーヘッドの削減とモデル精度の維持、時には向上が見られ、削減率はデータセットにより5%から30%ほど

周波数変換を使うことで効率的に通信コストを減らしてるのが面白そう！これで連合学習がもっと広がる可能性があるね。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.DC, cs.AI, **投稿日時:** 2024-09-08 23:05


- - -

### [Some Results on Neural Network Stability, Consistency, and Convergence: Insights into Non-IID Data, High-Dimensional Settings, and Physics-Informed Neural Networks](http://arxiv.org/abs/2409.05030)

**ニューラルネットワークの安定性、一貫性、収束性に関するいくつかの結果：非IIDデータ、高次元設定、および物理情報ニューラルネットワークへの洞察**

Ronald Katende, Henry Kasumba, Godwin Kakuba, John M. Mango

- 非凸設定における動的学習率でのニューラルネットワークの一様安定性について新しい理論的結果を提供
- 分散シフトと曲率効果を考慮し、非ユークリッド空間での連合学習モデルの一貫性境界を確立
- 物理情報ニューラルネットワーク（PINNs）において、ノイズ環境下での偏微分方程式（PDE）の解法に対する安定性、一貫性、および収束性を保証
- 複雑で非理想的な条件下でのモデル挙動の理解における重要なギャップを埋め、より堅牢で信頼性の高い機械学習応用を実現するための道を開く

物理情報ニューラルネットワーク（PINNs）がノイズの中でもきちんと動くなんて未来が広がりそう！データの揺らぎが多いリアルな世界でも、深層学習モデルが信頼できるなんて素敵だね。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.NA, math.NA, **投稿日時:** 2024-09-08 08:48


- - -

### [DynamicFL: Federated Learning with Dynamic Communication Resource Allocation](http://arxiv.org/abs/2409.04986)

**DynamicFL: ダイナミックな通信資源配分による連合学習**

Qi Le, Enmao Diao, Xinran Wang, Vahid Tarokh, Jie Ding, Ali Anwar

- 連合学習（FL）は、複数のユーザーがローカルデータを利用して分散的にモデルを訓練する枠組み
- ローカルデータの統計的不均一性がモデル性能を低下させる原因
- DynamicFLは、FedSGDとFedAvgの通信資源配分を動的に調整し、パフォーマンス向上を図る手法を提案
- 実験結果でモデルの精度が現行手法を最大10％上回ることを示し、DynamicFLの有効性を確認

通信リソースをうまく使いながら、性能も良くするなんてすごいと思う！どんな実験や条件でその結果になったのか、詳しく知りたいな。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, **投稿日時:** 2024-09-08 05:53


- - -

### [Balancing Security and Accuracy: A Novel Federated Learning Approach for Cyberattack Detection in Blockchain Networks](http://arxiv.org/abs/2409.04972)

**セキュリティと精度のバランス調整: ブロックチェーンネットワークにおけるサイバー攻撃検出のための新しい連合学習アプローチ**

Tran Viet Khoa, Mohammad Abu Alsheikh, Yibeltal Alem, Dinh Thai Hoang

- 新しいCollaborative Cyberattack Detection (CCD)システムを提案し、ブロックチェーンベースのデータ共有ネットワークのセキュリティを強化
- 差分プライバシーの理論を活用し、訓練済みのサブモデルにノイズを追加後、グローバルモデルを再構築
- ガウス、ラプラス、モーメントアカウンタなどのノイズの種類が性能指標に与える影響を体系的に調査
- データ保護とシステムの効率性の最適なバランスを達成するための実用的な推奨事項を提供

ブロックチェーンと連合学習が合わさると最強になりそう！データを守りながらも高い精度を維持するなんて未来が楽しみ！

**Comment:** 13 pages

**トピック:** [連合学習](../../fl), [差分プライバシー](../../dp), **カテゴリ:** cs.CR, cs.LG, **投稿日時:** 2024-09-08 04:38


- - -

### [Fair Allocation of Bandwidth At Edge Servers For Concurrent Hierarchical Federated Learning](http://arxiv.org/abs/2409.04921)

**エッジサーバでの同時階層連合学習のための公正な帯域幅割り当て**

Md Anwar Hossen, Fatema Siddika, Wensheng Zhang

- 三層システム内で同時連合学習プロセスを探る
- エッジデバイスからエッジサーバへの帯域幅が限られていることが課題
- ゲーム理論的アプローチで帯域幅割り当て問題をモデル化
- 分散型と集中型のヒューリスティックスキームが基準スキームを上回る

帯域幅割り当て問題を解決するこのゲーム理論的アプローチ、ちょっとかっこいいよね。集中型と分散型で競技力を持つって、実用性が高まりそうでワクワクする！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.GT, **投稿日時:** 2024-09-07 22:03


- - -

### [Unlocking the Potential of Model Calibration in Federated Learning](http://arxiv.org/abs/2409.04901)

**連合学習におけるモデルキャリブレーションの可能性を解き放つ**

Yun-Wei Chu, Dong-Jun Han, Seyyedali Hosseinalipour, Christopher Brinton

- 連合学習（FL）はモデル精度の向上を目的とするが、予測の信頼性が重要
- NUCFLというフレームワークでモデルキャリブレーションを統合
- データの異質性に対応するため、クライアントのローカルモデルとグローバルモデルの統計的関係を利用
- 実験でNUCFLがFLアルゴリズムで精度とモデルキャリブレーションの両方を向上させることを確認

モデルの信頼性も考慮するってすごく大事だよね！これで連合学習がもっと実用的になるかも。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, **投稿日時:** 2024-09-07 20:11


- - -

### [FedModule: A Modular Federated Learning Framework](http://arxiv.org/abs/2409.04849)

**FedModule: モジュラーフェデレーテッドラーニングフレームワーク**

Chuyi Chen, Zhe Zhang, Yanchao Zhao

- 連合学習がヘルスケアや金融、スマートシティなどで広く採用されている
- FedModuleは多様な連合学習パラダイムをサポートし、包括的なベンチマークを提供する
- モジュラー設計により、異なる連合学習パラダイムの統合が容易である
- 公開データセットでの実験により、FedModuleの柔軟性と拡張性が立証された

FedModuleの紹介を読んでみると、新しい実験がたくさんできそうでワクワクするね！他の連合学習ツールキットより優れているみたいだし、今後も注目しちゃお！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, **投稿日時:** 2024-09-07 15:03


- - -

### [Enhancing Quantum Security over Federated Learning via Post-Quantum Cryptography](http://arxiv.org/abs/2409.04637)

**連合学習における量子セキュリティ強化のためのポスト量子暗号の活用**

Pingzhi Li, Tianlong Chen, Junyu Liu

- 連合学習はエッジデバイス上で機械学習モデルを展開する標準的なアプローチ
- 伝達されるモデル更新が悪意ある改ざんに対して脆弱で、グローバルモデルの整合性にリスク
- ポスト量子暗号のNIST標準化アルゴリズム（Dilithium、FALCON、SPHINCS+）がこの脆弱性に対処
- デジタル署名においてDilithiumが最も効率的なポスト量子暗号アルゴリズムであることを実証

ポスト量子暗号に興味ある！これからの連合学習はもっと安心になるかもね。未来が楽しみ～！

**Comment:** Submission for IEEE 2024 IEEE Workshop on Quantum IntelLigence,   Learning & Security (QUILLS), https://sites.google.com/pitt.edu/quills/home

**トピック:** [連合学習](../../fl), **カテゴリ:** quant-ph, cs.AI, cs.CR, cs.LG, **投稿日時:** 2024-09-06 22:02


- - -

### [Active-Passive Federated Learning for Vertically Partitioned Multi-view Data](http://arxiv.org/abs/2409.04111)

**アクティブ-パッシブ連合学習による垂直分割されたマルチビューデータの処理**

Jiyuan Liu, Xinwang Liu, Siqi Wang, Xingchen Hu, Qing Liao, Xinhang Wan, Yi Zhang, Xin Lv, Kunlun He

- 垂直連合学習は、デバイス間で垂直分割されたマルチビューデータを統合し、プライバシーを保護しながら利用するアプローチ
- 既存の方法では、モデル推論においてすべてのクライアントの協力が必要であり、現実世界のシナリオでは困難が生じる可能性がある
- アクティブクライアントが学習タスクを開始し、完全なモデルを構築し、受動クライアントは補助役として機能する
- 再構築損失およびコントラスト損失を用いた2つの分類方法を実験でテストして望ましい結果を達成

垂直連合学習の新しい方法だね！クライアント間の協力が減るのはすごく便利だし、実践的だよね。なんか、これから色んな応用が考えられそうでワクワクするね！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, **投稿日時:** 2024-09-06 08:28


- - -

### [Heterogeneity-Aware Cooperative Federated Edge Learning with Adaptive Computation and Communication Compression](http://arxiv.org/abs/2409.04022)

**異質性対応型協調連合エッジ学習：適応計算および通信圧縮を用いたアプローチ**

Zhenxiao Zhang, Zhidong Gao, Yuanxiong Guo, Yanmin Gong

- クラウドベースの連合学習の欠点を改良するため、協調連合エッジ学習（CFEL）を提案
- CFELは動的かつ異質なデバイス特性による収束速度低下とリソース消費の増加に直面
- 適応計算と通信圧縮を用いてモデル精度を最大化しつつ、トレーニング時間とエネルギー消費を最小化
- HCEFはローカル更新頻度と圧縮率を動的に調整し、高いモデル精度を維持しつつトレーニング遅延とエネルギー効率を改善

協調するエッジサーバーがいっぱいあれば、もっと早く正確なモデルが作れるようになるんだね！こんな技術が普及したら、私たちの日常に役立つサービスももっとたくさん出てくるかも。

**Comment:** 20 pages, 7 figures

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.DC, cs.LG, **投稿日時:** 2024-09-06 04:26
