---
title: 連合学習 (2024-12-20 ~ 2024-12-26)
date: 2024-12-20
---

連合学習に関する論文まとめ (2024-12-20 ~ 2024-12-26)


- - -

### [FedVCK: Non-IID Robust and Communication-Efficient Federated Learning via Valuable Condensed Knowledge for Medical Image Analysis](http://arxiv.org/abs/2412.18557)

**FedVCK: 医学画像解析のための貴重な凝縮知識を活用した非独立同分布に強く通信効率の良い連合学習**

Guochen Yan, Luyuan Xie, Xinyi Gao, Wentao Zhang, Qingni Shen, Yuejian Fang, Zhonghai Wu

- 医療機関間の連携においてデータは非独立同分布であり、クライアントドリフトや成績低下を招く
- 提案するFedVCKはモデルが導く最も必要な知識を選び、通信コストを抑えつつ非IID問題を効果的に解決
- クライアント側で知識を凝縮した小さなデータセットを作成し、不要な通信を最小限に抑える
- サーバー側での関係的教師付きコントラスト学習により、モデル更新のための監視信号を強化

貴重な知識を凝縮することで通信頻度を抑えつつ、性能向上を目指すアイデアがすごいね！医療現場でのデータ共有のハードルをうまくクリアしてるのが興味深いなぁ。

**Comment:** Accepted by AAAI 2025

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, **投稿日時:** 2024-12-24 17:20


- - -

### [FedGIG: Graph Inversion from Gradient in Federated Learning](http://arxiv.org/abs/2412.18513)

**FedGIG: 連合学習における勾配からのグラフ逆推論**

Tianzhe Xiao, Yichen Li, Yining Qi, Haozhao Wang, Ruixuan Li

- 連合学習は勾配逆推論攻撃に脆弱で、プライベートデータが漏れる可能性がある
- 従来の手法は画像やテキスト向けで、グラフデータには直接適用できない
- FedGIGはグラフ構造データに特化した新しい逆推論手法である
- テストではFedGIGが既存の手法よりも高い精度を持つことを確認した

連合学習でのグラフデータのプライバシーを守るための技術なんて新鮮！グラフ構造特有のデータ扱いに特化しているところが特に興味深いなぁ。どんな応用ができるのかワクワクするね。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.CR, **投稿日時:** 2024-12-24 15:52


- - -

### [An Empirical Analysis of Federated Learning Models Subject to Label-Flipping Adversarial Attack](http://arxiv.org/abs/2412.18507)

**ラベルフリップ攻撃を受ける連合学習モデルの実証的分析**

Kunal Bhatnagar, Sagana Chattanathan, Angela Dang, Bhargav Eranki, Ronnit Rana, Charan Sridhar, Siddharth Vedam, Angie Yao, Mark Stamp

- 連合学習モデルに対するラベルフリップ攻撃を実証的に分析
- MLR、SVC、MLP、CNN、RNN、ランダムフォレスト、XGBoost、LSTMを用いて実験
- 10から100の連合クライアント、ラベルの変更率10%から100%で攻撃をシミュレート
- モデルによって対抗能力が異なり実用的な影響を考察

ラベルフリップ攻撃がどのモデルにどんな影響を与えるのか、リアルに分析しているのが面白いね。同じアルゴリズムでもこれだけ違いがあるのなら、攻撃対策もさらに必要だなって思う！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, **投稿日時:** 2024-12-24 15:47


- - -

### [GeFL: Model-Agnostic Federated Learning with Generative Models](http://arxiv.org/abs/2412.18460)

**GeFL: ジェネレーティブモデルを用いたモデルアグノスティックな連合学習**

Honggu Kang, Seohyeon Cha, Joonhyuk Kang

- 連合学習はユーザープライバシーを守る分散学習の有望な手法
- 大規模モデルは一部のユーザーには負担で、異なる計算能力のために異種モデルが必要
- ジェネレーティブモデルを用いるGeFLは異種モデル間の知識統合を図る
- GeFL-Fは特徴生成モデルでプライバシーとスケーラビリティを改善する

連合学習って未来的だよね！いろんな能力のデバイスが一緒に学習できるって、まるでチームプレイみたいでおもしろいなぁ。技術がもっと進むと、セキュリティもさらに向上して安心かな。

**Comment:** 20 pages

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, **投稿日時:** 2024-12-24 14:39


- - -

### [Addressing Spatial-Temporal Data Heterogeneity in Federated Continual Learning via Tail Anchor](http://arxiv.org/abs/2412.18355)

**連合継続学習における時空間データの不均一性への対応**

Hao Yu, Xin Yang, Le Zhang, Hanlin Gu, Tianrui Li, Lixin Fan, Qiang Yang

- 連合継続学習は、各クライアントがタスクストリームから知識を継続的に更新する手法
- クライアント間の空間データの不均一性とタスク間の時間データの不均一性が課題
- Federated Tail Anchor (FedTA)を提案し、モデルのパラメータと出力の忘却を克服
- 入力強化や選択的入力知識融合などを含むFedTAが、既存手法より優れた性能を実証

連合学習の課題を解決する新しい手法が提案されてて面白そう！時空間の不均一性をどう克服するのかっていう技術が、実用に向けた大きな一歩になりそうだね。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CV, cs.AI, cs.LG, **投稿日時:** 2024-12-24 11:35


- - -

### [Asynchronous Federated Learning: A Scalable Approach for Decentralized Machine Learning](http://arxiv.org/abs/2412.17723)

**非同期連合学習: 分散機械学習のためのスケーラブルなアプローチ**

Ali Forootani, Raffaele Iervolino

- 連合学習は多様なクライアント間で生データを共有せずにモデルを訓練する方法だが、同期的なクライアント更新で効率が悪い
- 提案する非同期連合学習（AFL）は、クライアントが独立かつ非同期にモデル更新を行えるアルゴリズム
- AFLはマーティンゲール差分と分散境界を使い、非同期更新でも堅牢な収束を保証する
- CMIP6気候データで非同期LSTMモデルを訓練し、現実的な非IIDかつ地理的に分散したデータに対応可能

この非同期アプローチは、バラバラな環境でも機械学習モデルの訓練をサポートできるのが面白いよね。特にリソースが限られた環境での省エネ性が期待できるところが魅力だなって思ったよ。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.SY, eess.SY, **投稿日時:** 2024-12-23 17:11


- - -

### [FedTLU: Federated Learning with Targeted Layer Updates](http://arxiv.org/abs/2412.17692)

**FedTLU: 特定層の更新による連合学習**

Jong-Ik Park, Carlee Joe-Wong

- 連合学習はプライバシーを守りながら複数のクライアントが言語モデルを訓練に貢献する方法。
- クライアント間でデータが非独立非同分布の場合、モデルの性能が制限される問題がある。
- 提案手法はスコアリングを用いた特定層の更新で、ノイズの多い更新を避ける。
- 広範な実験で非IID条件下の収束と性能向上を実現し、効率性を示す。

連合学習でのパフォーマンス問題を特定層の更新でカバーなんて面白そう！精度も効率もいい感じがするね。どんな応用ができるのか、もっと知りたくなっちゃう！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, cs.DC, **投稿日時:** 2024-12-23 16:17


- - -

### [Better Knowledge Enhancement for Privacy-Preserving Cross-Project Defect Prediction](http://arxiv.org/abs/2412.17317)

**プライバシー保護クロスプロジェクト欠陥予測のためのより良い知識強化**

Yuying Wang, Yichen Li, Haozhao Wang, Lei Zhao, Xiaofang Zhang

- クロスプロジェクト欠陥予測（CPDP）は、データプライバシーの懸念から信頼できる欠陥予測モデルの構築が困難
- 連合学習（FL）は、データを共有せずに複数の関係者間で協力してグローバルモデルを訓練し、プライバシーを保証する新しい手法
- データの異質性がモデル訓練に問題をもたらすが、FLをCPDPに直接適用することでプライバシー問題を解決できる
- 提案するFedDPは、地元の異質性認識とグローバルな知識蒸留の二つの手法でベースラインを大きく上回る

データの異質性を活かしつつ、ちゃんとプライバシーを守って問題を解決するなんてすごいね！実験結果も良好なのは本当に頼もしいと思うよ。これからもっと多くのプロジェクトでこの手法が広まるといいな。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.SE, **投稿日時:** 2024-12-23 06:21


- - -

### [FedLEC: Effective Federated Learning Algorithm with Spiking Neural Networks Under Label Skews](http://arxiv.org/abs/2412.17305)

**FedLEC: ラベルの偏りに対応したスパイキングニューラルネットワークによる効果的な連合学習アルゴリズム**

Di Yu, Xin Du, Linshan Jiang, Shunwen Bai, Wentao Tong, Shuiguang Deng

- スパイキングニューラルネットワークを用いた連合学習は省エネで、エッジデバイスに好適
- ラベルの偏りが非独立同分布のデータで問題を引き起こす
- FedLECはラベルの偏りを軽減し、各ローカルモデルの一般化能力を強化する
- 5つのデータセットで実験し、平均11.59%の精度向上を示す

この研究って、ラベルの偏りに悩む人たちに救世主的な感じがするよね。FedLECの改善効果11.59%はなかなかすごいし、新しいエッジデバイスの時代がさらに面白くなりそう〜！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.CV, **投稿日時:** 2024-12-23 05:52


- - -

### [FedMeld: A Model-dispersal Federated Learning Framework for Space-ground Integrated Networks](http://arxiv.org/abs/2412.17231)

**FedMeld: 宇宙地上統合ネットワークのためのモデル分散型連合学習フレームワーク**

Qian Chen, Xianhao Chen, Kaibin Huang

- 宇宙地上統合ネットワーク(SGINs)は6GにおいてAIサービスを地球全域に届ける役割を担う
- 従来のSGINs FLフレームワークは地上局や高価な衛星間リンクが必要で遅延が問題
- FedMeldはモデル分散を使い、インフラ不要で衛星の移動を利用し修正を実現
- FedMeldはグローバルなモデル収束を実証し精度を向上、通信コストを削減する実験結果

FedMeldのアイデアって面白い！人工知能がもっと身近になる未来が楽しみだね。衛星の"持ち運び"で地球中どこでもサービスなんてちょっとワクワクする！

**Comment:** 13 pages, 7 figures. This work has been submitted to the IEEE for   possible publication

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.IT, cs.NI, math.IT, **投稿日時:** 2024-12-23 02:58


- - -

### [Data value estimation on private gradients](http://arxiv.org/abs/2412.17008)

**プライベート勾配におけるデータ価値推定**

Zijian Zhou, Xinyi Xu, Daniela Rus, Bryan Kian Hsiang Low

- 勾配降下法での差分プライバシー技術は、勾配をランダムなガウスノイズで摂動するものである。
- データ価値はML性能を訓練データに帰属させ、データ価格設定や連合学習で利用。
- 既存手法ではDPでノイズ摂動すると推定の不確実性が増え、ほぼランダムな推定となる。
- 提案手法では相関のあるノイズを注入し、不確実性の線形増加を防ぎ、より良い価値推定を実現。

データの価値をしっかり計算できたら、いろんなとこで使えて便利そう！ノイズをうまく扱うテクニック、なんかクールだね！



**トピック:** [連合学習](../../fl), [差分プライバシー](../../dp), **カテゴリ:** cs.LG, cs.AI, cs.CR, **投稿日時:** 2024-12-22 13:15


- - -

### [FedCross: Intertemporal Federated Learning Under Evolutionary Games](http://arxiv.org/abs/2412.16968)

**FedCross: 進化ゲームに基づく時間差連合学習**

Jianfeng Lu, Ying Zhang, Riheng Jia, Shuqin Cao, Jing Liu, Hao Fu

- 連合学習は分散型機械学習でプライバシー漏洩を軽減するが、動的なモバイルネットワークが障害となる
- 高頻度なユーザ移動が問題を引き起こすため、FedCrossでは中断された学習を別デバイスに移行
- 第1段階では、リソース制約下でのタスク配分を多目的移行アルゴリズムと進化ゲーム理論で最適化
- 第2段階では、オークションメカニズムを用いて報酬配分し、高品質モデル提供者を奨励

FedCrossってすごく新しい考え方だね！動的なネットワーク環境にも柔軟に対応できるのが魅力的。これが普及したらモバイル学習の未来が変わりそう！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, cs.GT, **投稿日時:** 2024-12-22 10:59


- - -

### [Fed-ZOE: Communication-Efficient Over-the-Air Federated Learning via Zeroth-Order Estimation](http://arxiv.org/abs/2412.16779)

**Fed-ZOE: ゼロ次推定を用いた効率的なオーバーザエアの連合学習**

Jonggyu Jang, Hyeonsu Lyu, David J. Love, Hyun Jong Yang

- 6G時代において、連合学習は重要なパラダイムである
- オーバーザエア連合学習はデバイス数に依存せず一定の通信オーバーヘッドを実現
- Fed-ZOEはランダム化勾配推定器を用い、通信コストを大幅に削減
- 数値評価でFed-OtAと同等性能を維持しつつ通信コストの削減を実証

Fed-ZOEって、6G時代の通信問題を解決してくれるんだね。高度なネットワークをうまく活用するための重要なステップかも！楽しみだね。

**Comment:** 13 pages

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, eess.SP, **投稿日時:** 2024-12-21 21:24


- - -

### [Label Privacy in Split Learning for Large Models with Parameter-Efficient Training](http://arxiv.org/abs/2412.16669)

**大規模モデルのためのラベルプライバシー保護付きパラメータ効率的学習**

Philip Zmushko, Marat Mansurov, Ruslan Svirschevski, Denis Kuznedelev, Max Ryabinin, Aleksandr Beznosikov

- 大規模なモデルのファインチューニング時にクライアントデータのプライバシー懸念がある
- LoRAを用いるAPI上でのプライバシー保護手法を分析
- 複数者分散学習アルゴリズムP$^3$EFTを提案し、プライバシーと効率を両立
- LoRAアダプターを用いて多くのNLPタスクで高精度を達成

新しいプライバシー保護の方法って、どんな風に進化しているのかなってワクワクするね！P$^3$EFTが他の方法と比べてどれくらい精度が高いのか気になるなー。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.CR, **投稿日時:** 2024-12-21 15:32


- - -

### [FedGA: Federated Learning with Gradient Alignment for Error Asymmetry Mitigation](http://arxiv.org/abs/2412.16582)

**FedGA: エラー非対称性緩和のための勾配整列を用いた連合学習**

Chenguang Xiao, Zheming Zuo, Shuo Wang

- 連合学習はクライアント間でのクラス不均衡を引き起こし、偏った更新を招く
- FedGAは勾配整列を利用して、誤差の非対称性を軽減する手法を提案する
- 本手法はラベル校正を用いてモデルの収束と精度を向上させる
- FedGAは他の手法と比較して、バイアス軽減やF1スコア・精度改善で優れる

FedGAって初めて聞いたけど、勾配整列を使って誤差を調整するなんて面白いよね！今後もいろんな手法が工夫されて、もっと精度があがるかもってワクワクしちゃう！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.CR, **投稿日時:** 2024-12-21 11:15


- - -

### [Privacy in Fine-tuning Large Language Models: Attacks, Defenses, and Future Directions](http://arxiv.org/abs/2412.16504)

**大規模言語モデルの微調整におけるプライバシー: 攻撃、防御、そして将来の方向性**

Hao Du, Shang Liu, Lele Zheng, Yang Cao, Atsuyoshi Nakamura, Lei Chen

- 微調整はLLMの性能を引き出す一方で、プライバシーリスクを伴う
- メンバーシップ推論やデータ抽出攻撃など、微調整における脆弱性を調査
- 差分プライバシーなどの防御策を総括し、その利点と制限を考察
- 現行研究のギャップを指摘し、プライバシー保護の新たな道筋を提案

プライバシーを守りながら大規模モデルを微調整するなんて、なんだかスリリングだよね！何もかもがAIなのに人間らしさを求めたりして、ちょっと未来っぽい感じがするなぁ。



**トピック:** [連合学習](../../fl), [差分プライバシー](../../dp), **カテゴリ:** cs.AI, **投稿日時:** 2024-12-21 06:41


- - -

### [Differentially Private Federated Learning of Diffusion Models for Synthetic Tabular Data Generation](http://arxiv.org/abs/2412.16083)

**差分プライバシーを備えた拡散モデルの連合学習による合成表データ生成**

Timur Sattarov, Marco Schreyer, Damian Borth

- 財務分野でのプライバシー保護データ分析の需要が増えている
- DP-Fed-FinDiffフレームワークは差分プライバシーと連合学習を組み合わせたもの
- 厳しいプライバシー規制を遵守しつつ高品質な合成表データを生成する
- 実証評価でプライバシーとデータ品質のバランスを最適化 

差分プライバシーと拡散モデルの組み合わせが面白そうで、しっかりデータ品質を保っているのが魅力的！金融業界でも安全にデータをシェアする未来がすぐそこに感じるね。

**Comment:** 9 pages, 9 figures, preprint version, currently under review

**トピック:** [連合学習](../../fl), [合成データ](../../sd), [差分プライバシー](../../dp), **カテゴリ:** cs.LG, q-fin.ST, **投稿日時:** 2024-12-20 17:30


- - -

### [fluke: Federated Learning Utility frameworK for Experimentation and research](http://arxiv.org/abs/2412.15728)

**fluke: 実験と研究のための連合学習ユーティリティフレームワーク**

Mirko Polato

- 連合学習は2016年から人気が高まり、さまざまなフレームワークが提案されている
- 既存フレームワークは柔軟性が不足しており、学習曲線が急であるため独自実装が多い
- flukeはPythonで構築された新アルゴリズム開発を簡素化するためのオープンソースのパッケージ
- デフォルトで使用可能で、少ない手間で新アルゴリズムを拡張できる

flukeってめっちゃ便利そう！新しいアルゴリズムがどんどん生まれちゃうかもね。Pythonで使いやすいし、研究者にぴったりかも。

**Comment:** Accepted at FLUID workshop (AAAI 2025) [4 pages (+2 references), 2   figures, 1 algorithm]

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, **投稿日時:** 2024-12-20 09:51


- - -

### [Code Review Automation Via Multi-task Federated LLM -- An Empirical Study](http://arxiv.org/abs/2412.15676)

**連合学習によるマルチタスク型大規模言語モデルを用いたコードレビュー自動化の実証研究**

Jahnavi Kumar, Sridhar Chimalakonda

- コードレビュー自動化は重要だが、通常の手法は3つの独立したサブタスクとして扱われている
- 研究は、サブタスクの関係を利用しマルチタスクモデルで統合的に解決することを目指す
- 連合学習を用いることで、プロプライエタリコードの秘密保持と未知データへのモデルの頑健性を向上させる
- 累積的ファインチューニングが個別タスクモデルより効果的であることが示された

この研究、なんか新しい視点で興味深い感じ！連合学習活用で、セキュリティも保てて効率的に進化できるとか、もっと別の分野にも応用できそうだよね。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.SE, **投稿日時:** 2024-12-20 08:46


- - -

### [AutoRank: MCDA Based Rank Personalization for LoRA-Enabled Distributed Learning](http://arxiv.org/abs/2412.15553)

**AutoRank: LoRA対応分散学習のためのMCDAベースのランク個別化**

Shuaijun Chen, Omid Tavallaie, Niousha Nazemi, Xin Chen, Albert Y. Zomaya

- 分散学習は非独立同分布(Non-IID)データでのモデル学習が難しい問題に直面
- LoRAは低ランクの更新を個別化し計算を最小化するが初期ランク設定が難題
- AutoRankはデータの複雑性に基づいて動的にローカルランクを割り当てるアルゴリズム
- AutoRankにより計算負担を削減し、モデル性能を向上させ、収束を加速させることに成功

データの複雑さに応じて適応するAutoRankは、分散学習における重要な進化だね。ランクを自動設定するって、たくさんの参加者がいる場合にも効率的で嬉しい！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-12-20 04:27


- - -

### [The Impact of Cut Layer Selection in Split Federated Learning](http://arxiv.org/abs/2412.15536)

**スプリット連合学習におけるカット層選択の影響**

Justin Dachille, Chao Huang, Xin Liu

- スプリット連合学習（SFL）は連合学習とスプリット学習を組み合わせた分散学習パラダイムである
- カット層選択がモデル性能に与える影響の定量分析が重要だとわかった
- SFL-V1はカット層の選択に対して性能が比較的一定である
- SFL-V2はカット層選択により性能が大きく変わり、適切な選択でFedAvgより高性能

カット層の選び方で性能が変わるなんて面白いよね。SFLは複雑なデータセットに強そうなので、もっと研究が進むといいな。

**Comment:** 16 pages, 1 figure, AAAI FLUID Workshop 2025

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.DC, cs.LG, **投稿日時:** 2024-12-20 03:52


- - -

### [DualGFL: Federated Learning with a Dual-Level Coalition-Auction Game](http://arxiv.org/abs/2412.15492)

**DualGFL: 二重レベル連合学習フレームワークによる連合学習**

Xiaobing Chen, Xiangwei Zhou, Songyang Zhang, Mingxuan Sun

- 従来の連合学習では単一レベルのゲーム理論を使い、複雑な参加者間の動態を捉えるのが難しかった
- DualGFLは、協力と競争を兼ね備えた二重レベルのゲームを用いて、これを改善する新しいフレームワーク
- 下位レベルでは、クライアントの意思による最適な分割を見つけるため、オークションを意識した効用関数とパレート最適な分割アルゴリズムを提案
- 上位レベルでは、資源制約付きの多属性オークションゲームを設計し、クライアントとサーバー双方の利得を最大化する

DualGFLって、連合学習の協力と競争のバランスをうまく取ってて面白そうだね！これでどんな成果が上がるのか、早く知りたいなぁ。結果が出次第、最新のアプローチとして注目集めそう！

**Comment:** 12 pages, 6 figures. Accepted by AAAI25

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.GT, cs.LG, I.2.6; I.2.11, **投稿日時:** 2024-12-20 02:13
