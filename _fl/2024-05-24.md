---
title: 連合学習 (2024-05-24 ~ 2024-05-30)
date: 2024-05-24
---

連合学習に関する論文まとめ (2024-05-24 ~ 2024-05-30)


- - -

### [Federating Dynamic Models using Early-Exit Architectures for Automatic Speech Recognition on Heterogeneous Clients](http://arxiv.org/abs/2405.17376)

**異種クライアントにおける自動音声認識のための早期退出アーキテクチャを用いた動的モデルの連合**

Mohamed Nabih Ali, Alessio Brutti, Daniele Falavigna

- 自動音声認識モデルの訓練には大量の音声記録が必要であり、データ収集にプライバシーの懸念がある。
- 連合学習を用いて複数クライアントでデータをローカルのまま共有モデルを学習することが可能である。
- クライアントデバイスの計算・通信資源が限られており、単一モデルの生成は最適でない。
- 効率的なアーキテクチャとして早期退出ソリューションを用いた動的モデルを提案し、様々なデバイスで利用可能であることを実証。

早期退出するモデルとか、なんかキャッチーな感じがして良いね！色々なデバイスで一つのモデルが使えちゃうなんて、未来の技術って感じがするなぁ。

**Comment:** The paper is under review in Future Generation Computer Systems   Journal

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CL, **投稿日時:** 2024-05-27 17:32


- - -

### [FedHPL: Efficient Heterogeneous Federated Learning with Prompt Tuning and Logit Distillation](http://arxiv.org/abs/2405.17267)

**FedHPL: プロンプトチューニングとロジット蒸留による効率的な異種連合学習**

Yuting Ma, Lechao Cheng, Yaxiong Wang, Zhun Zhong, Xiaohua Xu, Meng Wang

- 連合学習はデータをローカルに保持しつつモデルを協調訓練するプライバシー保護パラダイム
- 異なるモデル構造やデータ分布、限られたリソースでの性能低下や収束速度の遅れが発生
- FedHPLはプロンプトチューニングとロジット蒸留を基にした統一的連合学習フレームワークを提案
- 実験で既存手法を上回る性能と少ない計算負荷、訓練ラウンドで効率的であることを確認

プロンプトチューニングとロジット蒸留が鍵になってるみたい！効率的にモデルを強化する方法が気になるね。😊

**Comment:** 35 pages

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.CV, **投稿日時:** 2024-05-27 15:25


- - -

### [LabObf: A Label Protection Scheme for Vertical Federated Learning Through Label Obfuscation](http://arxiv.org/abs/2405.17042)

**LabObf: ラベルあいまい化による垂直連合学習のためのラベル保護スキーム**

Ying He, Mingyang Niu, Jingyu Hua, Yunlong Mao, Xu Huang, Chen Li, Sheng Zhong

- 垂直連合学習におけるスプリット学習は、プライバシー保護特性のため広く利用されている
- 悪意ある参加者がアップロードされた埋め込みベクトルからラベル情報を推測し、プライバシー漏洩が懸念される
- Embedding Extension Attackを提案し、現行防御戦略を無力化する手法を扱う
- ラベルのあいまい化防御戦略「LabObf」を提案し、攻撃者の成功率をランダム推測レベルまで低減する

この研究、攻撃と防御のせめぎ合いが面白いよね！LabObfの防御戦略が今後どこまで実用化されるか興味津々だな～。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.CR, **投稿日時:** 2024-05-27 10:54


- - -

### [Visualizing the Shadows: Unveiling Data Poisoning Behaviors in Federated Learning](http://arxiv.org/abs/2405.16707)

**影を可視化する：連合学習におけるデータポイズニング行動の解明**

Xueqing Zhang, Junkai Zhang, Ka-Ho Chow, Juntao Chen, Ying Mao, Mohamed Rahouti, Xiang Li, Yuchen Liu, Wenqi Wei

- 連合学習システムは、ターゲットを絞ったデータポイズニング攻撃に対して脆弱である
- ラベル反転を用いたデータポイズニング攻撃をシミュレーションし、モデル性能への影響を分析
- 五つのコンポーネントで構成されるシステムを使用し、攻撃の可視化と対策を実施
- ラベル操作、攻撃タイミング、悪意ある攻撃の可用性からの観察で、システムの脆弱性を理解

連合学習に対する攻撃の可視化って、実際にどうやって見える化するのか興味津々！未来のセキュリティ対策に役立ちそう。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CR, **投稿日時:** 2024-05-26 21:58


- - -

### [A Systematic Review of Federated Generative Models](http://arxiv.org/abs/2405.16682)

**連合生成モデルの体系的レビュー**

Ashkan Vedadi Gargary, Emiliano De Cristofaro

- 連合学習（FL）と生成モデルを組み合わせることで分散システムでデータを共有せずにモデル訓練が可能
- FLと生成モデルの組み合わせは攻撃に脆弱で、最適なアーキテクチャの設計が困難
- 2019年から2024年にかけての約100本の論文を体系的に比較し、FLと生成モデルの手法、およびプライバシー考慮の点に焦点
- 最先端の進展を強調し、未解決の課題を特定することで、新参者にも理解しやすく今後の研究に洞察を提供

連合学習と生成モデルの融合って、新たなプライバシー技術の未来が見えそう！どんな技術が出てくるのかワクワクするね。

**Comment:** 24 Pages, 3 Figures, 5 Tables

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.CL, cs.CR, **投稿日時:** 2024-05-26 20:20


- - -

### [Fair Federated Learning under Domain Skew with Local Consistency and Domain Diversity](http://arxiv.org/abs/2405.16585)

**異なるドメインにおける局所的一貫性とドメイン多様性を考慮した公平な連合学習**

Yuhang Chen, Wenke Huang, Mang Ye

- ドメインスキュー下での連合学習（FL）は偏りがあり、公平性の問題が二つある
- パラメータアップデートの矛盾があり、不一致なアップデート方向が発生
- モデルの集約バイアスがあり、不公平な重み配分とドメイン多様性の無視が問題
- 提案手法は不要なパラメータアップデートを選択的に破棄し、公平な集約目標を設定

この研究、実践的なリアルな問題を解決しようとしてて、けっこうおもしろい！公平性を保つための工夫、現場にどう役立つか気になる〜。

**Comment:** Accepted by CVPR2024

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, **投稿日時:** 2024-05-26 14:29


- - -

### [Multi-Level Additive Modeling for Structured Non-IID Federated Learning](http://arxiv.org/abs/2405.16472)

**非独立同分布な連合学習のための多層加法モデリング**

Shutong Chen, Tianyi Zhou, Guodong Long, Jie Ma, Jing Jiang, Chengqi Zhang

- 連合学習の主要な課題は、クライアント間の非独立同分布（Non-IID）をモデル化すること
- 非IIDに対応するため、マルチレベル加法モデル（MAM）を用いて知識共有を改善
- 各クライアントは任意のレベルの1つのモデルに割り当てられ、個別の予測はモデルの出力を統合して算出
- FeMAMは既存のクラスタFLおよび個別FL手法を上回り、非IID環境で優れた性能を発揮することを実証

異なるクライアント間での知識共有を効率化するなんて、面白そう！普遍的なモデルと個別モデルを組み合わせるアプローチにワクワクするなぁ！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, **投稿日時:** 2024-05-26 07:54


- - -

### [Machine learning in business process management: A systematic literature review](http://arxiv.org/abs/2405.16396)

**ビジネスプロセスマネジメントにおける機械学習: 系統的文献レビュー**

Sven Weinzierl, Sandra Zilker, Sebastian Dunzer, Martin Matzner

- 機械学習はプログラムを明示的にコーディングせずにデータに基づいて作成するアルゴリズムを提供
- BPMにおける機械学習の用途は、意思決定支援、正確なプロセスモデルの発見、リソース配分の改善に多様
- 研究はBPMタスクをプロセスのライフサイクルのフェーズごとに整理し、機械学習がそのタスクにどう寄与するかを解説
- 結論として、今後の研究の方向性や新しい機械学習概念の適用に関する研究アジェンダを提供

機械学習とビジネスプロセスを一緒にするなんて、超面白そうじゃない！？今後の研究の方向性も提示されてて、革新的なアイデアが次々に生まれそうだね！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, **投稿日時:** 2024-05-26 01:12


- - -

### [Federated Unsupervised Domain Generalization using Global and Local Alignment of Gradients](http://arxiv.org/abs/2405.16304)

**連合学習を用いた教師なしドメイン一般化のための全球および局所の勾配整合**

Farhad Pourpanah, Mahdiyar Molahasani, Milad Soltany, Michael Greenspan, Ali Etemad

- 連合学習におけるドメインシフトと勾配整合の関連性を理論的に確立
- 新手法「FedGaLA」を提案し、クライアントとサーバーでの勾配整合を実施
- PACS、OfficeHome、DomainNet、TerraIncのデータセットで実験し、手法の有効性を検証
- 各コンポーネントとパラメータの影響を調査するためのアブレーション研究と感度分析を実施

勾配整合で新しいドメインに対応できるモデルを作るのすごいね！これって色んなデータセットで試してみてもうまくいくってことなら、めっちゃ注目されそう♪

**Comment:** 23 pages, 4 figure

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.AI, **投稿日時:** 2024-05-25 17:12


- - -

### [Analytic Federated Learning](http://arxiv.org/abs/2405.16240)

**解析的連合学習**

Huiping Zhuang, Run He, Kai Tong, Di Fang, Han Sun, Haoran Li, Tianyi Chen, Ziqian Zeng

- AFL（解析的連合学習）は、連合学習に解析的（閉形式）ソリューションを導入する新たなトレーニングパラダイム
- ローカルクライアントのトレーニングステージでは、1エポックでのトレーニングを実現し、多エポック更新不要
- 集約ステージで絶対集約法（AA法）を導出し、単一ラウンドの集約を可能にし、複数ラウンド集約が不要
- データ異質性やクライアント数に影響されず、AFLは競争力を持ちながら絶対収束し、初のハイパーパラメータフリーメソッド

解析的なアプローチで連合学習がどこまで進化するか楽しみね。クライアント数が多くても影響を受けないなんて、本当だったらすごく面白いかも。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, **投稿日時:** 2024-05-25 13:58


- - -

### [Client2Vec: Improving Federated Learning by Distribution Shifts Aware Client Indexing](http://arxiv.org/abs/2405.16233)

**Client2Vec: 分布シフトに対応したクライアントインデックスによる連合学習の改善**

Yongxin Guo, Lin Wang, Xiaoying Tang, Tao Lin

- 連合学習はプライバシー保護の分散型機械学習だが、クライアント間の大きな分布シフトが課題である
- Client2Vecという機構を導入し、FLトレーニング開始前にクライアントごとの固有のインデックスを生成
- 生成されたクライアントインデックスを利用して、クライアントサンプリング、モデル集約、ローカルトレーニングを改善
- 多様なデータセットとモデルアーキテクチャにおいて、Client2Vecが効率的に機能することを実験で検証

この研究では事前準備としてクライアントインデックスを使うことで、より効率的な学習ができるみたい。クライアントごとの個性を活かすって斬新だよね！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, **投稿日時:** 2024-05-25 13:49


- - -

### [An Experimental Study of Different Aggregation Schemes in Semi-Asynchronous Federated Learning](http://arxiv.org/abs/2405.16086)

**半非同期連合学習における異なる集約方式の実験的研究**

Yunbo Li, Jiaping Gui, Yue Wu

- 連合学習は分散環境での高性能計算とデータプライバシーの保護が特徴
- 半非同期連合学習（SAFL）ではリソースの異質性に対応
- FedSGDとFedAvgの2つのモードを比較、前者は精度高く収束が速いが精度の揺れが大きい
- 後者はストラグラー問題に強いが収束遅く、精度が低い

SAFLの特性を理解して使い分けることがキーだね。精度重視か安定性重視か、どっちが大事か考えるのが面白そう！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.DC, cs.PF, **投稿日時:** 2024-05-25 06:33


- - -

### [FedSheafHN: Personalized Federated Learning on Graph-structured Data](http://arxiv.org/abs/2405.16056)

**FedSheafHN: グラフ構造データにおけるパーソナライズ連合学習**

Wenfei Liang, Yanan Zhao, Rui She, Yiming Li, Wee Peng Tay

- グラフニューラルネットワーク（GNN）のカスタマイズによりクライアントごとのニーズに対応する
- FedSheafHNは協力グラフ埋め込みと効率的なパーソナルモデル生成を提案
- シーフ拡散を用いてクライアントの特性を学び、複雑なクライアント特徴の統合と解釈を改善
- エンピリカル評価で他の既存手法より優れ、モデル収束が速く新しいクライアントへの対応も効果的

この研究、めっちゃ面白そう！特に複雑なクライアント特性をシーフ拡散で改善するところ、すごく未来的だよね。今後この技術が普及したら、いろんなデータに対応できちゃうかも！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, **投稿日時:** 2024-05-25 04:51


- - -

### [Federated Learning for Non-factorizable Models using Deep Generative Prior Approximations](http://arxiv.org/abs/2405.16055)

**非因数分解モデルのための連合学習：深層生成事前分布近似の利用**

Conor Hassan, Joshua J Bon, Elizaveta Semenova, Antonietta Mira, Kerrie Mengersen

- 連合学習（FL）はデータの共有を避けることでプライバシーを保ちながら分散型クライアント間で協調モデルを訓練可能である
- 現在のFL手法はクライアントモデル間の条件付き独立を仮定し、依存を捉える事前分布の使用を制限する
- SIGMA事前分布は深層生成モデルの近似を用いてクライアント間での非因数分解モデルのFLを可能にする
- SIGMA事前分布の有効性は合成データで示され、オーストラリアの空間データFLの実例で実用性が立証された

SIGMA事前分布を使うことで、空間統計や疫学など依存関係をモデル化するのが重要な分野でFLが可能になるんだね。新しい応用分野が広がりそうでワクワクする！

**Comment:** 25 pages, 7 figures, 2 tables

**トピック:** [連合学習](../../fl), [合成データ](../../sd), **カテゴリ:** stat.ML, cs.LG, stat.CO, stat.ME, **投稿日時:** 2024-05-25 04:44


- - -

### [Achieving Dimension-Free Communication in Federated Learning via Zeroth-Order Optimization](http://arxiv.org/abs/2405.15861)

**連合学習におけるゼロ次最適化を通じた次元依存のない通信の実現**

Zhe Li, Bicheng Ying, Zidong Liu, Haibo Yang

- 連合学習は分散データソース間で協働的かつプライバシー保護型機械学習を提供する
- 大規模モデルでの通信コストが次元に比例し、効率性の大きな障害となっている
- 新しいアルゴリズム「FedDisco」により、各通信ラウンドでスカラー値のみを送信
- 実証結果として、従来の連合学習アプローチと比較して通信オーバーヘッドが大幅に削減される

次元に依存しない通信で連合学習がもっとスムーズになるなんて、すごく未来がラクになりそうだね。FedDiscoの名前もディスコみたいでちょっと楽しい気がする。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-05-24 18:07


- - -

### [CAFe: Cost and Age aware Federated Learning](http://arxiv.org/abs/2405.15744)

**CAFe: コストとエイジを考慮した連合学習**

Sahan Liyanaarachchi, Kanchana Thilakarathna, Sennur Ulukus

- FLモデルの訓練進行を確保するために、少なくとも$M$クライアントのローカル勾配の報告を待つが、期限内に報告がない場合はラウンドが失敗と見なされ、再スタートが必要
- 小さい$T$（締め切り）と大きい$M$（クライアント数）は、多くの失敗ラウンドと過剰な通信および計算リソースの無駄を招く
- 大きい$T$はラウンド時間の延長、小さい$M$はノイズの多い勾配を生じるため、バランスが必要
- クライアントの平均エイジが理論的収束境界に影響し、収束のメトリックとして使用可能であり、$M$と$T$の選定を行う分析的手法を提供

この研究、クライアントのエイジを使って効率的に連合学習の収束を目指しているのが面白い！さらに、通信コスト削減もできる方法だから実用的かもね。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, cs.IT, math.IT, **投稿日時:** 2024-05-24 17:41


- - -

### [Harnessing Increased Client Participation with Cohort-Parallel Federated Learning](http://arxiv.org/abs/2405.15644)

**クライアント参加の向上を活用したコホート並列連合学習**

Akash Dhasade, Anne-Marie Kermarrec, Tuan-Anh Nguyen, Rafael Pires, Martijn de Vos

- 連合学習では、多くのノードが参加するほど、個々のモデル更新の効果が減少
- ネットワークをコホートに分割して、各コホートで独立に学習する新手法CPFLを提案
- コホートごとのモデルを知識蒸留とクロスドメインのラベルなしデータセットで統合
- 実験では、従来のFLと比較し、トレーニング時間とリソース使用量を大幅に削減しつつ精度低下を最小限に抑えた

"連合学習のさらなる最適化なんて面白そう！これでより効率的なモデルが期待できるね！"



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-05-24 15:34


- - -

### [Federated Behavioural Planes: Explaining the Evolution of Client Behaviour in Federated Learning](http://arxiv.org/abs/2405.15632)

**連合行動プレーン: 連合学習におけるクライアント行動の進化を解明する**

Dario Fenoglio, Gabriele Dominici, Pietro Barbiero, Alberto Tonda, Martin Gjoreski, Marc Langheinrich

- 連合学習はプライバシーリスクを減らすが、クライアント行動の理解が依然として課題
- 連合行動プレーン（FBP）を提案し、クライアントの行動を予測性能と意思決定の2つの観点から解析
- FBPによりクライアントの進化状態を示し、似た行動を持つクライアントを識別可能
- FBPのパターンを活用し、悪意あるクライアントモデルを検出する技術（連合行動シールド）を提案

クライアントの行動を可視化するなんておもしろそう！連合行動シールドの効果がどれくらいあるのか気になるね。

**Comment:** [v1] Preprint (24 pages)

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-05-24 15:17


- - -

### [DAGER: Exact Gradient Inversion for Large Language Models](http://arxiv.org/abs/2405.15586)

**DAGER: 大規模言語モデルのための厳密な勾配逆転手法**

Ivo Petrov, Dimitar I. Dimitrov, Maximilian Baader, Mark Niklas Müller, Martin Vechev

- 連合学習は複数のクライアントから計算された勾配を集約し、プライベートデータを共有せずに協力的な学習を実現する
- 過去の研究では、サーバーが勾配逆転攻撃を用いてデータを回復できることが示されていたが、テキストでは精度が低かった
- DAGERは、クライアントデータのトークンシーケンスを効率的に検出し、全バッチのテキストを正確に回復する初のアルゴリズム
- DAGERは、大規模言語モデルに対して高速（同バッチサイズで20倍）、スケーラブル（10倍大きいバッチ）、高品質な再構築（ROUGE-1/2 > 0.99）を実現

大規模言語モデルのテキストデータがそんなに正確に回復できるなんてびっくり！これからのプライバシー保護、どうなるんだろうね？



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, I.2.7; I.2.11, **投稿日時:** 2024-05-24 14:14


- - -

### [Thinking Forward: Memory-Efficient Federated Finetuning of Language Models](http://arxiv.org/abs/2405.15551)

**先を見据えた考え: メモリ効率の良い連合学習による言語モデルの微調整**

Kunjal Panchal, Nisarg Parikh, Sunav Choudhary, Lijun Zhang, Yuriy Brun, Hui Guan

- 従来のバックプロパゲーションを用いた微調整は中間活性化層が大きなメモリを消費し、リソース制約のあるデバイスには困難
- スプライ（Spry）は、各クライアントがフォワードモード自動微分を用いて訓練し、メモリ使用量を低減しつつ高精度と高速収束を実現
- 同質データ分布ではグローバル勾配の無偏推定を理論的に確認、不均質性が増すと推定のバイアスも増加
- 1.4-7.1倍のメモリ削減効果と、1.2-20.3倍の収束時間短縮、最先端の0次手法と比べて5.2-13.5%の精度向上を実証

メモリ効率がめっちゃ良くなるってすごいね！こんな技術でどんどんFLの用途が広がりそうだよ、スマホでもどんどん使えちゃうね！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, **投稿日時:** 2024-05-24 13:37


- - -

### [Unlearning during Learning: An Efficient Federated Machine Unlearning Method](http://arxiv.org/abs/2405.15474)

**学習中の忘却：効率的な連合機械学習忘却方法**

Hanlin Gu, Gongxi Zhu, Jie Zhang, Xinyuan Zhao, Yuxing Han, Lixin Fan, Qiang Yang

- 連合学習は分散機械学習の新たなパラダイムとして注目
- 従来の連合機械忘却は時間がかかり、実用性に欠ける
- FedAUは軽量な補助モジュールと直線的操作で効率化
- FedAUは個別データ、クラス、クライアントレベルの忘却をサポート

実際のFL環境で使える効率的な忘却方法が出てきたなんて、ワクワクするね！これからもっとデータプライバシーが進化していくって感じがするよ。

**Comment:** Accepted by IJCAI 2024

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-05-24 11:53


- - -

### [FedCal: Achieving Local and Global Calibration in Federated Learning via Aggregated Parameterized Scaler](http://arxiv.org/abs/2405.15458)

**FedCal: 連合学習における集約パラメータスケーラーを用いたローカルおよびグローバルキャリブレーションの実現**

Hongyi Peng, Han Yu, Xiaoli Tang, Xiaoxiao Li

- 連合学習におけるデータの多様性がモデルのキャリブレーションにチャレンジをもたらす
- 現存の連合学習手法はキャリブレーションにおいて最適ではないと理論分析から示される
- FedCalアプローチはローカルおよびグローバルキャリブレーションを重視し、クライアント固有スケーラーを利用
- 実験ではFedCalがベースラインより47.66%もグローバルキャリブレーションエラーを削減

Federated Learningって、データがバラバラでも協力して学習できるのが魅力的だよね。このFedCalの方法なら、もっと精度よくなるなんてワクワクするね！

**Comment:** This paper has been accepted by ICML'24

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-05-24 11:33


- - -

### [Towards Client Driven Federated Learning](http://arxiv.org/abs/2405.15407)

**クライアント主導型連合学習に向けて**

Songze Li, Chenqing Zhu

- 従来の連合学習はサーバー中心で、クライアントの非同期なニーズに対応できない課題がある
- CDFLはクライアントが独立してモデルを更新し、サーバーがクラスター分布を管理
- CDFLにより、時間変動するクライアントのデータ分布に迅速に適応可能となる
- クライアントへの送信モデルを単一にすることで計算効率と推定精度向上を実現

これはすごい！クライアント主体でプライバシー保護できるなんて未来の技術にワクワクしちゃう。CDFSが普及すれば色んな問題が解決できそうね！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.DC, **投稿日時:** 2024-05-24 10:17


- - -

### [Transformer-based Federated Learning for Multi-Label Remote Sensing Image Classification](http://arxiv.org/abs/2405.15405)

**トランスフォーマーベースの連合学習によるマルチラベルリモートセンシング画像分類**

Barış Büyüktaş, Kenneth Weitzel, Sebastian Völkers, Felix Zailskas, Begüm Demir

- 連合学習はクライアントの訓練データにアクセスせずにモデルを学習するが、非独立同分布データの影響で収束が困難
- 最先端のトランスフォーマー（MLP-Mixer, ConvMixer, PoolFormer）が非独立同分布データの問題にどう対処するかを調査
- ResNet-50と比較し、異なる非独立同分布レベルでの頑健性、ローカルトレーニングの複雑さ、集約の複雑さを評価
- BigEarthNet-S2ベンチマークでの実験結果は、一般化能力が向上するがローカルトレーニングと集約の複雑さも増加

連合学習ってなんかおもしろいよね！トランスフォーマーのモデル選定についても詳しく研究されていて、実用的なガイドラインが得られるのがいいなぁ。

**Comment:** Accepted at IEEE International Geoscience and Remote Sensing   Symposium (IGARSS) 2024. Our code is available at   https://git.tu-berlin.de/rsim/FL-Transformer

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CV, **投稿日時:** 2024-05-24 10:13


- - -

### [Decaf: Data Distribution Decompose Attack against Federated Learning](http://arxiv.org/abs/2405.15316)

**Decaf: 連合学習に対するデータ分布分解攻撃**

Zhiyang Dai, Chunyi Zhou, Anmin Fu

- 既存のプライバシー攻撃技術とは異なり、連合学習に対する新しいプライバシー脅威を提案
- Decafの攻撃により、FLサーバーは被害者のデータ分布を高い精度でプロファイル可能
- 5つのベンチマークデータセットで実験し、多様なモデルアーキテクチャでDecafの有効性を検証
- Nullクラスがない場合、Decafのデータ分解精度は5%未満で、Nullクラス判定精度は100%

これは超面白そう！連合学習の仕組みを本当に分かってるFLサーバーだと、ついにこっそりデータ分析ができちゃうんだね。未来のAIプライバシー対策もますます重要になりそう。



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.LG, cs.CR, **投稿日時:** 2024-05-24 07:56


- - -

### [Leakage-Resilient and Carbon-Neutral Aggregation Featuring the Federated AI-enabled Critical Infrastructure](http://arxiv.org/abs/2405.15258)

**漏洩耐性およびカーボンニュートラルな連合AI支援型重要インフラ集約手法**

Zehang Deng, Ruoxi Sun, Minhui Xue, Sheng Wen, Seyit Camtepe, Surya Nepal, Yang Xiang

- 連合学習技術を活用する重要インフラでも、勾配最適化を通じてデータ再構築攻撃のリスクが存在
- 新たな圧縮差分プライベート集約（CDPA）で漏洩耐性、通信効率、カーボンニュートラルを実現
- ランダムビット反転メカニズムによりノイズの分散を拡大し、差分プライバシー保護とエネルギー節約を両立
- CDPAは通信コストを半減させつつモデルの有用性を保持し、最新のデータ再構築攻撃を効果的に防御

省エネとプライバシー保護が同時に進められるなんて、未来のインフラにとって嬉しいポイントだよね。二酸化炭素排出量も大幅に減らせるから、環境面でも優しそう！



**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CR, **投稿日時:** 2024-05-24 06:35


- - -

### [RFLPA: A Robust Federated Learning Framework against Poisoning Attacks with Secure Aggregation](http://arxiv.org/abs/2405.15182)

**RFLPA：セキュアアグリゲーションを用いた毒素攻撃に対する強靭な連合学習フレームワーク**

Peihua Mai, Ran Yan, Yan Pang

- 連合学習はデータ共有なしで複数デバイスがモデルを共同訓練するが、プライバシー漏洩と毒素攻撃の脆弱性がある
- 毒素攻撃防御策は平文でのローカル更新の解析に依存しているため、セキュアアグリゲーションと互換性がない
- 提案するRFLPAフレームワークはセキュアアグリゲーションプロトコルに基づき、ローカル更新とサーバー更新のコサイン類似度を計算して強靭なアグリゲーションを実施
- 実験結果では、RFLPAは最先端の防御方法（BREA）と比較して通信および計算オーバーヘッドを75%以上削減し、競争力のある精度を維持

毒素攻撃もプライバシーも一挙に解決できるなんて、すごく斬新！これが普及したらデータの扱い方が一段と安心になりそう。

**Comment:** 22 pages

**トピック:** [連合学習](../../fl), **カテゴリ:** cs.CR, cs.AI, E.4, **投稿日時:** 2024-05-24 03:31
